{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPool2D, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Endpoint: <samp>https://notify.run/UefDopOIh61y5Fyr</samp></p>\n",
       "<p>To subscribe, open: <a href=\"https://notify.run/c/UefDopOIh61y5Fyr\">https://notify.run/c/UefDopOIh61y5Fyr</a></p>\n",
       "<p>Or scan this QR code:</p>\n",
       "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"222\" width=\"222\" class=\"pyqrcode\"><path transform=\"scale(6)\" stroke=\"#000\" class=\"pyqrline\" d=\"M4 4.5h7m2 0h1m4 0h1m1 0h1m1 0h3m1 0h7m-29 1h1m5 0h1m1 0h2m1 0h3m1 0h2m3 0h1m1 0h1m5 0h1m-29 1h1m1 0h3m1 0h1m2 0h3m3 0h1m4 0h1m1 0h1m1 0h3m1 0h1m-29 1h1m1 0h3m1 0h1m1 0h2m1 0h1m4 0h2m2 0h1m1 0h1m1 0h3m1 0h1m-29 1h1m1 0h3m1 0h1m3 0h1m1 0h2m2 0h1m1 0h1m1 0h1m1 0h1m1 0h3m1 0h1m-29 1h1m5 0h1m1 0h1m1 0h1m2 0h3m1 0h1m2 0h1m1 0h1m5 0h1m-29 1h7m1 0h1m1 0h1m1 0h1m1 0h1m1 0h1m1 0h1m1 0h1m1 0h7m-17 1h2m2 0h1m1 0h3m-21 1h5m1 0h5m1 0h1m1 0h2m3 0h1m1 0h1m1 0h1m1 0h1m1 0h1m-26 1h4m3 0h1m5 0h2m1 0h2m1 0h4m3 0h1m-29 1h1m4 0h2m1 0h2m1 0h2m2 0h2m3 0h1m2 0h2m-17 1h4m2 0h3m1 0h2m1 0h2m2 0h1m1 0h1m-28 1h2m2 0h4m1 0h1m1 0h1m3 0h3m7 0h2m-26 1h1m5 0h2m1 0h1m1 0h3m2 0h4m1 0h3m3 0h1m-27 1h1m2 0h2m2 0h2m2 0h5m2 0h3m1 0h3m-26 1h2m2 0h1m1 0h3m2 0h2m1 0h1m3 0h2m2 0h2m2 0h1m-26 1h2m2 0h1m2 0h2m1 0h1m1 0h1m1 0h2m1 0h1m1 0h1m1 0h1m1 0h2m-27 1h1m1 0h1m1 0h2m10 0h1m2 0h1m1 0h4m1 0h1m1 0h1m-29 1h1m1 0h5m1 0h2m1 0h2m2 0h2m1 0h1m3 0h2m2 0h1m-27 1h1m2 0h3m1 0h3m1 0h1m2 0h2m2 0h6m3 0h1m-28 1h1m2 0h1m1 0h4m1 0h2m3 0h2m2 0h6m1 0h3m-21 1h3m1 0h3m2 0h2m1 0h1m3 0h5m-29 1h7m1 0h1m4 0h3m1 0h1m1 0h2m1 0h1m1 0h3m-27 1h1m5 0h1m2 0h2m1 0h2m2 0h1m2 0h2m3 0h1m-25 1h1m1 0h3m1 0h1m1 0h1m1 0h1m1 0h1m1 0h3m3 0h5m1 0h1m-27 1h1m1 0h3m1 0h1m1 0h1m1 0h1m6 0h3m5 0h4m-29 1h1m1 0h3m1 0h1m1 0h1m1 0h3m2 0h1m2 0h2m2 0h6m-28 1h1m5 0h1m1 0h1m2 0h1m2 0h2m3 0h3m3 0h1m1 0h1m-28 1h7m1 0h4m5 0h1m1 0h1m1 0h3m2 0h1\"/></svg>\n",
       "\n",
       "        "
      ],
      "text/plain": [
       "Endpoint: https://notify.run/UefDopOIh61y5Fyr\n",
       "To subscribe, open: https://notify.run/c/UefDopOIh61y5Fyr\n",
       "Or scan this QR code:\n",
       "\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\n",
       "        "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notify_run import Notify\n",
    "notify = Notify()\n",
    "notify.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom functions\n",
    "sys.path.append('C:\\\\Users\\\\Dylan\\\\Desktop\\\\Data Science\\\\Projects\\\\DistractedDrivers\\\\functions')\n",
    "from ddfuncs import trainsampling, cvrand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Set memory limit on GPU to keep it from freezing up when fitting TensorFlow models later\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB * 3 of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], \\\n",
    "                                                                [tf.config.experimental.\\\n",
    "                                                                 VirtualDeviceConfiguration\\\n",
    "                                                                 (memory_limit=1024 * 3)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed training data\n",
    "os.chdir('../data/processed')\n",
    "df = pd.read_csv('driver_image_list_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trainsampling(df, samples=80, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "      <th>imgpath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_13073.jpg</td>\n",
       "      <td>imgs/train/c0/img_13073.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_5585.jpg</td>\n",
       "      <td>imgs/train/c0/img_5585.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_48187.jpg</td>\n",
       "      <td>imgs/train/c0/img_48187.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_98115.jpg</td>\n",
       "      <td>imgs/train/c0/img_98115.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_66355.jpg</td>\n",
       "      <td>imgs/train/c0/img_66355.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22364</th>\n",
       "      <td>p081</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_95966.jpg</td>\n",
       "      <td>imgs/train/c9/img_95966.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22404</th>\n",
       "      <td>p081</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_18412.jpg</td>\n",
       "      <td>imgs/train/c9/img_18412.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22415</th>\n",
       "      <td>p081</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_23818.jpg</td>\n",
       "      <td>imgs/train/c9/img_23818.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22358</th>\n",
       "      <td>p081</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_54961.jpg</td>\n",
       "      <td>imgs/train/c9/img_54961.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22395</th>\n",
       "      <td>p081</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_13314.jpg</td>\n",
       "      <td>imgs/train/c9/img_13314.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20800 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject classname            img                      imgpath\n",
       "51       p002        c0  img_13073.jpg  imgs/train/c0/img_13073.jpg\n",
       "14       p002        c0   img_5585.jpg   imgs/train/c0/img_5585.jpg\n",
       "71       p002        c0  img_48187.jpg  imgs/train/c0/img_48187.jpg\n",
       "60       p002        c0  img_98115.jpg  imgs/train/c0/img_98115.jpg\n",
       "20       p002        c0  img_66355.jpg  imgs/train/c0/img_66355.jpg\n",
       "...       ...       ...            ...                          ...\n",
       "22364    p081        c9  img_95966.jpg  imgs/train/c9/img_95966.jpg\n",
       "22404    p081        c9  img_18412.jpg  imgs/train/c9/img_18412.jpg\n",
       "22415    p081        c9  img_23818.jpg  imgs/train/c9/img_23818.jpg\n",
       "22358    p081        c9  img_54961.jpg  imgs/train/c9/img_54961.jpg\n",
       "22395    p081        c9  img_13314.jpg  imgs/train/c9/img_13314.jpg\n",
       "\n",
       "[20800 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to appropriate directory for data generation\n",
    "os.chdir('../raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 - 1 Conv, 1 MaxPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 254, 254, 10)      280       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 25, 25, 10)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6250)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                62510     \n",
      "=================================================================\n",
      "Total params: 62,790\n",
      "Trainable params: 62,790\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Conv2D(10, kernel_size=3, activation='relu', input_shape=(256, 256, 3)))\n",
    "model1.add(MaxPool2D(10))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(10, activation='softmax'))\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "model1.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdgen = ImageDataGenerator(rescale=1./255)\n",
    "tedgen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV iteration 1 of 13\n",
      "Validation subjects are ['p026' 'p050' 'p002']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 31s 266ms/step - loss: 2.2712 - accuracy: 0.1598 - val_loss: 2.3211 - val_accuracy: 0.1175\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 26s 229ms/step - loss: 2.1409 - accuracy: 0.2522 - val_loss: 2.2922 - val_accuracy: 0.1158\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 27s 239ms/step - loss: 1.9534 - accuracy: 0.3875 - val_loss: 2.2661 - val_accuracy: 0.1321\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 27s 231ms/step - loss: 1.7529 - accuracy: 0.4793 - val_loss: 2.2995 - val_accuracy: 0.1904\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 27s 232ms/step - loss: 1.5629 - accuracy: 0.5614 - val_loss: 2.2295 - val_accuracy: 0.2729\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 27s 232ms/step - loss: 1.4034 - accuracy: 0.6087 - val_loss: 2.2611 - val_accuracy: 0.2288\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 26s 229ms/step - loss: 1.3112 - accuracy: 0.6315 - val_loss: 2.2407 - val_accuracy: 0.2604\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 1.1499 - accuracy: 0.7239 - val_loss: 2.3522 - val_accuracy: 0.1967\n",
      "CV iteration 2 of 13\n",
      "Validation subjects are ['p052' 'p039' 'p024']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 26s 230ms/step - loss: 2.2889 - accuracy: 0.1467 - val_loss: 2.2439 - val_accuracy: 0.1533\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 27s 232ms/step - loss: 2.2028 - accuracy: 0.2114 - val_loss: 2.1346 - val_accuracy: 0.3267\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 27s 231ms/step - loss: 2.1115 - accuracy: 0.2913 - val_loss: 2.0682 - val_accuracy: 0.3333\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 26s 229ms/step - loss: 2.0139 - accuracy: 0.3636 - val_loss: 1.9338 - val_accuracy: 0.4387\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 26s 227ms/step - loss: 1.9059 - accuracy: 0.4082 - val_loss: 1.8213 - val_accuracy: 0.4317\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 27s 236ms/step - loss: 1.7727 - accuracy: 0.4685 - val_loss: 1.7147 - val_accuracy: 0.5079\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 26s 228ms/step - loss: 1.6648 - accuracy: 0.5065 - val_loss: 1.6240 - val_accuracy: 0.5392\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 26s 225ms/step - loss: 1.5312 - accuracy: 0.5690 - val_loss: 1.5329 - val_accuracy: 0.4858\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 1.4102 - accuracy: 0.5962 - val_loss: 1.4836 - val_accuracy: 0.5600\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.3249 - accuracy: 0.6473 - val_loss: 1.3614 - val_accuracy: 0.5800\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.1932 - accuracy: 0.6967 - val_loss: 1.3101 - val_accuracy: 0.5888\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.0972 - accuracy: 0.7250 - val_loss: 1.2536 - val_accuracy: 0.6242\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 27s 233ms/step - loss: 0.9858 - accuracy: 0.7625 - val_loss: 1.2185 - val_accuracy: 0.6275\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 26s 224ms/step - loss: 0.9337 - accuracy: 0.7777 - val_loss: 1.1686 - val_accuracy: 0.6617\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 27s 231ms/step - loss: 0.8683 - accuracy: 0.7908 - val_loss: 1.1632 - val_accuracy: 0.6100\n",
      "CV iteration 3 of 13\n",
      "Validation subjects are ['p049' 'p064' 'p042']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 26s 224ms/step - loss: 2.2893 - accuracy: 0.1484 - val_loss: 2.2315 - val_accuracy: 0.2212\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 2.2239 - accuracy: 0.1913 - val_loss: 2.1798 - val_accuracy: 0.1883\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 27s 232ms/step - loss: 2.1260 - accuracy: 0.2810 - val_loss: 2.1210 - val_accuracy: 0.2525\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 222ms/step - loss: 2.0300 - accuracy: 0.3402 - val_loss: 2.0306 - val_accuracy: 0.2429\n",
      "CV iteration 4 of 13\n",
      "Validation subjects are ['p051' 'p066' 'p014']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 32s 280ms/step - loss: 2.2836 - accuracy: 0.1565 - val_loss: 2.2825 - val_accuracy: 0.1908\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 32s 280ms/step - loss: 2.1837 - accuracy: 0.2201 - val_loss: 2.2156 - val_accuracy: 0.2013\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 28s 244ms/step - loss: 2.0966 - accuracy: 0.2984 - val_loss: 2.1525 - val_accuracy: 0.2650\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 28s 243ms/step - loss: 1.9817 - accuracy: 0.3668 - val_loss: 2.0822 - val_accuracy: 0.3129\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.8570 - accuracy: 0.4326 - val_loss: 2.0629 - val_accuracy: 0.2683\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.6947 - accuracy: 0.4929 - val_loss: 1.9682 - val_accuracy: 0.3175\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 213ms/step - loss: 1.5677 - accuracy: 0.5375 - val_loss: 1.9184 - val_accuracy: 0.3250\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 213ms/step - loss: 1.4157 - accuracy: 0.6065 - val_loss: 1.8156 - val_accuracy: 0.3917\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.2573 - accuracy: 0.6750 - val_loss: 1.7843 - val_accuracy: 0.3771\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.1579 - accuracy: 0.6984 - val_loss: 1.7060 - val_accuracy: 0.4658\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.0322 - accuracy: 0.7299 - val_loss: 1.7364 - val_accuracy: 0.4254\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 0.9138 - accuracy: 0.7918 - val_loss: 1.6342 - val_accuracy: 0.4421\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 0.8866 - accuracy: 0.7870 - val_loss: 1.6501 - val_accuracy: 0.4538\n",
      "CV iteration 5 of 13\n",
      "Validation subjects are ['p021' 'p045' 'p056']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 2.2950 - accuracy: 0.1353 - val_loss: 2.2572 - val_accuracy: 0.1758\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 2.2012 - accuracy: 0.2190 - val_loss: 2.1898 - val_accuracy: 0.2296\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 2.1069 - accuracy: 0.2984 - val_loss: 2.1379 - val_accuracy: 0.1679\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 2.0292 - accuracy: 0.3380 - val_loss: 2.0740 - val_accuracy: 0.2521\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.9026 - accuracy: 0.3935 - val_loss: 1.9716 - val_accuracy: 0.3454\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.7924 - accuracy: 0.4658 - val_loss: 1.8366 - val_accuracy: 0.3600\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.6510 - accuracy: 0.5196 - val_loss: 1.8299 - val_accuracy: 0.3450\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.5338 - accuracy: 0.5543 - val_loss: 1.6992 - val_accuracy: 0.4421\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.4422 - accuracy: 0.5859 - val_loss: 1.6399 - val_accuracy: 0.4421\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.3384 - accuracy: 0.6288 - val_loss: 1.5638 - val_accuracy: 0.4571\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.2250 - accuracy: 0.6620 - val_loss: 1.5234 - val_accuracy: 0.4833\n",
      "CV iteration 6 of 13\n",
      "Validation subjects are ['p042' 'p050' 'p049']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 2.2963 - accuracy: 0.1462 - val_loss: 2.2628 - val_accuracy: 0.1375\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 2.1900 - accuracy: 0.2228 - val_loss: 2.2122 - val_accuracy: 0.16002.1964 - accu\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 2.1092 - accuracy: 0.2875 - val_loss: 2.1464 - val_accuracy: 0.1575\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 2.0311 - accuracy: 0.3065 - val_loss: 2.0969 - val_accuracy: 0.2958\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.8977 - accuracy: 0.3962 - val_loss: 2.0616 - val_accuracy: 0.1850\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.7514 - accuracy: 0.4766 - val_loss: 2.0087 - val_accuracy: 0.2254\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.6500 - accuracy: 0.4978 - val_loss: 1.9916 - val_accuracy: 0.2417\n",
      "CV iteration 7 of 13\n",
      "Validation subjects are ['p002' 'p049' 'p045']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 2.2956 - accuracy: 0.1424 - val_loss: 2.2938 - val_accuracy: 0.1550\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 2.1829 - accuracy: 0.2293 - val_loss: 2.2156 - val_accuracy: 0.1946\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 2.0987 - accuracy: 0.3092 - val_loss: 2.1574 - val_accuracy: 0.2633\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.9933 - accuracy: 0.3592 - val_loss: 2.0953 - val_accuracy: 0.3225\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.8576 - accuracy: 0.4326 - val_loss: 2.0590 - val_accuracy: 0.3313\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.7164 - accuracy: 0.4826 - val_loss: 1.9975 - val_accuracy: 0.3571\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.5602 - accuracy: 0.5402 - val_loss: 1.8739 - val_accuracy: 0.4288\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.4620 - accuracy: 0.5837 - val_loss: 1.8227 - val_accuracy: 0.4512\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.3033 - accuracy: 0.6625 - val_loss: 1.7986 - val_accuracy: 0.4688\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.1636 - accuracy: 0.7071 - val_loss: 1.6741 - val_accuracy: 0.4871\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.0392 - accuracy: 0.7380 - val_loss: 1.7509 - val_accuracy: 0.4725\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 0.9438 - accuracy: 0.7685 - val_loss: 1.7352 - val_accuracy: 0.4821\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 0.9000 - accuracy: 0.7788 - val_loss: 1.7408 - val_accuracy: 0.4546\n",
      "CV iteration 8 of 13\n",
      "Validation subjects are ['p061' 'p012' 'p041']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 2.3029 - accuracy: 0.1408 - val_loss: 2.2462 - val_accuracy: 0.1400\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 2.2097 - accuracy: 0.2005 - val_loss: 2.1840 - val_accuracy: 0.1892\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 2.1111 - accuracy: 0.3027 - val_loss: 2.1118 - val_accuracy: 0.2825\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 2.0169 - accuracy: 0.3630 - val_loss: 2.0375 - val_accuracy: 0.3017\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.8972 - accuracy: 0.4179 - val_loss: 1.9868 - val_accuracy: 0.2550\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.7776 - accuracy: 0.4658 - val_loss: 1.9495 - val_accuracy: 0.2796\n",
      "CV iteration 9 of 13\n",
      "Validation subjects are ['p026' 'p049' 'p015']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 2.2879 - accuracy: 0.1543 - val_loss: 2.2520 - val_accuracy: 0.2150\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 2.1880 - accuracy: 0.2196 - val_loss: 2.1999 - val_accuracy: 0.2579\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 2.0795 - accuracy: 0.2935 - val_loss: 2.1657 - val_accuracy: 0.2683\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.9676 - accuracy: 0.3641 - val_loss: 2.0850 - val_accuracy: 0.3033\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.8481 - accuracy: 0.4114 - val_loss: 2.0251 - val_accuracy: 0.3508\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 1.7358 - accuracy: 0.4533 - val_loss: 1.9521 - val_accuracy: 0.3167\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.5892 - accuracy: 0.5283 - val_loss: 1.9310 - val_accuracy: 0.3279\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.4443 - accuracy: 0.5918 - val_loss: 1.9186 - val_accuracy: 0.3638\n",
      "CV iteration 10 of 13\n",
      "Validation subjects are ['p052' 'p045' 'p051']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 2.2989 - accuracy: 0.1326 - val_loss: 2.2642 - val_accuracy: 0.1371\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 2.1961 - accuracy: 0.2109 - val_loss: 2.1821 - val_accuracy: 0.2267\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 2.1069 - accuracy: 0.2870 - val_loss: 2.0891 - val_accuracy: 0.3000\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 1.9941 - accuracy: 0.3755 - val_loss: 1.9995 - val_accuracy: 0.2683\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 1.8737 - accuracy: 0.4114 - val_loss: 1.9022 - val_accuracy: 0.3304\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 1.7668 - accuracy: 0.4772 - val_loss: 1.7685 - val_accuracy: 0.5158\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 1.6068 - accuracy: 0.5370 - val_loss: 1.6802 - val_accuracy: 0.4146\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 1.4937 - accuracy: 0.5897 - val_loss: 1.5230 - val_accuracy: 0.5746\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 1.3203 - accuracy: 0.6446 - val_loss: 1.4763 - val_accuracy: 0.4675\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.2086 - accuracy: 0.6842 - val_loss: 1.4228 - val_accuracy: 0.5983\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 1.1456 - accuracy: 0.7016 - val_loss: 1.3902 - val_accuracy: 0.6087\n",
      "CV iteration 11 of 13\n",
      "Validation subjects are ['p045' 'p021' 'p016']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 2.2925 - accuracy: 0.1310 - val_loss: 2.2742 - val_accuracy: 0.1275\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 2.2015 - accuracy: 0.2321 - val_loss: 2.1901 - val_accuracy: 0.2104\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 2.0962 - accuracy: 0.3022 - val_loss: 2.1336 - val_accuracy: 0.2396\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 2.0105 - accuracy: 0.3489 - val_loss: 2.0383 - val_accuracy: 0.3267\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 1.8985 - accuracy: 0.4016 - val_loss: 1.9172 - val_accuracy: 0.3438\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.7719 - accuracy: 0.4641 - val_loss: 1.8553 - val_accuracy: 0.3567\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 1.6556 - accuracy: 0.5065 - val_loss: 1.8305 - val_accuracy: 0.3117\n",
      "CV iteration 12 of 13\n",
      "Validation subjects are ['p022' 'p064' 'p035']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 2.2953 - accuracy: 0.1484 - val_loss: 2.2284 - val_accuracy: 0.1713\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 2.1846 - accuracy: 0.2375 - val_loss: 2.1724 - val_accuracy: 0.2362\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 2.0891 - accuracy: 0.3255 - val_loss: 2.1374 - val_accuracy: 0.2342\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.9714 - accuracy: 0.3826 - val_loss: 2.0510 - val_accuracy: 0.2500\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.8462 - accuracy: 0.4446 - val_loss: 2.0177 - val_accuracy: 0.2663\n",
      "CV iteration 13 of 13\n",
      "Validation subjects are ['p021' 'p061' 'p042']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 2.3005 - accuracy: 0.1397 - val_loss: 2.2486 - val_accuracy: 0.1650\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 2.1890 - accuracy: 0.2288 - val_loss: 2.1676 - val_accuracy: 0.1746\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 2.0963 - accuracy: 0.3114 - val_loss: 2.0779 - val_accuracy: 0.2321\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.9733 - accuracy: 0.3766 - val_loss: 1.9818 - val_accuracy: 0.2792\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.8421 - accuracy: 0.4549 - val_loss: 1.8526 - val_accuracy: 0.3013\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.7172 - accuracy: 0.4913 - val_loss: 1.7979 - val_accuracy: 0.3367\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.5266 - accuracy: 0.5897 - val_loss: 1.6770 - val_accuracy: 0.3046\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.3939 - accuracy: 0.6179 - val_loss: 1.7466 - val_accuracy: 0.3196\n"
     ]
    }
   ],
   "source": [
    "model1data = cvrand(model1,\n",
    "                    df,\n",
    "                    trdgen,\n",
    "                    tedgen,\n",
    "                    n_iterations=13,\n",
    "                    batch_size=16,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch=115,\n",
    "                    target_size=(256,256),\n",
    "                    random_state=42,\n",
    "                    min_delta=0.05,\n",
    "                    patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>validation_subjects</th>\n",
       "      <th>train_accuracies</th>\n",
       "      <th>validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[p026, p050, p002]</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[p052, p039, p024]</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[p049, p064, p042]</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[p051, p066, p014]</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[p021, p045, p056]</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[p042, p050, p049]</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[p002, p049, p045]</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[p061, p012, p041]</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[p026, p049, p015]</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[p052, p045, p051]</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[p045, p021, p016]</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[p022, p064, p035]</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[p021, p061, p042]</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   validation_subjects  train_accuracies  validation_accuracy\n",
       "0   [p026, p050, p002]             0.561                0.273\n",
       "1   [p052, p039, p024]             0.778                0.662\n",
       "2   [p049, p064, p042]             0.281                0.252\n",
       "3   [p051, p066, p014]             0.698                0.466\n",
       "4   [p021, p045, p056]             0.662                0.483\n",
       "5   [p042, p050, p049]             0.307                0.296\n",
       "6   [p002, p049, p045]             0.707                0.487\n",
       "7   [p061, p012, p041]             0.363                0.302\n",
       "8   [p026, p049, p015]             0.592                0.364\n",
       "9   [p052, p045, p051]             0.702                0.609\n",
       "10  [p045, p021, p016]             0.464                0.357\n",
       "11  [p022, p064, p035]             0.445                0.266\n",
       "12  [p021, p061, p042]             0.491                0.337"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1data.to_csv('../metrics/model1metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 - Add Dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 254, 254, 10)      280       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 254, 254, 10)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 25, 25, 10)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6250)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                62510     \n",
      "=================================================================\n",
      "Total params: 62,790\n",
      "Trainable params: 62,790\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Conv2D(10, kernel_size=3, activation='relu', input_shape=(256, 256, 3)))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(MaxPool2D(10))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "model2.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdgen = ImageDataGenerator(rescale=1./255)\n",
    "tedgen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV iteration 1 of 30\n",
      "Validation subjects are ['p026' 'p050' 'p002']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 211ms/step - loss: 2.2612 - accuracy: 0.1695 - val_loss: 2.3255 - val_accuracy: 0.1283\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.0558 - accuracy: 0.2860 - val_loss: 2.2887 - val_accuracy: 0.1425\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.8502 - accuracy: 0.4015 - val_loss: 2.2636 - val_accuracy: 0.1663\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.6384 - accuracy: 0.5070 - val_loss: 2.2852 - val_accuracy: 0.2050\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.4786 - accuracy: 0.5570 - val_loss: 2.2708 - val_accuracy: 0.2250\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.3146 - accuracy: 0.6295 - val_loss: 2.2767 - val_accuracy: 0.2537\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.1796 - accuracy: 0.6715 - val_loss: 2.2571 - val_accuracy: 0.2725\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.0625 - accuracy: 0.7075 - val_loss: 2.3291 - val_accuracy: 0.2529\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 0.9540 - accuracy: 0.7715 - val_loss: 2.3331 - val_accuracy: 0.2517\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 0.8487 - accuracy: 0.7870 - val_loss: 2.2580 - val_accuracy: 0.2825\n",
      "CV iteration 2 of 30\n",
      "Validation subjects are ['p052' 'p039' 'p024']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 2.3137 - accuracy: 0.1315 - val_loss: 2.2508 - val_accuracy: 0.1796\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 2.1552 - accuracy: 0.2285 - val_loss: 2.1205 - val_accuracy: 0.2537\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 210ms/step - loss: 2.0134 - accuracy: 0.3325 - val_loss: 2.0053 - val_accuracy: 0.4079\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 210ms/step - loss: 1.8966 - accuracy: 0.3965 - val_loss: 1.8935 - val_accuracy: 0.4267\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 210ms/step - loss: 1.7572 - accuracy: 0.4470 - val_loss: 1.8169 - val_accuracy: 0.4058\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 1.6509 - accuracy: 0.5070 - val_loss: 1.7383 - val_accuracy: 0.3758\n",
      "CV iteration 3 of 30\n",
      "Validation subjects are ['p049' 'p064' 'p042']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 2.2833 - accuracy: 0.1670 - val_loss: 2.2383 - val_accuracy: 0.1950\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 2.1174 - accuracy: 0.2735 - val_loss: 2.1594 - val_accuracy: 0.2129\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.9902 - accuracy: 0.3585 - val_loss: 2.0858 - val_accuracy: 0.2504\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.8285 - accuracy: 0.4180 - val_loss: 2.0244 - val_accuracy: 0.2454\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.6819 - accuracy: 0.4855 - val_loss: 1.9777 - val_accuracy: 0.3029\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.5463 - accuracy: 0.5325 - val_loss: 1.8587 - val_accuracy: 0.2912\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.4049 - accuracy: 0.5955 - val_loss: 1.8193 - val_accuracy: 0.3171\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.2606 - accuracy: 0.6610 - val_loss: 1.8160 - val_accuracy: 0.3067\n",
      "CV iteration 4 of 30\n",
      "Validation subjects are ['p051' 'p066' 'p014']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 2.3092 - accuracy: 0.1420 - val_loss: 2.2562 - val_accuracy: 0.1350\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 2.1249 - accuracy: 0.2425 - val_loss: 2.1963 - val_accuracy: 0.1771\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 1.9796 - accuracy: 0.3475 - val_loss: 2.1214 - val_accuracy: 0.2933\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 1.8206 - accuracy: 0.4365 - val_loss: 2.0637 - val_accuracy: 0.3033\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 1.6964 - accuracy: 0.4865 - val_loss: 1.9859 - val_accuracy: 0.3104\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.5205 - accuracy: 0.5645 - val_loss: 1.9200 - val_accuracy: 0.3587\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.3816 - accuracy: 0.6050 - val_loss: 1.8802 - val_accuracy: 0.3375\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.2215 - accuracy: 0.6840 - val_loss: 1.8412 - val_accuracy: 0.4187\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 1.1236 - accuracy: 0.6980 - val_loss: 1.8093 - val_accuracy: 0.4233\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.0323 - accuracy: 0.7245 - val_loss: 1.8026 - val_accuracy: 0.3933\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 0.9370 - accuracy: 0.7600 - val_loss: 1.7465 - val_accuracy: 0.3929\n",
      "CV iteration 5 of 30\n",
      "Validation subjects are ['p021' 'p045' 'p056']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.3196 - accuracy: 0.1475 - val_loss: 2.2666 - val_accuracy: 0.1688\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.1497 - accuracy: 0.2310 - val_loss: 2.1999 - val_accuracy: 0.2338\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.9971 - accuracy: 0.3535 - val_loss: 2.0968 - val_accuracy: 0.3304\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.8731 - accuracy: 0.4030 - val_loss: 2.0161 - val_accuracy: 0.3208\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.7451 - accuracy: 0.4605 - val_loss: 1.9289 - val_accuracy: 0.3929\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.6072 - accuracy: 0.5270 - val_loss: 1.8237 - val_accuracy: 0.4408\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.4831 - accuracy: 0.5630 - val_loss: 1.7383 - val_accuracy: 0.4871\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.3170 - accuracy: 0.6430 - val_loss: 1.6623 - val_accuracy: 0.5042\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.2283 - accuracy: 0.6585 - val_loss: 1.6155 - val_accuracy: 0.4721\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.0935 - accuracy: 0.7195 - val_loss: 1.5666 - val_accuracy: 0.4654\n",
      "CV iteration 6 of 30\n",
      "Validation subjects are ['p042' 'p050' 'p049']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.2867 - accuracy: 0.1575 - val_loss: 2.3088 - val_accuracy: 0.1171\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.1278 - accuracy: 0.2445 - val_loss: 2.2280 - val_accuracy: 0.1633\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.9915 - accuracy: 0.3430 - val_loss: 2.2010 - val_accuracy: 0.1600\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.8727 - accuracy: 0.4160 - val_loss: 2.1416 - val_accuracy: 0.1871\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.7121 - accuracy: 0.4825 - val_loss: 2.1667 - val_accuracy: 0.2208\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.5561 - accuracy: 0.5365 - val_loss: 2.0187 - val_accuracy: 0.2304\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.4054 - accuracy: 0.6035 - val_loss: 1.9995 - val_accuracy: 0.2700\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.2831 - accuracy: 0.6445 - val_loss: 2.0388 - val_accuracy: 0.2321\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.2019 - accuracy: 0.6635 - val_loss: 1.9619 - val_accuracy: 0.2433\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.0527 - accuracy: 0.7185 - val_loss: 2.0232 - val_accuracy: 0.2779\n",
      "CV iteration 7 of 30\n",
      "Validation subjects are ['p002' 'p049' 'p045']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.2952 - accuracy: 0.1535 - val_loss: 2.2735 - val_accuracy: 0.1083\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.1372 - accuracy: 0.2550 - val_loss: 2.2145 - val_accuracy: 0.2079\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.9689 - accuracy: 0.3465 - val_loss: 2.1537 - val_accuracy: 0.3208\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.8629 - accuracy: 0.3965 - val_loss: 2.0810 - val_accuracy: 0.3217\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.6892 - accuracy: 0.4775 - val_loss: 2.0284 - val_accuracy: 0.3967\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.5562 - accuracy: 0.5250 - val_loss: 2.0175 - val_accuracy: 0.3896\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.4495 - accuracy: 0.5675 - val_loss: 1.9465 - val_accuracy: 0.3708\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.2975 - accuracy: 0.6205 - val_loss: 1.8187 - val_accuracy: 0.4483\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.1596 - accuracy: 0.6945 - val_loss: 1.7387 - val_accuracy: 0.4892\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.0886 - accuracy: 0.7005 - val_loss: 1.8431 - val_accuracy: 0.4288\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 0.9963 - accuracy: 0.7350 - val_loss: 1.7354 - val_accuracy: 0.4604\n",
      "CV iteration 8 of 30\n",
      "Validation subjects are ['p061' 'p012' 'p041']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 27s 218ms/step - loss: 2.3105 - accuracy: 0.1475 - val_loss: 2.2508 - val_accuracy: 0.1612\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 27s 218ms/step - loss: 2.1383 - accuracy: 0.2545 - val_loss: 2.1824 - val_accuracy: 0.1629\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 27s 218ms/step - loss: 2.0073 - accuracy: 0.3345 - val_loss: 2.0771 - val_accuracy: 0.3512\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 27s 215ms/step - loss: 1.8722 - accuracy: 0.3945 - val_loss: 2.0514 - val_accuracy: 0.2992\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 27s 215ms/step - loss: 1.7132 - accuracy: 0.4660 - val_loss: 1.9601 - val_accuracy: 0.2554\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 27s 219ms/step - loss: 1.5581 - accuracy: 0.5535 - val_loss: 1.8717 - val_accuracy: 0.2988\n",
      "CV iteration 9 of 30\n",
      "Validation subjects are ['p026' 'p049' 'p015']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 2.2979 - accuracy: 0.1340 - val_loss: 2.2651 - val_accuracy: 0.1642\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.1217 - accuracy: 0.2565 - val_loss: 2.2003 - val_accuracy: 0.2837\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.9611 - accuracy: 0.3510 - val_loss: 2.1557 - val_accuracy: 0.2754\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.8114 - accuracy: 0.4095 - val_loss: 2.0946 - val_accuracy: 0.2692\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.6558 - accuracy: 0.5145 - val_loss: 2.0345 - val_accuracy: 0.2621\n",
      "CV iteration 10 of 30\n",
      "Validation subjects are ['p052' 'p045' 'p051']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 2.2912 - accuracy: 0.1615 - val_loss: 2.2366 - val_accuracy: 0.1958\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.1373 - accuracy: 0.2385 - val_loss: 2.1311 - val_accuracy: 0.2933\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 210ms/step - loss: 1.9972 - accuracy: 0.3270 - val_loss: 2.0319 - val_accuracy: 0.3738\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 1.8222 - accuracy: 0.4535 - val_loss: 1.9106 - val_accuracy: 0.4842\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.6882 - accuracy: 0.4795 - val_loss: 1.8072 - val_accuracy: 0.4529\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 1.5667 - accuracy: 0.5335 - val_loss: 1.7107 - val_accuracy: 0.5617\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 1.3874 - accuracy: 0.6280 - val_loss: 1.6885 - val_accuracy: 0.4021\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 26s 210ms/step - loss: 1.2710 - accuracy: 0.6485 - val_loss: 1.5041 - val_accuracy: 0.5850\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.1499 - accuracy: 0.7045 - val_loss: 1.4416 - val_accuracy: 0.5046\n",
      "CV iteration 11 of 30\n",
      "Validation subjects are ['p045' 'p021' 'p016']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 2.2982 - accuracy: 0.1440 - val_loss: 2.2515 - val_accuracy: 0.1400\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 2.1282 - accuracy: 0.2655 - val_loss: 2.1722 - val_accuracy: 0.2275\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 2.0026 - accuracy: 0.3240 - val_loss: 2.0761 - val_accuracy: 0.3575\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 1.8576 - accuracy: 0.4135 - val_loss: 2.0189 - val_accuracy: 0.2717\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.7183 - accuracy: 0.4610 - val_loss: 1.8724 - val_accuracy: 0.4638\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.6002 - accuracy: 0.5275 - val_loss: 1.7869 - val_accuracy: 0.4846\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.4339 - accuracy: 0.6165 - val_loss: 1.7123 - val_accuracy: 0.4729\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.3058 - accuracy: 0.6310 - val_loss: 1.6431 - val_accuracy: 0.4554\n",
      "CV iteration 12 of 30\n",
      "Validation subjects are ['p022' 'p064' 'p035']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 2.2919 - accuracy: 0.1555 - val_loss: 2.2235 - val_accuracy: 0.2029\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 2.1210 - accuracy: 0.2555 - val_loss: 2.1768 - val_accuracy: 0.2317\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.9824 - accuracy: 0.3510 - val_loss: 2.0991 - val_accuracy: 0.2400\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.8292 - accuracy: 0.4240 - val_loss: 2.0461 - val_accuracy: 0.2229\n",
      "CV iteration 13 of 30\n",
      "Validation subjects are ['p021' 'p061' 'p042']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 2.3195 - accuracy: 0.1310 - val_loss: 2.2549 - val_accuracy: 0.1571\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 2.1213 - accuracy: 0.2520 - val_loss: 2.1474 - val_accuracy: 0.2113\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.9816 - accuracy: 0.3675 - val_loss: 2.0784 - val_accuracy: 0.2237\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.8266 - accuracy: 0.4250 - val_loss: 1.9808 - val_accuracy: 0.2829\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.6757 - accuracy: 0.5015 - val_loss: 1.9396 - val_accuracy: 0.2433\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.4852 - accuracy: 0.5965 - val_loss: 1.8309 - val_accuracy: 0.3229\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.3467 - accuracy: 0.6225 - val_loss: 1.7334 - val_accuracy: 0.3946\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.2331 - accuracy: 0.6700 - val_loss: 1.7745 - val_accuracy: 0.3638\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.0991 - accuracy: 0.7200 - val_loss: 1.6865 - val_accuracy: 0.3262\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 0.9689 - accuracy: 0.7690 - val_loss: 1.6607 - val_accuracy: 0.3475\n",
      "CV iteration 14 of 30\n",
      "Validation subjects are ['p026' 'p075' 'p051']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.3136 - accuracy: 0.1400 - val_loss: 2.2391 - val_accuracy: 0.1704\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.1245 - accuracy: 0.2305 - val_loss: 2.1574 - val_accuracy: 0.2667\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.0023 - accuracy: 0.3150 - val_loss: 2.1329 - val_accuracy: 0.2533\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.8554 - accuracy: 0.4060 - val_loss: 2.0439 - val_accuracy: 0.3071\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.7349 - accuracy: 0.4445 - val_loss: 1.9931 - val_accuracy: 0.2908\n",
      "CV iteration 15 of 30\n",
      "Validation subjects are ['p024' 'p049' 'p052']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 2.2964 - accuracy: 0.1490 - val_loss: 2.2406 - val_accuracy: 0.2113\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 2.1242 - accuracy: 0.2600 - val_loss: 2.1265 - val_accuracy: 0.3379\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.9747 - accuracy: 0.3510 - val_loss: 2.0372 - val_accuracy: 0.3042\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 1.8574 - accuracy: 0.3960 - val_loss: 1.9144 - val_accuracy: 0.3654\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.7261 - accuracy: 0.4765 - val_loss: 1.7797 - val_accuracy: 0.5029\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.5601 - accuracy: 0.5400 - val_loss: 1.6985 - val_accuracy: 0.4800\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.3977 - accuracy: 0.6225 - val_loss: 1.6384 - val_accuracy: 0.4908\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.2929 - accuracy: 0.6340 - val_loss: 1.5416 - val_accuracy: 0.5454\n",
      "CV iteration 16 of 30\n",
      "Validation subjects are ['p045' 'p075' 'p049']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.2952 - accuracy: 0.1560 - val_loss: 2.2280 - val_accuracy: 0.2221\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.1468 - accuracy: 0.2450 - val_loss: 2.1580 - val_accuracy: 0.2321\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.9884 - accuracy: 0.3650 - val_loss: 2.0763 - val_accuracy: 0.3029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.8429 - accuracy: 0.4060 - val_loss: 2.0133 - val_accuracy: 0.2504\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.6968 - accuracy: 0.4720 - val_loss: 1.9239 - val_accuracy: 0.2721\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.5545 - accuracy: 0.5315 - val_loss: 1.8190 - val_accuracy: 0.4608\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.4136 - accuracy: 0.5940 - val_loss: 1.7565 - val_accuracy: 0.3879\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.2798 - accuracy: 0.6530 - val_loss: 1.7091 - val_accuracy: 0.4504\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.1419 - accuracy: 0.6925 - val_loss: 1.6874 - val_accuracy: 0.4017\n",
      "CV iteration 17 of 30\n",
      "Validation subjects are ['p016' 'p039' 'p075']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 210ms/step - loss: 2.3108 - accuracy: 0.1475 - val_loss: 2.2171 - val_accuracy: 0.2471\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 210ms/step - loss: 2.1435 - accuracy: 0.2370 - val_loss: 2.1547 - val_accuracy: 0.2371\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 2.0102 - accuracy: 0.3300 - val_loss: 2.0776 - val_accuracy: 0.3137\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 1.8665 - accuracy: 0.4035 - val_loss: 1.9965 - val_accuracy: 0.3371\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 1.7142 - accuracy: 0.4880 - val_loss: 1.9332 - val_accuracy: 0.3067\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 1.5605 - accuracy: 0.5230 - val_loss: 1.8816 - val_accuracy: 0.3229\n",
      "CV iteration 18 of 30\n",
      "Validation subjects are ['p052' 'p075' 'p072']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 2.2924 - accuracy: 0.1480 - val_loss: 2.2550 - val_accuracy: 0.1796\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.1354 - accuracy: 0.2565 - val_loss: 2.1629 - val_accuracy: 0.2700\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.9929 - accuracy: 0.3325 - val_loss: 2.1209 - val_accuracy: 0.2663\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.8121 - accuracy: 0.4375 - val_loss: 2.0517 - val_accuracy: 0.3167\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.6416 - accuracy: 0.5250 - val_loss: 2.0018 - val_accuracy: 0.2683\n",
      "CV iteration 19 of 30\n",
      "Validation subjects are ['p075' 'p049' 'p015']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.2999 - accuracy: 0.1480 - val_loss: 2.2381 - val_accuracy: 0.1875\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.1204 - accuracy: 0.2615 - val_loss: 2.1919 - val_accuracy: 0.2104\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 1.9602 - accuracy: 0.3690 - val_loss: 2.1576 - val_accuracy: 0.2517\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 1.8039 - accuracy: 0.4230 - val_loss: 2.1295 - val_accuracy: 0.1808\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.6143 - accuracy: 0.5230 - val_loss: 2.0483 - val_accuracy: 0.2842\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.4678 - accuracy: 0.5635 - val_loss: 2.0100 - val_accuracy: 0.2512\n",
      "CV iteration 20 of 30\n",
      "Validation subjects are ['p026' 'p024' 'p012']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 2.3040 - accuracy: 0.1290 - val_loss: 2.2522 - val_accuracy: 0.1646\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.1259 - accuracy: 0.2580 - val_loss: 2.1717 - val_accuracy: 0.1900\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.9804 - accuracy: 0.3290 - val_loss: 2.0865 - val_accuracy: 0.3279\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 1.8299 - accuracy: 0.4290 - val_loss: 2.0059 - val_accuracy: 0.3129\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.6703 - accuracy: 0.5030 - val_loss: 1.9330 - val_accuracy: 0.3700\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 1.5322 - accuracy: 0.5560 - val_loss: 1.8474 - val_accuracy: 0.3725\n",
      "CV iteration 21 of 30\n",
      "Validation subjects are ['p045' 'p021' 'p051']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 2.2910 - accuracy: 0.1535 - val_loss: 2.2262 - val_accuracy: 0.2346\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 2.1398 - accuracy: 0.2255 - val_loss: 2.1448 - val_accuracy: 0.3050\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 1.9914 - accuracy: 0.3475 - val_loss: 2.0537 - val_accuracy: 0.3283\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.8243 - accuracy: 0.4325 - val_loss: 1.9268 - val_accuracy: 0.4600\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.6883 - accuracy: 0.4815 - val_loss: 1.8146 - val_accuracy: 0.5475\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.5423 - accuracy: 0.5515 - val_loss: 1.7822 - val_accuracy: 0.4613\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.4079 - accuracy: 0.6000 - val_loss: 1.6556 - val_accuracy: 0.4762\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.2506 - accuracy: 0.6470 - val_loss: 1.6191 - val_accuracy: 0.4579\n",
      "CV iteration 22 of 30\n",
      "Validation subjects are ['p081' 'p026' 'p015']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.2833 - accuracy: 0.1490 - val_loss: 2.2704 - val_accuracy: 0.1508\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.1297 - accuracy: 0.2590 - val_loss: 2.2257 - val_accuracy: 0.1554\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.9640 - accuracy: 0.3780 - val_loss: 2.1842 - val_accuracy: 0.2208\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.8085 - accuracy: 0.4430 - val_loss: 2.1452 - val_accuracy: 0.2742\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 1.6329 - accuracy: 0.5295 - val_loss: 2.0954 - val_accuracy: 0.2546\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 210ms/step - loss: 1.4655 - accuracy: 0.5850 - val_loss: 2.0772 - val_accuracy: 0.2900\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 26s 211ms/step - loss: 1.3385 - accuracy: 0.6285 - val_loss: 2.0267 - val_accuracy: 0.3583\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 26s 211ms/step - loss: 1.1942 - accuracy: 0.6785 - val_loss: 2.0055 - val_accuracy: 0.2733\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 26s 211ms/step - loss: 1.1162 - accuracy: 0.6935 - val_loss: 1.9993 - val_accuracy: 0.2817\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 1.0038 - accuracy: 0.7415 - val_loss: 1.9866 - val_accuracy: 0.3137\n",
      "CV iteration 23 of 30\n",
      "Validation subjects are ['p056' 'p021' 'p081']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 205ms/step - loss: 2.2860 - accuracy: 0.1535 - val_loss: 2.3024 - val_accuracy: 0.1187\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 2.1177 - accuracy: 0.2530 - val_loss: 2.2214 - val_accuracy: 0.1475\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.9874 - accuracy: 0.3330 - val_loss: 2.1362 - val_accuracy: 0.2192\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.8557 - accuracy: 0.4020 - val_loss: 2.0705 - val_accuracy: 0.3179\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.7120 - accuracy: 0.4860 - val_loss: 1.9865 - val_accuracy: 0.3212\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.5494 - accuracy: 0.5570 - val_loss: 1.9084 - val_accuracy: 0.3775\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.4013 - accuracy: 0.6100 - val_loss: 1.9148 - val_accuracy: 0.3396\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.2827 - accuracy: 0.6410 - val_loss: 1.8173 - val_accuracy: 0.3621\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.1348 - accuracy: 0.7120 - val_loss: 1.7690 - val_accuracy: 0.3975\n",
      "CV iteration 24 of 30\n",
      "Validation subjects are ['p012' 'p081' 'p064']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 2.2935 - accuracy: 0.1460 - val_loss: 2.2651 - val_accuracy: 0.1529\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 2.1265 - accuracy: 0.2650 - val_loss: 2.1940 - val_accuracy: 0.2017\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.9749 - accuracy: 0.3550 - val_loss: 2.1309 - val_accuracy: 0.2633\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 1.8660 - accuracy: 0.4050 - val_loss: 2.1260 - val_accuracy: 0.1917\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.6792 - accuracy: 0.5055 - val_loss: 2.1241 - val_accuracy: 0.2013\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.5274 - accuracy: 0.5585 - val_loss: 1.9694 - val_accuracy: 0.3029\n",
      "CV iteration 25 of 30\n",
      "Validation subjects are ['p024' 'p002' 'p050']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 2.2950 - accuracy: 0.1360 - val_loss: 2.2815 - val_accuracy: 0.1683\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 2.1031 - accuracy: 0.2790 - val_loss: 2.2541 - val_accuracy: 0.2400\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 1.9732 - accuracy: 0.3590 - val_loss: 2.2001 - val_accuracy: 0.2917\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 1.7978 - accuracy: 0.4320 - val_loss: 2.1514 - val_accuracy: 0.3317\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.6126 - accuracy: 0.5270 - val_loss: 2.1322 - val_accuracy: 0.2742\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.4340 - accuracy: 0.6085 - val_loss: 2.1267 - val_accuracy: 0.2579\n",
      "CV iteration 26 of 30\n",
      "Validation subjects are ['p075' 'p072' 'p002']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 2.3143 - accuracy: 0.1400 - val_loss: 2.2662 - val_accuracy: 0.1617\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 2.1324 - accuracy: 0.2510 - val_loss: 2.2361 - val_accuracy: 0.1696\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.9580 - accuracy: 0.3430 - val_loss: 2.2584 - val_accuracy: 0.1388\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.7986 - accuracy: 0.4645 - val_loss: 2.2036 - val_accuracy: 0.1717\n",
      "CV iteration 27 of 30\n",
      "Validation subjects are ['p024' 'p045' 'p051']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.2967 - accuracy: 0.1405 - val_loss: 2.2344 - val_accuracy: 0.1567\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 2.1361 - accuracy: 0.2435 - val_loss: 2.1513 - val_accuracy: 0.2292\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 2.0126 - accuracy: 0.3195 - val_loss: 2.0208 - val_accuracy: 0.3058\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.8563 - accuracy: 0.4040 - val_loss: 1.8950 - val_accuracy: 0.4058\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.6936 - accuracy: 0.4750 - val_loss: 1.7722 - val_accuracy: 0.4700\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.5412 - accuracy: 0.5460 - val_loss: 1.6971 - val_accuracy: 0.4946\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.3988 - accuracy: 0.6030 - val_loss: 1.5748 - val_accuracy: 0.5163\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.2570 - accuracy: 0.6640 - val_loss: 1.5160 - val_accuracy: 0.5475\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.1393 - accuracy: 0.6975 - val_loss: 1.4068 - val_accuracy: 0.6246\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 26s 207ms/step - loss: 1.0283 - accuracy: 0.7470 - val_loss: 1.3898 - val_accuracy: 0.5775\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 0.9364 - accuracy: 0.7730 - val_loss: 1.3212 - val_accuracy: 0.6137\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 0.8382 - accuracy: 0.8060 - val_loss: 1.3684 - val_accuracy: 0.5267\n",
      "CV iteration 28 of 30\n",
      "Validation subjects are ['p049' 'p035' 'p022']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 2.3023 - accuracy: 0.1420 - val_loss: 2.2233 - val_accuracy: 0.1729\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 2.1482 - accuracy: 0.2230 - val_loss: 2.1557 - val_accuracy: 0.2542\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 2.0246 - accuracy: 0.3090 - val_loss: 2.0967 - val_accuracy: 0.1983\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 1.8928 - accuracy: 0.3720 - val_loss: 2.0189 - val_accuracy: 0.3025\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 1.7024 - accuracy: 0.4815 - val_loss: 1.9629 - val_accuracy: 0.3017\n",
      "CV iteration 29 of 30\n",
      "Validation subjects are ['p052' 'p064' 'p066']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.3004 - accuracy: 0.1520 - val_loss: 2.2189 - val_accuracy: 0.2029\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 2.1232 - accuracy: 0.2730 - val_loss: 2.1276 - val_accuracy: 0.2362\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.9760 - accuracy: 0.3700 - val_loss: 2.0562 - val_accuracy: 0.3158\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.8232 - accuracy: 0.4355 - val_loss: 1.9815 - val_accuracy: 0.3358\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.6648 - accuracy: 0.5075 - val_loss: 1.8995 - val_accuracy: 0.3413\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.5150 - accuracy: 0.5575 - val_loss: 1.8189 - val_accuracy: 0.3883\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.3568 - accuracy: 0.6200 - val_loss: 1.8037 - val_accuracy: 0.3804\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.2580 - accuracy: 0.6555 - val_loss: 1.7505 - val_accuracy: 0.3954\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.1307 - accuracy: 0.6860 - val_loss: 1.7417 - val_accuracy: 0.3771\n",
      "CV iteration 30 of 30\n",
      "Validation subjects are ['p035' 'p081' 'p075']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.3025 - accuracy: 0.1410 - val_loss: 2.2349 - val_accuracy: 0.1688\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.1359 - accuracy: 0.2515 - val_loss: 2.1548 - val_accuracy: 0.2167\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 2.0027 - accuracy: 0.3620 - val_loss: 2.1163 - val_accuracy: 0.2421\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.8246 - accuracy: 0.4415 - val_loss: 2.0324 - val_accuracy: 0.3179\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.6911 - accuracy: 0.5075 - val_loss: 1.9832 - val_accuracy: 0.2842\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.5322 - accuracy: 0.5530 - val_loss: 1.9713 - val_accuracy: 0.2692\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 1.4086 - accuracy: 0.6005 - val_loss: 1.8973 - val_accuracy: 0.3008\n"
     ]
    }
   ],
   "source": [
    "model2data = cvrand(model2, \n",
    "                    df,\n",
    "                    trdgen,\n",
    "                    tedgen,\n",
    "                    n_iterations=30,\n",
    "                    batch_size=16,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch=125,\n",
    "                    target_size=(256, 256),\n",
    "                    random_state=42,\n",
    "                    min_delta=0.05,\n",
    "                    patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>validation_subjects</th>\n",
       "      <th>train_accuracies</th>\n",
       "      <th>validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[p026, p050, p002]</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[p052, p039, p024]</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[p049, p064, p042]</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[p051, p066, p014]</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[p021, p045, p056]</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[p042, p050, p049]</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[p002, p049, p045]</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[p061, p012, p041]</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[p026, p049, p015]</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[p052, p045, p051]</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[p045, p021, p016]</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[p022, p064, p035]</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[p021, p061, p042]</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[p026, p075, p051]</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[p024, p049, p052]</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[p045, p075, p049]</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[p016, p039, p075]</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[p052, p075, p072]</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[p075, p049, p015]</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[p026, p024, p012]</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[p045, p021, p051]</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[p081, p026, p015]</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[p056, p021, p081]</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[p012, p081, p064]</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[p024, p002, p050]</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[p075, p072, p002]</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[p024, p045, p051]</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[p049, p035, p022]</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[p052, p064, p066]</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[p035, p081, p075]</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   validation_subjects  train_accuracies  validation_accuracy\n",
       "0   [p026, p050, p002]             0.787                0.282\n",
       "1   [p052, p039, p024]             0.396                0.427\n",
       "2   [p049, p064, p042]             0.596                0.317\n",
       "3   [p051, p066, p014]             0.698                0.423\n",
       "4   [p021, p045, p056]             0.643                0.504\n",
       "5   [p042, p050, p049]             0.718                0.278\n",
       "6   [p002, p049, p045]             0.694                0.489\n",
       "7   [p061, p012, p041]             0.334                0.351\n",
       "8   [p026, p049, p015]             0.256                0.284\n",
       "9   [p052, p045, p051]             0.648                0.585\n",
       "10  [p045, p021, p016]             0.528                0.485\n",
       "11  [p022, p064, p035]             0.351                0.240\n",
       "12  [p021, p061, p042]             0.622                0.395\n",
       "13  [p026, p075, p051]             0.406                0.307\n",
       "14  [p024, p049, p052]             0.634                0.545\n",
       "15  [p045, p075, p049]             0.532                0.461\n",
       "16  [p016, p039, p075]             0.404                0.337\n",
       "17  [p052, p075, p072]             0.438                0.317\n",
       "18  [p075, p049, p015]             0.523                0.284\n",
       "19  [p026, p024, p012]             0.556                0.372\n",
       "20  [p045, p021, p051]             0.482                0.548\n",
       "21  [p081, p026, p015]             0.628                0.358\n",
       "22  [p056, p021, p081]             0.712                0.398\n",
       "23  [p012, p081, p064]             0.558                0.303\n",
       "24  [p024, p002, p050]             0.432                0.332\n",
       "25  [p075, p072, p002]             0.464                0.172\n",
       "26  [p024, p045, p051]             0.698                0.625\n",
       "27  [p049, p035, p022]             0.372                0.302\n",
       "28  [p052, p064, p066]             0.656                0.395\n",
       "29  [p035, p081, p075]             0.442                0.318"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2data.to_csv('../metrics/model2metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 - Architecture Modeled off AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 55, 55, 99)        36036     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 27, 27, 99)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 27, 27, 256)       633856    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 384)       885120    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 13, 13, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 13, 13, 256)       884992    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 43264)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               4326500   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 8,105,102\n",
      "Trainable params: 8,105,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Conv2D(99,\n",
    "                 kernel_size=11,\n",
    "                 strides=4,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 input_shape=(227, 227, 3)))\n",
    "model3.add(MaxPool2D(3,\n",
    "                    strides=2,\n",
    "                    padding='valid'))\n",
    "model3.add(Conv2D(256,\n",
    "                 kernel_size=5,\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model3.add(MaxPool2D(3,\n",
    "                    strides=2,\n",
    "                    padding='valid'))\n",
    "model3.add(Conv2D(384,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model3.add(Conv2D(384,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model3.add(Conv2D(256,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(100, activation='relu'))\n",
    "model3.add(Dense(100, activation='relu'))\n",
    "model3.add(Dense(10, activation='softmax'))\n",
    "opt = Adam(learning_rate=0.00001)\n",
    "model3.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdgen = ImageDataGenerator(rescale=1./255)\n",
    "tedgen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV iteration 1 of 30\n",
      "Validation subjects are ['p026' 'p050' 'p002']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 26s 210ms/step - loss: 2.2960 - accuracy: 0.1390 - val_loss: 2.2983 - val_accuracy: 0.1304\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.1958 - accuracy: 0.2115 - val_loss: 2.3497 - val_accuracy: 0.0921\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 1.8044 - accuracy: 0.3415 - val_loss: 2.4959 - val_accuracy: 0.1267\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 204ms/step - loss: 1.5297 - accuracy: 0.4320 - val_loss: 2.3496 - val_accuracy: 0.2158\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 25s 204ms/step - loss: 1.3608 - accuracy: 0.5045 - val_loss: 2.3028 - val_accuracy: 0.2708\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 1.1754 - accuracy: 0.5830 - val_loss: 2.5009 - val_accuracy: 0.2850\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 1.0509 - accuracy: 0.6355 - val_loss: 2.6496 - val_accuracy: 0.2683\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.9191 - accuracy: 0.6685 - val_loss: 2.8201 - val_accuracy: 0.2854\n",
      "CV iteration 2 of 30\n",
      "Validation subjects are ['p052' 'p039' 'p024']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.3007 - accuracy: 0.1105 - val_loss: 2.2997 - val_accuracy: 0.1050\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2981 - accuracy: 0.1290 - val_loss: 2.2949 - val_accuracy: 0.1233u\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2913 - accuracy: 0.1840 - val_loss: 2.2843 - val_accuracy: 0.2679\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2767 - accuracy: 0.2235 - val_loss: 2.2663 - val_accuracy: 0.1421\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 2.2379 - accuracy: 0.2155 - val_loss: 2.1889 - val_accuracy: 0.2463\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 2.1210 - accuracy: 0.2460 - val_loss: 1.9517 - val_accuracy: 0.2937\n",
      "CV iteration 3 of 30\n",
      "Validation subjects are ['p049' 'p064' 'p042']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.3011 - accuracy: 0.1055 - val_loss: 2.2985 - val_accuracy: 0.1004\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2967 - accuracy: 0.1280 - val_loss: 2.2953 - val_accuracy: 0.1758\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2904 - accuracy: 0.1685 - val_loss: 2.2858 - val_accuracy: 0.1875\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2722 - accuracy: 0.2440 - val_loss: 2.2556 - val_accuracy: 0.1817\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2200 - accuracy: 0.2240 - val_loss: 2.1438 - val_accuracy: 0.2262\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 2.0269 - accuracy: 0.2780 - val_loss: 1.8855 - val_accuracy: 0.3083\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 1.8082 - accuracy: 0.3375 - val_loss: 1.8157 - val_accuracy: 0.3283\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 1.5991 - accuracy: 0.4160 - val_loss: 1.6151 - val_accuracy: 0.3562\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 26s 205ms/step - loss: 1.4468 - accuracy: 0.4810 - val_loss: 1.6891 - val_accuracy: 0.4167\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 1.3050 - accuracy: 0.5305 - val_loss: 1.5756 - val_accuracy: 0.4358\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 25s 204ms/step - loss: 1.1139 - accuracy: 0.6290 - val_loss: 1.5622 - val_accuracy: 0.4412\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.9658 - accuracy: 0.6840 - val_loss: 1.8069 - val_accuracy: 0.4075\n",
      "CV iteration 4 of 30\n",
      "Validation subjects are ['p051' 'p066' 'p014']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.3011 - accuracy: 0.1070 - val_loss: 2.3016 - val_accuracy: 0.0938\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2981 - accuracy: 0.1440 - val_loss: 2.2991 - val_accuracy: 0.0725\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2928 - accuracy: 0.1305 - val_loss: 2.2942 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2804 - accuracy: 0.1435 - val_loss: 2.2863 - val_accuracy: 0.1379\n",
      "CV iteration 5 of 30\n",
      "Validation subjects are ['p021' 'p045' 'p056']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.3016 - accuracy: 0.1020 - val_loss: 2.3005 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2989 - accuracy: 0.1155 - val_loss: 2.2975 - val_accuracy: 0.1350\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2955 - accuracy: 0.1455 - val_loss: 2.2934 - val_accuracy: 0.1600\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2874 - accuracy: 0.2185 - val_loss: 2.2853 - val_accuracy: 0.1067\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2736 - accuracy: 0.1585 - val_loss: 2.2594 - val_accuracy: 0.1688\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2317 - accuracy: 0.2020 - val_loss: 2.1858 - val_accuracy: 0.1871\n",
      "CV iteration 6 of 30\n",
      "Validation subjects are ['p042' 'p050' 'p049']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.3016 - accuracy: 0.1120 - val_loss: 2.2997 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2959 - accuracy: 0.1115 - val_loss: 2.2943 - val_accuracy: 0.1017\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2886 - accuracy: 0.1475 - val_loss: 2.2834 - val_accuracy: 0.1558\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2696 - accuracy: 0.2040 - val_loss: 2.2493 - val_accuracy: 0.1787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.1924 - accuracy: 0.2095 - val_loss: 2.0843 - val_accuracy: 0.2325\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 1.9390 - accuracy: 0.2985 - val_loss: 1.8787 - val_accuracy: 0.2679\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 1.6931 - accuracy: 0.3750 - val_loss: 1.8009 - val_accuracy: 0.3742\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 1.4505 - accuracy: 0.4785 - val_loss: 1.8372 - val_accuracy: 0.3529\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 1.2339 - accuracy: 0.5550 - val_loss: 1.9796 - val_accuracy: 0.3550\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 26s 204ms/step - loss: 1.0576 - accuracy: 0.6340 - val_loss: 2.2762 - val_accuracy: 0.4604\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 0.8829 - accuracy: 0.7145 - val_loss: 2.5214 - val_accuracy: 0.4225\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 0.7390 - accuracy: 0.7680 - val_loss: 2.4687 - val_accuracy: 0.3313\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 0.6252 - accuracy: 0.7955 - val_loss: 2.9002 - val_accuracy: 0.4412\n",
      "CV iteration 7 of 30\n",
      "Validation subjects are ['p002' 'p049' 'p045']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.3018 - accuracy: 0.1185 - val_loss: 2.3009 - val_accuracy: 0.1079\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2992 - accuracy: 0.1330 - val_loss: 2.2983 - val_accuracy: 0.1500\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2958 - accuracy: 0.1700 - val_loss: 2.2962 - val_accuracy: 0.1388\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2923 - accuracy: 0.1905 - val_loss: 2.2912 - val_accuracy: 0.1850\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2840 - accuracy: 0.1980 - val_loss: 2.2800 - val_accuracy: 0.2412\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2554 - accuracy: 0.1965 - val_loss: 2.2631 - val_accuracy: 0.1417\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.1902 - accuracy: 0.2125 - val_loss: 2.1966 - val_accuracy: 0.2154\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.0680 - accuracy: 0.2370 - val_loss: 2.2032 - val_accuracy: 0.2004\n",
      "CV iteration 8 of 30\n",
      "Validation subjects are ['p061' 'p012' 'p041']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.3007 - accuracy: 0.1070 - val_loss: 2.3000 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2979 - accuracy: 0.1015 - val_loss: 2.2960 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2923 - accuracy: 0.1125 - val_loss: 2.2895 - val_accuracy: 0.1608\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2785 - accuracy: 0.1695 - val_loss: 2.2734 - val_accuracy: 0.1637\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2522 - accuracy: 0.1900 - val_loss: 2.2203 - val_accuracy: 0.2175\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.1418 - accuracy: 0.2515 - val_loss: 2.0908 - val_accuracy: 0.2729\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 1.9502 - accuracy: 0.2940 - val_loss: 1.8401 - val_accuracy: 0.3325\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 1.6959 - accuracy: 0.3830 - val_loss: 1.7081 - val_accuracy: 0.3729\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 1.5545 - accuracy: 0.4400 - val_loss: 1.5790 - val_accuracy: 0.4042\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 1.3250 - accuracy: 0.5395 - val_loss: 1.5144 - val_accuracy: 0.4500\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 26s 205ms/step - loss: 1.1143 - accuracy: 0.6205 - val_loss: 1.5627 - val_accuracy: 0.4837\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 26s 204ms/step - loss: 1.0639 - accuracy: 0.6430 - val_loss: 1.4253 - val_accuracy: 0.5213\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.9001 - accuracy: 0.6955 - val_loss: 1.5243 - val_accuracy: 0.4863\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 0.7739 - accuracy: 0.7435 - val_loss: 1.5697 - val_accuracy: 0.5121\n",
      "CV iteration 9 of 30\n",
      "Validation subjects are ['p026' 'p049' 'p015']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.3021 - accuracy: 0.1070 - val_loss: 2.3013 - val_accuracy: 0.0917\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2997 - accuracy: 0.1340 - val_loss: 2.3001 - val_accuracy: 0.0854\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2968 - accuracy: 0.1480 - val_loss: 2.2983 - val_accuracy: 0.1508\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2928 - accuracy: 0.1780 - val_loss: 2.2957 - val_accuracy: 0.1942\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2863 - accuracy: 0.1715 - val_loss: 2.2896 - val_accuracy: 0.1446\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2672 - accuracy: 0.1945 - val_loss: 2.2725 - val_accuracy: 0.1604\n",
      "CV iteration 10 of 30\n",
      "Validation subjects are ['p052' 'p045' 'p051']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.3018 - accuracy: 0.1070 - val_loss: 2.2999 - val_accuracy: 0.2004\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 2.2980 - accuracy: 0.1305 - val_loss: 2.2957 - val_accuracy: 0.1246\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2920 - accuracy: 0.1435 - val_loss: 2.2884 - val_accuracy: 0.1342\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2834 - accuracy: 0.1425 - val_loss: 2.2684 - val_accuracy: 0.1625\n",
      "CV iteration 11 of 30\n",
      "Validation subjects are ['p045' 'p021' 'p016']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.3007 - accuracy: 0.1045 - val_loss: 2.2993 - val_accuracy: 0.1217\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2971 - accuracy: 0.1030 - val_loss: 2.2939 - val_accuracy: 0.1221\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2900 - accuracy: 0.1480 - val_loss: 2.2830 - val_accuracy: 0.0842\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2729 - accuracy: 0.1655 - val_loss: 2.2508 - val_accuracy: 0.2492\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2218 - accuracy: 0.1870 - val_loss: 2.0974 - val_accuracy: 0.2425\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.0176 - accuracy: 0.2835 - val_loss: 1.7698 - val_accuracy: 0.3154\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 1.8111 - accuracy: 0.3455 - val_loss: 1.6086 - val_accuracy: 0.3808\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 1.6224 - accuracy: 0.4085 - val_loss: 1.5148 - val_accuracy: 0.3950\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 1.4259 - accuracy: 0.4975 - val_loss: 1.4500 - val_accuracy: 0.4600\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 26s 204ms/step - loss: 1.2723 - accuracy: 0.5430 - val_loss: 1.2669 - val_accuracy: 0.5592\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 26s 205ms/step - loss: 1.1167 - accuracy: 0.6140 - val_loss: 1.2526 - val_accuracy: 0.5825\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.9351 - accuracy: 0.6730 - val_loss: 1.2548 - val_accuracy: 0.5625\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 26s 205ms/step - loss: 0.8133 - accuracy: 0.7435 - val_loss: 1.3174 - val_accuracy: 0.5883\n",
      "CV iteration 12 of 30\n",
      "Validation subjects are ['p022' 'p064' 'p035']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.3022 - accuracy: 0.0990 - val_loss: 2.3014 - val_accuracy: 0.0904\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2994 - accuracy: 0.1235 - val_loss: 2.3003 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2977 - accuracy: 0.1130 - val_loss: 2.2978 - val_accuracy: 0.1054\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2929 - accuracy: 0.1155 - val_loss: 2.2942 - val_accuracy: 0.0933\n",
      "CV iteration 13 of 30\n",
      "Validation subjects are ['p021' 'p061' 'p042']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.3016 - accuracy: 0.1015 - val_loss: 2.3001 - val_accuracy: 0.1271\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2990 - accuracy: 0.1200 - val_loss: 2.2968 - val_accuracy: 0.1046\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2945 - accuracy: 0.1260 - val_loss: 2.2909 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2882 - accuracy: 0.1195 - val_loss: 2.2802 - val_accuracy: 0.1321\n",
      "CV iteration 14 of 30\n",
      "Validation subjects are ['p026' 'p075' 'p051']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.3015 - accuracy: 0.1040 - val_loss: 2.3006 - val_accuracy: 0.1287\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2976 - accuracy: 0.1670 - val_loss: 2.2986 - val_accuracy: 0.1513\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2920 - accuracy: 0.1590 - val_loss: 2.2930 - val_accuracy: 0.1033\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2771 - accuracy: 0.1925 - val_loss: 2.2789 - val_accuracy: 0.1333\n",
      "CV iteration 15 of 30\n",
      "Validation subjects are ['p024' 'p049' 'p052']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.3011 - accuracy: 0.1045 - val_loss: 2.2987 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2978 - accuracy: 0.1260 - val_loss: 2.2934 - val_accuracy: 0.1771\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2898 - accuracy: 0.1655 - val_loss: 2.2814 - val_accuracy: 0.1333\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2677 - accuracy: 0.1905 - val_loss: 2.2307 - val_accuracy: 0.2408\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.1810 - accuracy: 0.1985 - val_loss: 2.0373 - val_accuracy: 0.2604\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 1.9428 - accuracy: 0.2880 - val_loss: 1.7704 - val_accuracy: 0.3812\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 1.6820 - accuracy: 0.3850 - val_loss: 1.5276 - val_accuracy: 0.4850\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 1.4685 - accuracy: 0.4660 - val_loss: 1.5018 - val_accuracy: 0.4617\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 1.2792 - accuracy: 0.5450 - val_loss: 1.3820 - val_accuracy: 0.4817\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 1.1519 - accuracy: 0.5860 - val_loss: 1.3393 - val_accuracy: 0.5567\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 26s 205ms/step - loss: 1.0270 - accuracy: 0.6585 - val_loss: 1.2934 - val_accuracy: 0.5929\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 0.8854 - accuracy: 0.7060 - val_loss: 1.2598 - val_accuracy: 0.5663\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 0.7635 - accuracy: 0.7645 - val_loss: 1.3575 - val_accuracy: 0.5633\n",
      "CV iteration 16 of 30\n",
      "Validation subjects are ['p045' 'p075' 'p049']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.3021 - accuracy: 0.0980 - val_loss: 2.3012 - val_accuracy: 0.1029\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2998 - accuracy: 0.1160 - val_loss: 2.2998 - val_accuracy: 0.1138\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2979 - accuracy: 0.1605 - val_loss: 2.2988 - val_accuracy: 0.1842\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2940 - accuracy: 0.1840 - val_loss: 2.2943 - val_accuracy: 0.1446\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2879 - accuracy: 0.1640 - val_loss: 2.2863 - val_accuracy: 0.1529\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2695 - accuracy: 0.1780 - val_loss: 2.2622 - val_accuracy: 0.1608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV iteration 17 of 30\n",
      "Validation subjects are ['p016' 'p039' 'p075']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 2.3004 - accuracy: 0.1240 - val_loss: 2.3005 - val_accuracy: 0.1371\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2982 - accuracy: 0.1435 - val_loss: 2.2980 - val_accuracy: 0.1267\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2933 - accuracy: 0.1665 - val_loss: 2.2945 - val_accuracy: 0.1625\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2816 - accuracy: 0.2195 - val_loss: 2.2863 - val_accuracy: 0.1508\n",
      "CV iteration 18 of 30\n",
      "Validation subjects are ['p052' 'p075' 'p072']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.3014 - accuracy: 0.0980 - val_loss: 2.3006 - val_accuracy: 0.1054\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 2.2974 - accuracy: 0.1310 - val_loss: 2.2976 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 2.2907 - accuracy: 0.1430 - val_loss: 2.2935 - val_accuracy: 0.1458\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2774 - accuracy: 0.1640 - val_loss: 2.2810 - val_accuracy: 0.1000\n",
      "CV iteration 19 of 30\n",
      "Validation subjects are ['p075' 'p049' 'p015']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.3013 - accuracy: 0.1025 - val_loss: 2.3002 - val_accuracy: 0.1171\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2968 - accuracy: 0.1355 - val_loss: 2.2973 - val_accuracy: 0.1475\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2888 - accuracy: 0.1890 - val_loss: 2.2912 - val_accuracy: 0.1175\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2745 - accuracy: 0.1700 - val_loss: 2.2705 - val_accuracy: 0.1792\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.1920 - accuracy: 0.2345 - val_loss: 2.1879 - val_accuracy: 0.1617\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 1.9530 - accuracy: 0.2860 - val_loss: 2.0869 - val_accuracy: 0.2637\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 1.7077 - accuracy: 0.3560 - val_loss: 2.0071 - val_accuracy: 0.3067\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 1.4698 - accuracy: 0.4705 - val_loss: 2.1608 - val_accuracy: 0.3429\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 1.3038 - accuracy: 0.5410 - val_loss: 2.2529 - val_accuracy: 0.3283\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 1.1145 - accuracy: 0.6200 - val_loss: 2.1672 - val_accuracy: 0.3113\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 0.9506 - accuracy: 0.6830 - val_loss: 2.2998 - val_accuracy: 0.3758\n",
      "CV iteration 20 of 30\n",
      "Validation subjects are ['p026' 'p024' 'p012']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.3009 - accuracy: 0.1155 - val_loss: 2.2999 - val_accuracy: 0.1262\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2979 - accuracy: 0.1520 - val_loss: 2.2968 - val_accuracy: 0.1071\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2931 - accuracy: 0.1420 - val_loss: 2.2936 - val_accuracy: 0.1013\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2878 - accuracy: 0.1715 - val_loss: 2.2844 - val_accuracy: 0.1304\n",
      "CV iteration 21 of 30\n",
      "Validation subjects are ['p045' 'p021' 'p051']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.3012 - accuracy: 0.1100 - val_loss: 2.2995 - val_accuracy: 0.1346\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2975 - accuracy: 0.1345 - val_loss: 2.2950 - val_accuracy: 0.2396\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2919 - accuracy: 0.1810 - val_loss: 2.2869 - val_accuracy: 0.1937\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2809 - accuracy: 0.1855 - val_loss: 2.2676 - val_accuracy: 0.1908\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2521 - accuracy: 0.1815 - val_loss: 2.1764 - val_accuracy: 0.2546\n",
      "CV iteration 22 of 30\n",
      "Validation subjects are ['p081' 'p026' 'p015']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.3006 - accuracy: 0.1395 - val_loss: 2.2997 - val_accuracy: 0.1275\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 2.2952 - accuracy: 0.1550 - val_loss: 2.2966 - val_accuracy: 0.1408\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2841 - accuracy: 0.1835 - val_loss: 2.2865 - val_accuracy: 0.1246\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 2.2467 - accuracy: 0.2275 - val_loss: 2.2404 - val_accuracy: 0.1800\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 2.0775 - accuracy: 0.2630 - val_loss: 2.0728 - val_accuracy: 0.2113\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 1.8043 - accuracy: 0.3385 - val_loss: 1.9538 - val_accuracy: 0.3338\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 1.5855 - accuracy: 0.4215 - val_loss: 2.1198 - val_accuracy: 0.3475\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 1.3565 - accuracy: 0.5165 - val_loss: 1.9805 - val_accuracy: 0.2637\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 1.2049 - accuracy: 0.5730 - val_loss: 2.0909 - val_accuracy: 0.3525\n",
      "CV iteration 23 of 30\n",
      "Validation subjects are ['p056' 'p021' 'p081']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.3015 - accuracy: 0.1090 - val_loss: 2.3011 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2986 - accuracy: 0.1125 - val_loss: 2.2990 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2944 - accuracy: 0.1065 - val_loss: 2.2966 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2874 - accuracy: 0.1190 - val_loss: 2.2890 - val_accuracy: 0.1258\n",
      "CV iteration 24 of 30\n",
      "Validation subjects are ['p012' 'p081' 'p064']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.3016 - accuracy: 0.1155 - val_loss: 2.3011 - val_accuracy: 0.1037\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2980 - accuracy: 0.1675 - val_loss: 2.2987 - val_accuracy: 0.1446\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2916 - accuracy: 0.1540 - val_loss: 2.2950 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2811 - accuracy: 0.1440 - val_loss: 2.2843 - val_accuracy: 0.1392\n",
      "CV iteration 25 of 30\n",
      "Validation subjects are ['p024' 'p002' 'p050']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.3004 - accuracy: 0.1255 - val_loss: 2.3005 - val_accuracy: 0.0958\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2962 - accuracy: 0.1485 - val_loss: 2.2977 - val_accuracy: 0.0979\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2857 - accuracy: 0.1520 - val_loss: 2.2890 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2605 - accuracy: 0.1560 - val_loss: 2.2687 - val_accuracy: 0.1096\n",
      "CV iteration 26 of 30\n",
      "Validation subjects are ['p075' 'p072' 'p002']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.3006 - accuracy: 0.1240 - val_loss: 2.3011 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 2.2951 - accuracy: 0.1305 - val_loss: 2.2989 - val_accuracy: 0.1033\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2790 - accuracy: 0.1885 - val_loss: 2.2954 - val_accuracy: 0.1133\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2321 - accuracy: 0.1855 - val_loss: 2.2914 - val_accuracy: 0.1692\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 1.9630 - accuracy: 0.2830 - val_loss: 2.4798 - val_accuracy: 0.1233\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 1.6680 - accuracy: 0.3925 - val_loss: 2.4650 - val_accuracy: 0.2083\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 1.3871 - accuracy: 0.5155 - val_loss: 2.5384 - val_accuracy: 0.2175\n",
      "CV iteration 27 of 30\n",
      "Validation subjects are ['p024' 'p045' 'p051']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.3012 - accuracy: 0.1130 - val_loss: 2.2999 - val_accuracy: 0.1654\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2980 - accuracy: 0.1595 - val_loss: 2.2976 - val_accuracy: 0.1458\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2957 - accuracy: 0.1420 - val_loss: 2.2927 - val_accuracy: 0.1637\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2902 - accuracy: 0.1760 - val_loss: 2.2816 - val_accuracy: 0.2442\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2685 - accuracy: 0.2135 - val_loss: 2.2434 - val_accuracy: 0.2479acy: 0.\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2053 - accuracy: 0.2165 - val_loss: 2.0690 - val_accuracy: 0.2379\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.0725 - accuracy: 0.2490 - val_loss: 1.8362 - val_accuracy: 0.3204\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 1.7708 - accuracy: 0.3520 - val_loss: 1.5346 - val_accuracy: 0.4775\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 1.5743 - accuracy: 0.4220 - val_loss: 1.3538 - val_accuracy: 0.57581\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 1.3960 - accuracy: 0.5100 - val_loss: 1.2779 - val_accuracy: 0.5096\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 1.1822 - accuracy: 0.5720 - val_loss: 1.2720 - val_accuracy: 0.4846\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 26s 204ms/step - loss: 1.0217 - accuracy: 0.6485 - val_loss: 1.0957 - val_accuracy: 0.6546\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 0.8754 - accuracy: 0.7010 - val_loss: 1.1944 - val_accuracy: 0.5429\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 26s 204ms/step - loss: 0.7204 - accuracy: 0.7685 - val_loss: 1.0043 - val_accuracy: 0.6725\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 26s 205ms/step - loss: 0.6045 - accuracy: 0.8130 - val_loss: 1.0852 - val_accuracy: 0.6938\n",
      "CV iteration 28 of 30\n",
      "Validation subjects are ['p049' 'p035' 'p022']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 2.3010 - accuracy: 0.1105 - val_loss: 2.3009 - val_accuracy: 0.1187\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 2.2998 - accuracy: 0.1185 - val_loss: 2.2999 - val_accuracy: 0.0996\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 2.2949 - accuracy: 0.1210 - val_loss: 2.2979 - val_accuracy: 0.1271\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 199ms/step - loss: 2.2945 - accuracy: 0.1225 - val_loss: 2.2931 - val_accuracy: 0.1000\n",
      "CV iteration 29 of 30\n",
      "Validation subjects are ['p052' 'p064' 'p066']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.3009 - accuracy: 0.1135 - val_loss: 2.3008 - val_accuracy: 0.1421\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2983 - accuracy: 0.1220 - val_loss: 2.2991 - val_accuracy: 0.1092\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2955 - accuracy: 0.1390 - val_loss: 2.2950 - val_accuracy: 0.1158\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 200ms/step - loss: 2.2883 - accuracy: 0.1450 - val_loss: 2.2860 - val_accuracy: 0.1392\n",
      "CV iteration 30 of 30\n",
      "Validation subjects are ['p035' 'p081' 'p075']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.3010 - accuracy: 0.1165 - val_loss: 2.2985 - val_accuracy: 0.1183\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2957 - accuracy: 0.1650 - val_loss: 2.2942 - val_accuracy: 0.1612\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 2.2842 - accuracy: 0.1775 - val_loss: 2.2817 - val_accuracy: 0.2354\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.2487 - accuracy: 0.1900 - val_loss: 2.2456 - val_accuracy: 0.1346\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 2.1338 - accuracy: 0.2115 - val_loss: 2.1215 - val_accuracy: 0.1783\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 25s 201ms/step - loss: 1.8771 - accuracy: 0.3060 - val_loss: 2.0491 - val_accuracy: 0.2587\n"
     ]
    }
   ],
   "source": [
    "model3data = cvrand(model3, \n",
    "                    df,\n",
    "                    trdgen,\n",
    "                    tedgen,\n",
    "                    n_iterations=30,\n",
    "                    batch_size=16,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch=125,\n",
    "                    target_size=(227,227),\n",
    "                    random_state=42,\n",
    "                    min_delta=0.05,\n",
    "                    patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>validation_subjects</th>\n",
       "      <th>train_accuracies</th>\n",
       "      <th>validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[p026, p050, p002]</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[p052, p039, p024]</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[p049, p064, p042]</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[p051, p066, p014]</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[p021, p045, p056]</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[p042, p050, p049]</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[p002, p049, p045]</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[p061, p012, p041]</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[p026, p049, p015]</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[p052, p045, p051]</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[p045, p021, p016]</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[p022, p064, p035]</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[p021, p061, p042]</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[p026, p075, p051]</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[p024, p049, p052]</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[p045, p075, p049]</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[p016, p039, p075]</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[p052, p075, p072]</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[p075, p049, p015]</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[p026, p024, p012]</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[p045, p021, p051]</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[p081, p026, p015]</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[p056, p021, p081]</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[p012, p081, p064]</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[p024, p002, p050]</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[p075, p072, p002]</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[p024, p045, p051]</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[p049, p035, p022]</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[p052, p064, p066]</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[p035, p081, p075]</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   validation_subjects  train_accuracies  validation_accuracy\n",
       "0   [p026, p050, p002]             0.668                0.285\n",
       "1   [p052, p039, p024]             0.246                0.294\n",
       "2   [p049, p064, p042]             0.629                0.441\n",
       "3   [p051, p066, p014]             0.144                0.138\n",
       "4   [p021, p045, p056]             0.202                0.187\n",
       "5   [p042, p050, p049]             0.634                0.460\n",
       "6   [p002, p049, p045]             0.198                0.241\n",
       "7   [p061, p012, p041]             0.643                0.521\n",
       "8   [p026, p049, p015]             0.178                0.194\n",
       "9   [p052, p045, p051]             0.107                0.200\n",
       "10  [p045, p021, p016]             0.744                0.588\n",
       "11  [p022, p064, p035]             0.113                0.105\n",
       "12  [p021, p061, p042]             0.120                0.132\n",
       "13  [p026, p075, p051]             0.167                0.151\n",
       "14  [p024, p049, p052]             0.658                0.593\n",
       "15  [p045, p075, p049]             0.160                0.184\n",
       "16  [p016, p039, p075]             0.166                0.162\n",
       "17  [p052, p075, p072]             0.143                0.146\n",
       "18  [p075, p049, p015]             0.683                0.376\n",
       "19  [p026, p024, p012]             0.172                0.130\n",
       "20  [p045, p021, p051]             0.182                0.255\n",
       "21  [p081, p026, p015]             0.573                0.352\n",
       "22  [p056, p021, p081]             0.119                0.126\n",
       "23  [p012, p081, p064]             0.168                0.145\n",
       "24  [p024, p002, p050]             0.156                0.110\n",
       "25  [p075, p072, p002]             0.516                0.218\n",
       "26  [p024, p045, p051]             0.813                0.694\n",
       "27  [p049, p035, p022]             0.121                0.127\n",
       "28  [p052, p064, p066]             0.114                0.142\n",
       "29  [p035, p081, p075]             0.306                0.259"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3data.to_csv('../metrics/model3metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet Corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Conv2D(99,\n",
    "                  kernel_size=11,\n",
    "                  strides=4,\n",
    "                  padding='valid',\n",
    "                  input_shape=(227, 227, 3)))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(MaxPool2D(3,\n",
    "                     strides=2,\n",
    "                     padding='valid'))\n",
    "model4.add(Conv2D(256,\n",
    "                  kernel_size=5,\n",
    "                  strides=1,\n",
    "                  padding='same'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(MaxPool2D(3,\n",
    "                    strides=2,\n",
    "                    padding='valid'))\n",
    "model4.add(Conv2D(384,\n",
    "                  kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(Conv2D(384,\n",
    "                  kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(Conv2D(256,\n",
    "                  kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(MaxPool2D(3,\n",
    "                     strides=2,\n",
    "                     padding='valid'))\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(4096, activation='relu'))\n",
    "model4.add(Dropout(0.5))\n",
    "model4.add(Dense(4096, activation='relu'))\n",
    "model4.add(Dropout(0.5))\n",
    "model4.add(Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 55, 55, 99)        36036     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 55, 55, 99)        396       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 55, 55, 99)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 27, 27, 99)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 27, 27, 256)       633856    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 27, 27, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 27, 27, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 13, 13, 384)       885120    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 13, 13, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 13, 13, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 13, 13, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 13, 13, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 13, 13, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 13, 13, 256)       884992    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 13, 13, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4096)              37752832  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 58,348,122\n",
      "Trainable params: 58,345,364\n",
      "Non-trainable params: 2,758\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(learning_rate=0.00001)\n",
    "model4.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a batch size of 16 and training set consisting of 18,400 images, performing 50 epochs of 115 steps will mean that the training data is gone over 5 times. Early stopping callback is set to 10, so if the validation accuracy does not improve 10 times in a row then the training will cease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdgen = ImageDataGenerator(rescale=1./255)\n",
    "tedgen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV iteration 1 of 13\n",
      "Validation subjects are ['p026' 'p050' 'p002']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 30s 262ms/step - loss: 3.2027 - accuracy: 0.1408 - val_loss: 2.3582 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 29s 249ms/step - loss: 2.3792 - accuracy: 0.2353 - val_loss: 2.3696 - val_accuracy: 0.1008\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.9414 - accuracy: 0.3201 - val_loss: 2.4820 - val_accuracy: 0.0979\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 1.6719 - accuracy: 0.4364 - val_loss: 2.3349 - val_accuracy: 0.0825\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 29s 249ms/step - loss: 1.4972 - accuracy: 0.4663 - val_loss: 2.2403 - val_accuracy: 0.1692\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 29s 252ms/step - loss: 1.3143 - accuracy: 0.5489 - val_loss: 2.1444 - val_accuracy: 0.2429\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 29s 255ms/step - loss: 1.1103 - accuracy: 0.6245 - val_loss: 2.0374 - val_accuracy: 0.3583\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 29s 250ms/step - loss: 1.0568 - accuracy: 0.6321 - val_loss: 1.9475 - val_accuracy: 0.3679\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.8580 - accuracy: 0.7158 - val_loss: 1.9717 - val_accuracy: 0.3200\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 29s 253ms/step - loss: 0.7841 - accuracy: 0.7283 - val_loss: 1.8242 - val_accuracy: 0.4104\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.6463 - accuracy: 0.7989 - val_loss: 2.0038 - val_accuracy: 0.3692\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 0.6609 - accuracy: 0.7826 - val_loss: 1.9031 - val_accuracy: 0.3821\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.5141 - accuracy: 0.8380 - val_loss: 1.8049 - val_accuracy: 0.4033\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.4709 - accuracy: 0.8522 - val_loss: 2.0110 - val_accuracy: 0.3992\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 29s 252ms/step - loss: 0.4611 - accuracy: 0.8560 - val_loss: 1.7432 - val_accuracy: 0.4396\n",
      "CV iteration 2 of 13\n",
      "Validation subjects are ['p052' 'p039' 'p024']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 3.2742 - accuracy: 0.1397 - val_loss: 2.3313 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 213ms/step - loss: 2.3025 - accuracy: 0.2174 - val_loss: 2.4152 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 213ms/step - loss: 1.9388 - accuracy: 0.3304 - val_loss: 2.2908 - val_accuracy: 0.1292\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 213ms/step - loss: 1.6997 - accuracy: 0.3984 - val_loss: 2.0753 - val_accuracy: 0.2117\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 29s 252ms/step - loss: 1.4689 - accuracy: 0.4880 - val_loss: 1.6195 - val_accuracy: 0.4733\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 29s 256ms/step - loss: 1.3306 - accuracy: 0.5543 - val_loss: 1.3372 - val_accuracy: 0.6087\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 29s 254ms/step - loss: 1.1400 - accuracy: 0.6141 - val_loss: 1.1839 - val_accuracy: 0.6458\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 30s 258ms/step - loss: 0.9763 - accuracy: 0.6712 - val_loss: 1.1249 - val_accuracy: 0.6658\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 0.8173 - accuracy: 0.7337 - val_loss: 1.0650 - val_accuracy: 0.6617\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 0.7857 - accuracy: 0.7370 - val_loss: 0.9838 - val_accuracy: 0.6483\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 24s 213ms/step - loss: 0.6588 - accuracy: 0.7902 - val_loss: 1.0323 - val_accuracy: 0.6192\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 31s 266ms/step - loss: 0.5849 - accuracy: 0.8130 - val_loss: 0.8332 - val_accuracy: 0.6846\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 25s 213ms/step - loss: 0.5199 - accuracy: 0.8457 - val_loss: 0.9663 - val_accuracy: 0.6396\n",
      "CV iteration 3 of 13\n",
      "Validation subjects are ['p049' 'p064' 'p042']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 3.2419 - accuracy: 0.1386 - val_loss: 2.3263 - val_accuracy: 0.1367\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 2.2774 - accuracy: 0.2326 - val_loss: 2.3301 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.8580 - accuracy: 0.3495 - val_loss: 2.3687 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.6831 - accuracy: 0.4239 - val_loss: 2.2048 - val_accuracy: 0.1992\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.3994 - accuracy: 0.5190 - val_loss: 1.6729 - val_accuracy: 0.4204\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.2663 - accuracy: 0.5728 - val_loss: 1.4138 - val_accuracy: 0.4825\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.1379 - accuracy: 0.6114 - val_loss: 1.2692 - val_accuracy: 0.5667\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.9522 - accuracy: 0.6804 - val_loss: 1.2062 - val_accuracy: 0.5767\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.8433 - accuracy: 0.7179 - val_loss: 1.0948 - val_accuracy: 0.5983\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.7120 - accuracy: 0.7734 - val_loss: 1.0056 - val_accuracy: 0.6400\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.6543 - accuracy: 0.7897 - val_loss: 1.0529 - val_accuracy: 0.6400\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.5426 - accuracy: 0.8299 - val_loss: 0.9879 - val_accuracy: 0.6646\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.4744 - accuracy: 0.8543 - val_loss: 1.0422 - val_accuracy: 0.6662\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.4239 - accuracy: 0.8690 - val_loss: 1.1430 - val_accuracy: 0.6187\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 30s 258ms/step - loss: 0.3523 - accuracy: 0.8973 - val_loss: 0.7921 - val_accuracy: 0.7175\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.3605 - accuracy: 0.8886 - val_loss: 1.0071 - val_accuracy: 0.6396\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.2852 - accuracy: 0.9185 - val_loss: 0.8370 - val_accuracy: 0.7083\n",
      "Epoch 18/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.2606 - accuracy: 0.9315 - val_loss: 1.0333 - val_accuracy: 0.6233\n",
      "Epoch 19/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.2817 - accuracy: 0.9223 - val_loss: 0.9855 - val_accuracy: 0.6775\n",
      "Epoch 20/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.2173 - accuracy: 0.9315 - val_loss: 1.2124 - val_accuracy: 0.6121\n",
      "CV iteration 4 of 13\n",
      "Validation subjects are ['p051' 'p066' 'p014']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 3.2385 - accuracy: 0.1397 - val_loss: 2.3273 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 2.2214 - accuracy: 0.2402 - val_loss: 2.4327 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.8644 - accuracy: 0.3576 - val_loss: 2.4432 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.6345 - accuracy: 0.4283 - val_loss: 2.2130 - val_accuracy: 0.1079\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.3816 - accuracy: 0.5109 - val_loss: 1.7517 - val_accuracy: 0.3900\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.2020 - accuracy: 0.5842 - val_loss: 1.5230 - val_accuracy: 0.4521\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.0609 - accuracy: 0.6212 - val_loss: 1.4303 - val_accuracy: 0.5225\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.9278 - accuracy: 0.6853 - val_loss: 1.3077 - val_accuracy: 0.5667\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 0.7813 - accuracy: 0.7511 - val_loss: 1.4075 - val_accuracy: 0.4850\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.7173 - accuracy: 0.7641 - val_loss: 1.2036 - val_accuracy: 0.5500\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.6134 - accuracy: 0.8033 - val_loss: 1.1559 - val_accuracy: 0.5867\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 0.5136 - accuracy: 0.8337 - val_loss: 1.0692 - val_accuracy: 0.6796\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.4460 - accuracy: 0.8625 - val_loss: 1.0991 - val_accuracy: 0.6229\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.4058 - accuracy: 0.8750 - val_loss: 1.2329 - val_accuracy: 0.5938\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.3406 - accuracy: 0.9000 - val_loss: 1.1257 - val_accuracy: 0.6575\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.3116 - accuracy: 0.9109 - val_loss: 1.0673 - val_accuracy: 0.6062\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.2299 - accuracy: 0.9364 - val_loss: 1.1424 - val_accuracy: 0.6150\n",
      "CV iteration 5 of 13\n",
      "Validation subjects are ['p021' 'p045' 'p056']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 3.2529 - accuracy: 0.1603 - val_loss: 2.3369 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 2.2494 - accuracy: 0.2402 - val_loss: 2.3704 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 1.8256 - accuracy: 0.3625 - val_loss: 2.3685 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 1.6165 - accuracy: 0.4299 - val_loss: 2.1779 - val_accuracy: 0.1754\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.3658 - accuracy: 0.5272 - val_loss: 1.7823 - val_accuracy: 0.3242\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.2224 - accuracy: 0.5804 - val_loss: 1.5555 - val_accuracy: 0.4825\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.0787 - accuracy: 0.6370 - val_loss: 1.4220 - val_accuracy: 0.5163\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.9232 - accuracy: 0.6908 - val_loss: 1.4043 - val_accuracy: 0.4712\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.7584 - accuracy: 0.7614 - val_loss: 1.3364 - val_accuracy: 0.5267\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.6650 - accuracy: 0.7821 - val_loss: 1.1685 - val_accuracy: 0.6292\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.5815 - accuracy: 0.8152 - val_loss: 1.1756 - val_accuracy: 0.6242\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.4978 - accuracy: 0.8489 - val_loss: 1.1335 - val_accuracy: 0.5996\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.4241 - accuracy: 0.8815 - val_loss: 1.2446 - val_accuracy: 0.5475\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.3863 - accuracy: 0.8788 - val_loss: 1.2863 - val_accuracy: 0.5508\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.3337 - accuracy: 0.8984 - val_loss: 1.1788 - val_accuracy: 0.5983\n",
      "CV iteration 6 of 13\n",
      "Validation subjects are ['p042' 'p050' 'p049']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 3.2397 - accuracy: 0.1549 - val_loss: 2.3443 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 2.2509 - accuracy: 0.2560 - val_loss: 2.3241 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 1.8609 - accuracy: 0.3440 - val_loss: 2.4766 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.6052 - accuracy: 0.4429 - val_loss: 2.4019 - val_accuracy: 0.0954\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 1.3702 - accuracy: 0.5130 - val_loss: 1.9943 - val_accuracy: 0.2750\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.2474 - accuracy: 0.5652 - val_loss: 1.7034 - val_accuracy: 0.4433\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 1.0591 - accuracy: 0.6266 - val_loss: 1.7102 - val_accuracy: 0.4538\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.9698 - accuracy: 0.6679 - val_loss: 1.8941 - val_accuracy: 0.4608\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.7959 - accuracy: 0.7293 - val_loss: 1.6423 - val_accuracy: 0.4975\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.7255 - accuracy: 0.7674 - val_loss: 1.5788 - val_accuracy: 0.4954\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.5750 - accuracy: 0.8082 - val_loss: 1.4993 - val_accuracy: 0.5042\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 0.5324 - accuracy: 0.8457 - val_loss: 1.5576 - val_accuracy: 0.5121\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 0.4447 - accuracy: 0.8620 - val_loss: 1.6311 - val_accuracy: 0.5158\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.4049 - accuracy: 0.8690 - val_loss: 1.5978 - val_accuracy: 0.5142\n",
      "CV iteration 7 of 13\n",
      "Validation subjects are ['p002' 'p049' 'p045']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 24s 212ms/step - loss: 3.2525 - accuracy: 0.1418 - val_loss: 2.3221 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 2.2317 - accuracy: 0.2620 - val_loss: 2.4036 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 1.8678 - accuracy: 0.3402 - val_loss: 2.2984 - val_accuracy: 0.1004\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 1.6403 - accuracy: 0.4245 - val_loss: 2.2027 - val_accuracy: 0.1596\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.4448 - accuracy: 0.4864 - val_loss: 1.7513 - val_accuracy: 0.4021\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.1969 - accuracy: 0.5870 - val_loss: 1.4533 - val_accuracy: 0.4754\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.0383 - accuracy: 0.6505 - val_loss: 1.3809 - val_accuracy: 0.5125\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.9049 - accuracy: 0.6940 - val_loss: 1.4494 - val_accuracy: 0.5275\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.8545 - accuracy: 0.7092 - val_loss: 1.4510 - val_accuracy: 0.5417\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 0.6915 - accuracy: 0.7750 - val_loss: 1.2563 - val_accuracy: 0.6121\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.6433 - accuracy: 0.7902 - val_loss: 1.2431 - val_accuracy: 0.5650\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.5431 - accuracy: 0.8332 - val_loss: 1.3716 - val_accuracy: 0.6125\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.4645 - accuracy: 0.8565 - val_loss: 1.3325 - val_accuracy: 0.5908\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.4176 - accuracy: 0.8734 - val_loss: 1.3150 - val_accuracy: 0.5946\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.3627 - accuracy: 0.8957 - val_loss: 1.1985 - val_accuracy: 0.6296\n",
      "CV iteration 8 of 13\n",
      "Validation subjects are ['p061' 'p012' 'p041']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 3.3043 - accuracy: 0.1380 - val_loss: 2.3220 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 2.1758 - accuracy: 0.2451 - val_loss: 2.3714 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.8649 - accuracy: 0.3549 - val_loss: 2.3404 - val_accuracy: 0.1250\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.5870 - accuracy: 0.4500 - val_loss: 2.2169 - val_accuracy: 0.1129\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 1.3642 - accuracy: 0.5212 - val_loss: 1.8207 - val_accuracy: 0.3404\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.1699 - accuracy: 0.6000 - val_loss: 1.5182 - val_accuracy: 0.5154\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.0547 - accuracy: 0.6380 - val_loss: 1.5156 - val_accuracy: 0.5063\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.9313 - accuracy: 0.6902 - val_loss: 1.3596 - val_accuracy: 0.5900\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.8125 - accuracy: 0.7370 - val_loss: 1.2448 - val_accuracy: 0.5788\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.6628 - accuracy: 0.7875 - val_loss: 1.2607 - val_accuracy: 0.5821\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.5776 - accuracy: 0.8125 - val_loss: 1.2683 - val_accuracy: 0.6246\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 0.5266 - accuracy: 0.8293 - val_loss: 1.3083 - val_accuracy: 0.5771\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.4673 - accuracy: 0.8554 - val_loss: 1.2084 - val_accuracy: 0.5892\n",
      "CV iteration 9 of 13\n",
      "Validation subjects are ['p026' 'p049' 'p015']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 3.2366 - accuracy: 0.1473 - val_loss: 2.3104 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 2.2378 - accuracy: 0.2505 - val_loss: 2.3572 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 1.8209 - accuracy: 0.3658 - val_loss: 2.3290 - val_accuracy: 0.1004\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 1.6230 - accuracy: 0.4364 - val_loss: 2.1588 - val_accuracy: 0.1967\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 24s 213ms/step - loss: 1.3765 - accuracy: 0.5141 - val_loss: 1.8543 - val_accuracy: 0.3142\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 1.2328 - accuracy: 0.5799 - val_loss: 1.6311 - val_accuracy: 0.4137\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 1.0617 - accuracy: 0.6451 - val_loss: 1.5614 - val_accuracy: 0.4946\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.8773 - accuracy: 0.7043 - val_loss: 1.4112 - val_accuracy: 0.5221\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 24s 213ms/step - loss: 0.7439 - accuracy: 0.7527 - val_loss: 1.3381 - val_accuracy: 0.6025\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.6974 - accuracy: 0.7728 - val_loss: 1.4375 - val_accuracy: 0.5446\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 24s 213ms/step - loss: 0.5933 - accuracy: 0.8092 - val_loss: 1.3861 - val_accuracy: 0.6129\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.5003 - accuracy: 0.8424 - val_loss: 1.4756 - val_accuracy: 0.5013\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 24s 213ms/step - loss: 0.4783 - accuracy: 0.8484 - val_loss: 1.2940 - val_accuracy: 0.6137\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 24s 213ms/step - loss: 0.3877 - accuracy: 0.8886 - val_loss: 1.4392 - val_accuracy: 0.5679\n",
      "CV iteration 10 of 13\n",
      "Validation subjects are ['p052' 'p045' 'p051']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 3.2653 - accuracy: 0.1446 - val_loss: 2.3182 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 2.3235 - accuracy: 0.2239 - val_loss: 2.3868 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.9388 - accuracy: 0.3130 - val_loss: 2.3298 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 213ms/step - loss: 1.6960 - accuracy: 0.4071 - val_loss: 2.0356 - val_accuracy: 0.2675\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.4488 - accuracy: 0.5049 - val_loss: 1.5831 - val_accuracy: 0.5125\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 213ms/step - loss: 1.3226 - accuracy: 0.5321 - val_loss: 1.2771 - val_accuracy: 0.6208\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.0944 - accuracy: 0.6261 - val_loss: 1.1604 - val_accuracy: 0.6025\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 0.9510 - accuracy: 0.6777 - val_loss: 1.0856 - val_accuracy: 0.6908\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 0.8541 - accuracy: 0.7163 - val_loss: 1.0517 - val_accuracy: 0.6925\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 31s 266ms/step - loss: 0.7277 - accuracy: 0.7658 - val_loss: 0.9269 - val_accuracy: 0.7654\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 213ms/step - loss: 0.6720 - accuracy: 0.7761 - val_loss: 1.0031 - val_accuracy: 0.6900\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 30s 262ms/step - loss: 0.5609 - accuracy: 0.8304 - val_loss: 0.8180 - val_accuracy: 0.7800\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 0.5065 - accuracy: 0.8500 - val_loss: 1.0300 - val_accuracy: 0.6988\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 31s 265ms/step - loss: 0.4170 - accuracy: 0.8630 - val_loss: 0.8790 - val_accuracy: 0.7937\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 25s 213ms/step - loss: 0.3746 - accuracy: 0.8870 - val_loss: 0.9090 - val_accuracy: 0.7529\n",
      "CV iteration 11 of 13\n",
      "Validation subjects are ['p045' 'p021' 'p016']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 3.2355 - accuracy: 0.1620 - val_loss: 2.3164 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 2.2813 - accuracy: 0.2451 - val_loss: 2.3836 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 1.8948 - accuracy: 0.3375 - val_loss: 2.3805 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 1.6575 - accuracy: 0.4049 - val_loss: 2.1877 - val_accuracy: 0.1321\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.4354 - accuracy: 0.5207 - val_loss: 1.7541 - val_accuracy: 0.3929\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 1.2226 - accuracy: 0.5788 - val_loss: 1.3579 - val_accuracy: 0.6058\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 1.0897 - accuracy: 0.6223 - val_loss: 1.2420 - val_accuracy: 0.5971\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.9501 - accuracy: 0.6810 - val_loss: 1.1511 - val_accuracy: 0.6579\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.8098 - accuracy: 0.7418 - val_loss: 1.1528 - val_accuracy: 0.6637\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.7385 - accuracy: 0.7614 - val_loss: 0.9838 - val_accuracy: 0.6800\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.6053 - accuracy: 0.8022 - val_loss: 1.0109 - val_accuracy: 0.7092\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.5250 - accuracy: 0.8418 - val_loss: 0.9377 - val_accuracy: 0.7183\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.4413 - accuracy: 0.8679 - val_loss: 1.0502 - val_accuracy: 0.6913\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.4202 - accuracy: 0.8690 - val_loss: 0.9825 - val_accuracy: 0.7029\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.3872 - accuracy: 0.8837 - val_loss: 1.1266 - val_accuracy: 0.6538\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.3225 - accuracy: 0.9109 - val_loss: 1.0074 - val_accuracy: 0.7063\n",
      "CV iteration 12 of 13\n",
      "Validation subjects are ['p022' 'p064' 'p035']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 3.2850 - accuracy: 0.1500 - val_loss: 2.3282 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 2.1736 - accuracy: 0.2636 - val_loss: 2.3918 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 1.8252 - accuracy: 0.3609 - val_loss: 2.4091 - val_accuracy: 0.0900\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 1.5932 - accuracy: 0.4413 - val_loss: 2.3139 - val_accuracy: 0.1521\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 1.4277 - accuracy: 0.5071 - val_loss: 1.8303 - val_accuracy: 0.2642\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 1.1638 - accuracy: 0.6065 - val_loss: 1.4788 - val_accuracy: 0.4442\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 1.0589 - accuracy: 0.6587 - val_loss: 1.3924 - val_accuracy: 0.4804\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 0.9096 - accuracy: 0.6913 - val_loss: 1.3162 - val_accuracy: 0.5300\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 0.7777 - accuracy: 0.7353 - val_loss: 1.3584 - val_accuracy: 0.4875\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 0.6718 - accuracy: 0.7918 - val_loss: 1.3413 - val_accuracy: 0.5233acy\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 0.5780 - accuracy: 0.8196 - val_loss: 1.3242 - val_accuracy: 0.5038\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 0.5399 - accuracy: 0.8310 - val_loss: 1.2282 - val_accuracy: 0.5825\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 0.4644 - accuracy: 0.8603 - val_loss: 1.1331 - val_accuracy: 0.5725\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 0.3358 - accuracy: 0.8984 - val_loss: 1.1341 - val_accuracy: 0.6037\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 0.3201 - accuracy: 0.9082 - val_loss: 1.1596 - val_accuracy: 0.6108\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 0.2830 - accuracy: 0.9065 - val_loss: 1.2193 - val_accuracy: 0.6275\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 10523s 92s/step - loss: 0.2505 - accuracy: 0.9315 - val_loss: 1.3237 - val_accuracy: 0.5688\n",
      "CV iteration 13 of 13\n",
      "Validation subjects are ['p021' 'p061' 'p042']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 27s 233ms/step - loss: 3.3128 - accuracy: 0.1380 - val_loss: 2.3374 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 26s 229ms/step - loss: 2.2302 - accuracy: 0.2495 - val_loss: 2.4402 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 26s 228ms/step - loss: 1.8661 - accuracy: 0.3457 - val_loss: 2.4440 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 26s 228ms/step - loss: 1.6249 - accuracy: 0.4424 - val_loss: 2.1964 - val_accuracy: 0.1142\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 26s 229ms/step - loss: 1.4448 - accuracy: 0.4918 - val_loss: 1.7913 - val_accuracy: 0.3821\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 1.2228 - accuracy: 0.5842 - val_loss: 1.4315 - val_accuracy: 0.5171\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 25s 214ms/step - loss: 1.1012 - accuracy: 0.6245 - val_loss: 1.2167 - val_accuracy: 0.5667\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 213ms/step - loss: 0.9655 - accuracy: 0.6783 - val_loss: 1.1927 - val_accuracy: 0.5383\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 0.7969 - accuracy: 0.7310 - val_loss: 1.0866 - val_accuracy: 0.6233\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 213ms/step - loss: 0.7238 - accuracy: 0.7538 - val_loss: 0.9969 - val_accuracy: 0.6263\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 24s 213ms/step - loss: 0.6250 - accuracy: 0.8033 - val_loss: 0.9736 - val_accuracy: 0.6650\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 0.5270 - accuracy: 0.8332 - val_loss: 0.9390 - val_accuracy: 0.6558\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 0.4471 - accuracy: 0.8701 - val_loss: 1.1185 - val_accuracy: 0.5829\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 0.4013 - accuracy: 0.8788 - val_loss: 1.0050 - val_accuracy: 0.6217\n"
     ]
    }
   ],
   "source": [
    "model4data = cvrand(model4, \n",
    "                    df,\n",
    "                    trdgen,\n",
    "                    tedgen,\n",
    "                    n_iterations=13,\n",
    "                    batch_size=16,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch=115,\n",
    "                    target_size=(227,227),\n",
    "                    random_state=42,\n",
    "                    min_delta=0.05,\n",
    "                    patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>validation_subjects</th>\n",
       "      <th>train_accuracies</th>\n",
       "      <th>validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[p026, p050, p002]</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[p052, p039, p024]</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[p049, p064, p042]</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[p051, p066, p014]</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[p021, p045, p056]</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[p042, p050, p049]</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[p002, p049, p045]</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[p061, p012, p041]</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[p026, p049, p015]</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[p052, p045, p051]</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[p045, p021, p016]</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[p022, p064, p035]</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[p021, p061, p042]</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   validation_subjects  train_accuracies  validation_accuracy\n",
       "0   [p026, p050, p002]             0.856                0.440\n",
       "1   [p052, p039, p024]             0.813                0.685\n",
       "2   [p049, p064, p042]             0.897                0.718\n",
       "3   [p051, p066, p014]             0.834                0.680\n",
       "4   [p021, p045, p056]             0.782                0.629\n",
       "5   [p042, p050, p049]             0.862                0.516\n",
       "6   [p002, p049, p045]             0.896                0.630\n",
       "7   [p061, p012, p041]             0.812                0.625\n",
       "8   [p026, p049, p015]             0.848                0.614\n",
       "9   [p052, p045, p051]             0.863                0.794\n",
       "10  [p045, p021, p016]             0.842                0.718\n",
       "11  [p022, p064, p035]             0.907                0.628\n",
       "12  [p021, p061, p042]             0.803                0.665"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4data.to_csv('../metrics/model4metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.figure(figsize=(10,10))\n",
    "_ = plt.xlim(0, 1)\n",
    "_ = plt.ylim(0, 1)\n",
    "_ = sns.scatterplot(data=model4data, x='train_accuracies', y='validation_accuracy')\n",
    "_ = plt.plot([0,1], [0,1])\n",
    "_ = plt.title('Cross Validation Training Accuracies by Validation Accuracies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performed pretty well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4 with Datagen Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelbuild():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(99,\n",
    "                  kernel_size=11,\n",
    "                  strides=4,\n",
    "                  padding='valid',\n",
    "                  input_shape=(227, 227, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPool2D(3,\n",
    "                         strides=2,\n",
    "                         padding='valid'))\n",
    "    model.add(Conv2D(256,\n",
    "                      kernel_size=5,\n",
    "                      strides=1,\n",
    "                      padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPool2D(3,\n",
    "                        strides=2,\n",
    "                        padding='valid'))\n",
    "    model.add(Conv2D(384,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(384,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(256,\n",
    "                      kernel_size=3,\n",
    "                      strides=1,\n",
    "                      padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPool2D(3,\n",
    "                         strides=2,\n",
    "                         padding='valid'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    opt = Adam(learning_rate=0.00001)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model5 = modelbuild()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdgen = ImageDataGenerator(rescale=1./255, samplewise_center=True)\n",
    "tedgen = ImageDataGenerator(rescale=1./255, samplewise_center=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV iteration 1 of 13\n",
      "Validation subjects are ['p026' 'p050' 'p002']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 32s 279ms/step - loss: 3.1446 - accuracy: 0.1592 - val_loss: 2.3285 - val_accuracy: 0.0862\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 31s 268ms/step - loss: 2.3926 - accuracy: 0.2299 - val_loss: 2.3653 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 2.0153 - accuracy: 0.3185 - val_loss: 2.4402 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 29s 256ms/step - loss: 1.7643 - accuracy: 0.3870 - val_loss: 2.3493 - val_accuracy: 0.1187\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 30s 259ms/step - loss: 1.5139 - accuracy: 0.4620 - val_loss: 2.1725 - val_accuracy: 0.1567\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 30s 259ms/step - loss: 1.3131 - accuracy: 0.5380 - val_loss: 2.0385 - val_accuracy: 0.2329\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 32s 279ms/step - loss: 1.1525 - accuracy: 0.6038 - val_loss: 1.9639 - val_accuracy: 0.3025\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 32s 281ms/step - loss: 0.9888 - accuracy: 0.6717 - val_loss: 1.9578 - val_accuracy: 0.3542\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 32s 276ms/step - loss: 0.9228 - accuracy: 0.6853 - val_loss: 1.9633 - val_accuracy: 0.3717\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 32s 280ms/step - loss: 0.7856 - accuracy: 0.7473 - val_loss: 1.9588 - val_accuracy: 0.3767\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 32s 279ms/step - loss: 0.6291 - accuracy: 0.7978 - val_loss: 1.7122 - val_accuracy: 0.4308\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.6224 - accuracy: 0.8033 - val_loss: 2.1474 - val_accuracy: 0.3217\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 32s 281ms/step - loss: 0.5324 - accuracy: 0.8337 - val_loss: 1.8647 - val_accuracy: 0.4504\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.4746 - accuracy: 0.8413 - val_loss: 1.9367 - val_accuracy: 0.3779\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 0.4241 - accuracy: 0.8739 - val_loss: 2.1188 - val_accuracy: 0.4096\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 32s 276ms/step - loss: 0.3915 - accuracy: 0.8815 - val_loss: 1.8092 - val_accuracy: 0.4538\n",
      "CV iteration 2 of 13\n",
      "Validation subjects are ['p052' 'p039' 'p024']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 3.2704 - accuracy: 0.1457 - val_loss: 2.3133 - val_accuracy: 0.1013\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 2.3556 - accuracy: 0.2168 - val_loss: 2.3482 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.9744 - accuracy: 0.3239 - val_loss: 2.3015 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 26s 227ms/step - loss: 1.6752 - accuracy: 0.4223 - val_loss: 2.0987 - val_accuracy: 0.2158\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 33s 285ms/step - loss: 1.4914 - accuracy: 0.4777 - val_loss: 1.6228 - val_accuracy: 0.4933\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 1.2834 - accuracy: 0.5636 - val_loss: 1.4379 - val_accuracy: 0.4675\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 33s 284ms/step - loss: 1.1532 - accuracy: 0.6060 - val_loss: 1.2502 - val_accuracy: 0.6142\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 0.9732 - accuracy: 0.6739 - val_loss: 1.2541 - val_accuracy: 0.6092\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 0.8411 - accuracy: 0.7353 - val_loss: 1.2174 - val_accuracy: 0.5679\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 33s 286ms/step - loss: 0.7682 - accuracy: 0.7489 - val_loss: 1.0848 - val_accuracy: 0.6504\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 26s 224ms/step - loss: 0.6480 - accuracy: 0.7891 - val_loss: 1.0850 - val_accuracy: 0.6367\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 0.5729 - accuracy: 0.8277 - val_loss: 1.0868 - val_accuracy: 0.6246\n",
      "CV iteration 3 of 13\n",
      "Validation subjects are ['p049' 'p064' 'p042']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 3.2239 - accuracy: 0.1609 - val_loss: 2.3159 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 26s 228ms/step - loss: 2.3093 - accuracy: 0.2342 - val_loss: 2.3279 - val_accuracy: 0.1412\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 27s 233ms/step - loss: 1.9339 - accuracy: 0.3082 - val_loss: 2.3109 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 1.6749 - accuracy: 0.4293 - val_loss: 2.0979 - val_accuracy: 0.2738\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 1.4590 - accuracy: 0.4957 - val_loss: 1.7410 - val_accuracy: 0.4212\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 1.2848 - accuracy: 0.5533 - val_loss: 1.5013 - val_accuracy: 0.4058\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 1.1221 - accuracy: 0.6223 - val_loss: 1.2864 - val_accuracy: 0.5954\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 1.0072 - accuracy: 0.6603 - val_loss: 1.1442 - val_accuracy: 0.5667\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 32s 277ms/step - loss: 0.8619 - accuracy: 0.7130 - val_loss: 0.9918 - val_accuracy: 0.6692\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 32s 280ms/step - loss: 0.7014 - accuracy: 0.7717 - val_loss: 0.9130 - val_accuracy: 0.6737\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 0.6235 - accuracy: 0.8082 - val_loss: 1.0121 - val_accuracy: 0.6642\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 32s 283ms/step - loss: 0.5204 - accuracy: 0.8457 - val_loss: 0.8708 - val_accuracy: 0.6867\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 31s 274ms/step - loss: 0.4668 - accuracy: 0.8598 - val_loss: 0.8260 - val_accuracy: 0.7196\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 0.4301 - accuracy: 0.8685 - val_loss: 0.9819 - val_accuracy: 0.6479\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 0.3744 - accuracy: 0.8804 - val_loss: 0.8271 - val_accuracy: 0.7000\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 0.3274 - accuracy: 0.9054 - val_loss: 0.9332 - val_accuracy: 0.6612\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 0.3134 - accuracy: 0.9082 - val_loss: 0.7672 - val_accuracy: 0.7192\n",
      "Epoch 18/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 0.2885 - accuracy: 0.9125 - val_loss: 0.8437 - val_accuracy: 0.6742\n",
      "CV iteration 4 of 13\n",
      "Validation subjects are ['p051' 'p066' 'p014']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 3.2622 - accuracy: 0.1451 - val_loss: 2.3163 - val_accuracy: 0.1117\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 2.2547 - accuracy: 0.2402 - val_loss: 2.3370 - val_accuracy: 0.1412\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 1.9191 - accuracy: 0.3223 - val_loss: 2.2603 - val_accuracy: 0.1954\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 1.6052 - accuracy: 0.4250 - val_loss: 2.1033 - val_accuracy: 0.2650\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 1.4385 - accuracy: 0.4815 - val_loss: 1.8048 - val_accuracy: 0.3604\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 1.2525 - accuracy: 0.5511 - val_loss: 1.5580 - val_accuracy: 0.4725\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 1.0754 - accuracy: 0.6375 - val_loss: 1.4439 - val_accuracy: 0.4688\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.9755 - accuracy: 0.6663 - val_loss: 1.2915 - val_accuracy: 0.5442\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.8597 - accuracy: 0.7130 - val_loss: 1.2684 - val_accuracy: 0.5275\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 0.7085 - accuracy: 0.7777 - val_loss: 1.2343 - val_accuracy: 0.5679\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.6246 - accuracy: 0.7973 - val_loss: 1.3366 - val_accuracy: 0.5229\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.5455 - accuracy: 0.8321 - val_loss: 1.2309 - val_accuracy: 0.5542\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.4674 - accuracy: 0.8543 - val_loss: 1.1048 - val_accuracy: 0.5946\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.3947 - accuracy: 0.8788 - val_loss: 1.1191 - val_accuracy: 0.6033\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.3902 - accuracy: 0.8870 - val_loss: 1.2789 - val_accuracy: 0.5825\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 0.3177 - accuracy: 0.9185 - val_loss: 1.2964 - val_accuracy: 0.5783\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.3001 - accuracy: 0.9103 - val_loss: 1.2169 - val_accuracy: 0.5767\n",
      "Epoch 18/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.2541 - accuracy: 0.9315 - val_loss: 1.2465 - val_accuracy: 0.6096\n",
      "CV iteration 5 of 13\n",
      "Validation subjects are ['p021' 'p045' 'p056']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 3.4259 - accuracy: 0.1429 - val_loss: 2.3313 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 2.2877 - accuracy: 0.2348 - val_loss: 2.3325 - val_accuracy: 0.1396\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.8720 - accuracy: 0.3359 - val_loss: 2.3434 - val_accuracy: 0.1183\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 1.6435 - accuracy: 0.4266 - val_loss: 2.1936 - val_accuracy: 0.1946\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.4290 - accuracy: 0.5103 - val_loss: 1.8815 - val_accuracy: 0.3200\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.2590 - accuracy: 0.5587 - val_loss: 1.6085 - val_accuracy: 0.4650\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 1.1065 - accuracy: 0.6207 - val_loss: 1.4624 - val_accuracy: 0.4604\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.9442 - accuracy: 0.6875 - val_loss: 1.5859 - val_accuracy: 0.4121\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.8102 - accuracy: 0.7223 - val_loss: 1.3680 - val_accuracy: 0.5138\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.7027 - accuracy: 0.7696 - val_loss: 1.3927 - val_accuracy: 0.5483\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.5793 - accuracy: 0.8196 - val_loss: 1.3126 - val_accuracy: 0.5696\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.5101 - accuracy: 0.8413 - val_loss: 1.2950 - val_accuracy: 0.5579\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 0.4669 - accuracy: 0.8522 - val_loss: 1.1926 - val_accuracy: 0.5888\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.3877 - accuracy: 0.8918 - val_loss: 1.1520 - val_accuracy: 0.6321\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 0.3687 - accuracy: 0.8902 - val_loss: 1.1908 - val_accuracy: 0.5933\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.2888 - accuracy: 0.9239 - val_loss: 1.1994 - val_accuracy: 0.6137\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.2735 - accuracy: 0.9239 - val_loss: 1.1272 - val_accuracy: 0.6325\n",
      "Epoch 18/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.2178 - accuracy: 0.9413 - val_loss: 1.2316 - val_accuracy: 0.6233\n",
      "Epoch 19/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.2456 - accuracy: 0.9239 - val_loss: 1.2428 - val_accuracy: 0.6392\n",
      "CV iteration 6 of 13\n",
      "Validation subjects are ['p042' 'p050' 'p049']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 3.3619 - accuracy: 0.1533 - val_loss: 2.3184 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 2.2989 - accuracy: 0.2266 - val_loss: 2.3723 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 1.8750 - accuracy: 0.3326 - val_loss: 2.3774 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 1.6578 - accuracy: 0.4261 - val_loss: 2.2608 - val_accuracy: 0.1017\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 1.4357 - accuracy: 0.4973 - val_loss: 1.7862 - val_accuracy: 0.4062\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 1.2217 - accuracy: 0.5750 - val_loss: 1.5272 - val_accuracy: 0.4563\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.0691 - accuracy: 0.6418 - val_loss: 1.6074 - val_accuracy: 0.4371\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.9062 - accuracy: 0.6891 - val_loss: 1.6079 - val_accuracy: 0.4658\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.8140 - accuracy: 0.7261 - val_loss: 1.4880 - val_accuracy: 0.5238\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.6931 - accuracy: 0.7880 - val_loss: 1.3575 - val_accuracy: 0.5371\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.6116 - accuracy: 0.8136 - val_loss: 1.4870 - val_accuracy: 0.4737\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.5316 - accuracy: 0.8310 - val_loss: 1.3284 - val_accuracy: 0.5258\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.4426 - accuracy: 0.8652 - val_loss: 1.2817 - val_accuracy: 0.5367\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.4010 - accuracy: 0.8734 - val_loss: 1.3410 - val_accuracy: 0.5387\n",
      "CV iteration 7 of 13\n",
      "Validation subjects are ['p002' 'p049' 'p045']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 3.3382 - accuracy: 0.1505 - val_loss: 2.3192 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 2.2603 - accuracy: 0.2359 - val_loss: 2.3786 - val_accuracy: 0.1013\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 1.8891 - accuracy: 0.3380 - val_loss: 2.3786 - val_accuracy: 0.1900\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.6104 - accuracy: 0.4408 - val_loss: 2.1703 - val_accuracy: 0.1783\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 1.4165 - accuracy: 0.4995 - val_loss: 1.6755 - val_accuracy: 0.4346\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 1.2456 - accuracy: 0.5587 - val_loss: 1.4832 - val_accuracy: 0.5079\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 1.0655 - accuracy: 0.6310 - val_loss: 1.3784 - val_accuracy: 0.5775\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.9443 - accuracy: 0.6707 - val_loss: 1.3871 - val_accuracy: 0.5842\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 0.8549 - accuracy: 0.7239 - val_loss: 1.2199 - val_accuracy: 0.6129\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.6916 - accuracy: 0.7658 - val_loss: 1.3063 - val_accuracy: 0.5942\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.6486 - accuracy: 0.7864 - val_loss: 1.2942 - val_accuracy: 0.5929\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.5605 - accuracy: 0.8190 - val_loss: 1.1959 - val_accuracy: 0.5900\n",
      "CV iteration 8 of 13\n",
      "Validation subjects are ['p061' 'p012' 'p041']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 3.3693 - accuracy: 0.1462 - val_loss: 2.3117 - val_accuracy: 0.1146\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 26s 226ms/step - loss: 2.2486 - accuracy: 0.2391 - val_loss: 2.3507 - val_accuracy: 0.1650\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.8866 - accuracy: 0.3451 - val_loss: 2.3299 - val_accuracy: 0.1321\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 1.6187 - accuracy: 0.4152 - val_loss: 2.1475 - val_accuracy: 0.2346\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.4058 - accuracy: 0.4908 - val_loss: 1.8256 - val_accuracy: 0.4238\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.2216 - accuracy: 0.5685 - val_loss: 1.6104 - val_accuracy: 0.5104\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 1.0358 - accuracy: 0.6489 - val_loss: 1.6258 - val_accuracy: 0.4817\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 0.9068 - accuracy: 0.7076 - val_loss: 1.5716 - val_accuracy: 0.5283\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 0.7951 - accuracy: 0.7446 - val_loss: 1.4508 - val_accuracy: 0.5575\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 0.7135 - accuracy: 0.7587 - val_loss: 1.4358 - val_accuracy: 0.5537\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 0.6096 - accuracy: 0.8016 - val_loss: 1.3580 - val_accuracy: 0.5621\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 0.5389 - accuracy: 0.8239 - val_loss: 1.3471 - val_accuracy: 0.5767\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 0.4523 - accuracy: 0.8630 - val_loss: 1.3970 - val_accuracy: 0.5808\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 0.3925 - accuracy: 0.8842 - val_loss: 1.4462 - val_accuracy: 0.5925\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 0.3404 - accuracy: 0.8989 - val_loss: 1.2800 - val_accuracy: 0.6450\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 25s 222ms/step - loss: 0.3044 - accuracy: 0.9168 - val_loss: 1.3446 - val_accuracy: 0.6104\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 0.2986 - accuracy: 0.9103 - val_loss: 1.3811 - val_accuracy: 0.5871\n",
      "Epoch 18/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 0.2452 - accuracy: 0.9304 - val_loss: 1.3488 - val_accuracy: 0.6292\n",
      "Epoch 19/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 0.2322 - accuracy: 0.9391 - val_loss: 1.3600 - val_accuracy: 0.6567\n",
      "Epoch 20/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 0.2200 - accuracy: 0.9370 - val_loss: 1.3286 - val_accuracy: 0.6529\n",
      "CV iteration 9 of 13\n",
      "Validation subjects are ['p026' 'p049' 'p015']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 3.3344 - accuracy: 0.1342 - val_loss: 2.3357 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 222ms/step - loss: 2.2185 - accuracy: 0.2429 - val_loss: 2.3908 - val_accuracy: 0.0933\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 1.8674 - accuracy: 0.3554 - val_loss: 2.3840 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 1.5875 - accuracy: 0.4418 - val_loss: 2.0976 - val_accuracy: 0.2479\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 1.3863 - accuracy: 0.5109 - val_loss: 1.7621 - val_accuracy: 0.3183\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 1.2078 - accuracy: 0.5848 - val_loss: 1.6301 - val_accuracy: 0.4363\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 1.0501 - accuracy: 0.6429 - val_loss: 1.4137 - val_accuracy: 0.5492\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 0.9217 - accuracy: 0.6989 - val_loss: 1.4411 - val_accuracy: 0.5092\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 222ms/step - loss: 0.7642 - accuracy: 0.7484 - val_loss: 1.3117 - val_accuracy: 0.6092\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 0.7016 - accuracy: 0.7707 - val_loss: 1.2679 - val_accuracy: 0.6208\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 222ms/step - loss: 0.5646 - accuracy: 0.8179 - val_loss: 1.3127 - val_accuracy: 0.6167\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 0.4961 - accuracy: 0.8457 - val_loss: 1.2075 - val_accuracy: 0.6521\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 0.4292 - accuracy: 0.8707 - val_loss: 1.2561 - val_accuracy: 0.6296\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 0.3829 - accuracy: 0.8913 - val_loss: 1.3074 - val_accuracy: 0.6392\n",
      "CV iteration 10 of 13\n",
      "Validation subjects are ['p052' 'p045' 'p051']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 3.2646 - accuracy: 0.1592 - val_loss: 2.3020 - val_accuracy: 0.1437\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 2.3459 - accuracy: 0.2266 - val_loss: 2.2950 - val_accuracy: 0.1329\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 1.8995 - accuracy: 0.3266 - val_loss: 2.2052 - val_accuracy: 0.1892\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 1.7151 - accuracy: 0.3995 - val_loss: 2.0281 - val_accuracy: 0.2338\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 1.5207 - accuracy: 0.4739 - val_loss: 1.5461 - val_accuracy: 0.4654\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 1.3150 - accuracy: 0.5277 - val_loss: 1.4238 - val_accuracy: 0.4721\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.1264 - accuracy: 0.6125 - val_loss: 1.0864 - val_accuracy: 0.6929\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 31s 268ms/step - loss: 0.9917 - accuracy: 0.6516 - val_loss: 1.0708 - val_accuracy: 0.7337\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 0.8292 - accuracy: 0.7299 - val_loss: 1.0984 - val_accuracy: 0.6762\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 0.7502 - accuracy: 0.7522 - val_loss: 1.0121 - val_accuracy: 0.6771\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.6528 - accuracy: 0.7891 - val_loss: 0.9345 - val_accuracy: 0.7183\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 0.5572 - accuracy: 0.8234 - val_loss: 1.0653 - val_accuracy: 0.6467\n",
      "CV iteration 11 of 13\n",
      "Validation subjects are ['p045' 'p021' 'p016']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 3.3532 - accuracy: 0.1511 - val_loss: 2.3135 - val_accuracy: 0.0867\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 2.2870 - accuracy: 0.2147 - val_loss: 2.3474 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 1.9187 - accuracy: 0.3315 - val_loss: 2.3675 - val_accuracy: 0.1042\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 1.7068 - accuracy: 0.3973 - val_loss: 2.1036 - val_accuracy: 0.2017\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 1.4894 - accuracy: 0.4821 - val_loss: 1.7108 - val_accuracy: 0.4246\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 1.2707 - accuracy: 0.5549 - val_loss: 1.3748 - val_accuracy: 0.5529\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.1095 - accuracy: 0.6065 - val_loss: 1.2089 - val_accuracy: 0.6575\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.9657 - accuracy: 0.6728 - val_loss: 1.1243 - val_accuracy: 0.6712\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.8402 - accuracy: 0.7147 - val_loss: 1.0790 - val_accuracy: 0.6908\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.7625 - accuracy: 0.7457 - val_loss: 1.0628 - val_accuracy: 0.6604\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.6007 - accuracy: 0.8033 - val_loss: 1.0103 - val_accuracy: 0.7188\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 0.5358 - accuracy: 0.8304 - val_loss: 1.0321 - val_accuracy: 0.6554\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.4624 - accuracy: 0.8560 - val_loss: 0.9764 - val_accuracy: 0.6808\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 0.3989 - accuracy: 0.8826 - val_loss: 1.0093 - val_accuracy: 0.6917\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 0.3895 - accuracy: 0.8848 - val_loss: 0.9507 - val_accuracy: 0.7054\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.3202 - accuracy: 0.9087 - val_loss: 0.9172 - val_accuracy: 0.7096\n",
      "CV iteration 12 of 13\n",
      "Validation subjects are ['p022' 'p064' 'p035']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 3.2913 - accuracy: 0.1516 - val_loss: 2.3265 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 2.2854 - accuracy: 0.2489 - val_loss: 2.3406 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 1.9084 - accuracy: 0.3359 - val_loss: 2.3607 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 1.6443 - accuracy: 0.4380 - val_loss: 2.1171 - val_accuracy: 0.1954\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 1.4474 - accuracy: 0.5049 - val_loss: 1.8525 - val_accuracy: 0.2504\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 1.2579 - accuracy: 0.5707 - val_loss: 1.4882 - val_accuracy: 0.5058\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 1.1200 - accuracy: 0.6239 - val_loss: 1.4309 - val_accuracy: 0.5100\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.9419 - accuracy: 0.6880 - val_loss: 1.5987 - val_accuracy: 0.4308\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 26s 225ms/step - loss: 0.8203 - accuracy: 0.7457 - val_loss: 1.5121 - val_accuracy: 0.4875\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 0.7045 - accuracy: 0.7723 - val_loss: 1.4612 - val_accuracy: 0.4875\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 0.6598 - accuracy: 0.7989 - val_loss: 1.4028 - val_accuracy: 0.5133\n",
      "CV iteration 13 of 13\n",
      "Validation subjects are ['p021' 'p061' 'p042']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 3.3089 - accuracy: 0.1446 - val_loss: 2.3108 - val_accuracy: 0.1408\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 2.2862 - accuracy: 0.2255 - val_loss: 2.3149 - val_accuracy: 0.1538\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 1.9532 - accuracy: 0.3168 - val_loss: 2.2394 - val_accuracy: 0.1787\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 1.6906 - accuracy: 0.4082 - val_loss: 2.0902 - val_accuracy: 0.1850\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 1.4902 - accuracy: 0.4880 - val_loss: 1.7359 - val_accuracy: 0.3288\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 1.3012 - accuracy: 0.5424 - val_loss: 1.4020 - val_accuracy: 0.4888\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 1.1764 - accuracy: 0.5886 - val_loss: 1.2636 - val_accuracy: 0.5450\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 0.9925 - accuracy: 0.6663 - val_loss: 1.1683 - val_accuracy: 0.5654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.8813 - accuracy: 0.7054 - val_loss: 1.1267 - val_accuracy: 0.5592\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.7574 - accuracy: 0.7565 - val_loss: 1.1068 - val_accuracy: 0.6000\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 0.6360 - accuracy: 0.8065 - val_loss: 0.9667 - val_accuracy: 0.7017\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 25s 222ms/step - loss: 0.5671 - accuracy: 0.8250 - val_loss: 0.9367 - val_accuracy: 0.6850\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 0.4875 - accuracy: 0.8707 - val_loss: 0.9492 - val_accuracy: 0.6858\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.4599 - accuracy: 0.8571 - val_loss: 0.9987 - val_accuracy: 0.6662\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 0.3799 - accuracy: 0.8935 - val_loss: 1.0685 - val_accuracy: 0.6237\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 0.3276 - accuracy: 0.9043 - val_loss: 0.7929 - val_accuracy: 0.7312\n"
     ]
    }
   ],
   "source": [
    "model5data = cvrand(model5, \n",
    "                    df,\n",
    "                    trdgen,\n",
    "                    tedgen,\n",
    "                    n_iterations=13,\n",
    "                    batch_size=16,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch=115,\n",
    "                    target_size=(227,227),\n",
    "                    random_state=42,\n",
    "                    min_delta=0.05,\n",
    "                    patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>validation_subjects</th>\n",
       "      <th>train_accuracies</th>\n",
       "      <th>validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[p026, p050, p002]</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[p052, p039, p024]</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[p049, p064, p042]</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[p051, p066, p014]</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[p021, p045, p056]</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[p042, p050, p049]</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[p002, p049, p045]</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[p061, p012, p041]</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[p026, p049, p015]</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[p052, p045, p051]</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[p045, p021, p016]</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[p022, p064, p035]</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[p021, p061, p042]</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   validation_subjects  train_accuracies  validation_accuracy\n",
       "0   [p026, p050, p002]             0.882                0.454\n",
       "1   [p052, p039, p024]             0.749                0.650\n",
       "2   [p049, p064, p042]             0.860                0.720\n",
       "3   [p051, p066, p014]             0.932                0.610\n",
       "4   [p021, p045, p056]             0.924                0.639\n",
       "5   [p042, p050, p049]             0.873                0.539\n",
       "6   [p002, p049, p045]             0.724                0.613\n",
       "7   [p061, p012, p041]             0.939                0.657\n",
       "8   [p026, p049, p015]             0.846                0.652\n",
       "9   [p052, p045, p051]             0.652                0.734\n",
       "10  [p045, p021, p016]             0.803                0.719\n",
       "11  [p022, p064, p035]             0.799                0.513\n",
       "12  [p021, p061, p042]             0.904                0.731"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5data.to_csv('../metrics/model5metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with additional Datagen Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model6 = modelbuild()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdgen = ImageDataGenerator(samplewise_center=True,\n",
    "                            rescale=1./255,\n",
    "                            rotation_range=10,\n",
    "                            width_shift_range=0.05,\n",
    "                            height_shift_range=0.05,\n",
    "                            brightness_range=[0.8, 1.2])\n",
    "\n",
    "tedgen = ImageDataGenerator(samplewise_center=True,\n",
    "                            rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV iteration 1 of 13\n",
      "Validation subjects are ['p026' 'p050' 'p002']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 58s 502ms/step - loss: 3.4176 - accuracy: 0.1163 - val_loss: 2.3143 - val_accuracy: 0.0996\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 51s 444ms/step - loss: 2.6676 - accuracy: 0.1598 - val_loss: 2.3292 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 46s 397ms/step - loss: 2.2938 - accuracy: 0.2120 - val_loss: 2.3639 - val_accuracy: 0.0933\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 52s 453ms/step - loss: 2.1096 - accuracy: 0.2614 - val_loss: 2.2583 - val_accuracy: 0.1408\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 51s 444ms/step - loss: 1.9489 - accuracy: 0.3147 - val_loss: 2.1674 - val_accuracy: 0.2175\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 53s 462ms/step - loss: 1.8129 - accuracy: 0.3424 - val_loss: 2.0606 - val_accuracy: 0.2562\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 51s 447ms/step - loss: 1.7067 - accuracy: 0.3957 - val_loss: 2.1381 - val_accuracy: 0.2892\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 1.6150 - accuracy: 0.4283 - val_loss: 2.2167 - val_accuracy: 0.2850\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 55s 476ms/step - loss: 1.4791 - accuracy: 0.4821 - val_loss: 2.1145 - val_accuracy: 0.3058\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 51s 447ms/step - loss: 1.3989 - accuracy: 0.5060 - val_loss: 1.9931 - val_accuracy: 0.3775\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 46s 396ms/step - loss: 1.3589 - accuracy: 0.5212 - val_loss: 1.9257 - val_accuracy: 0.3533\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 46s 396ms/step - loss: 1.2660 - accuracy: 0.5266 - val_loss: 1.9223 - val_accuracy: 0.3467\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 46s 397ms/step - loss: 1.2026 - accuracy: 0.5652 - val_loss: 2.0573 - val_accuracy: 0.3754\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 52s 454ms/step - loss: 1.0715 - accuracy: 0.6109 - val_loss: 1.8886 - val_accuracy: 0.3938\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 46s 397ms/step - loss: 1.0258 - accuracy: 0.6277 - val_loss: 1.7701 - val_accuracy: 0.3896\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 51s 442ms/step - loss: 1.0314 - accuracy: 0.6364 - val_loss: 1.9311 - val_accuracy: 0.4000\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 52s 453ms/step - loss: 0.9651 - accuracy: 0.6620 - val_loss: 1.9621 - val_accuracy: 0.4142\n",
      "CV iteration 2 of 13\n",
      "Validation subjects are ['p052' 'p039' 'p024']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 3.4332 - accuracy: 0.1299 - val_loss: 2.3184 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 2.5637 - accuracy: 0.1696 - val_loss: 2.3166 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 2.2146 - accuracy: 0.2217 - val_loss: 2.2940 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 2.0449 - accuracy: 0.2815 - val_loss: 2.1451 - val_accuracy: 0.1875\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 52s 452ms/step - loss: 1.9674 - accuracy: 0.3022 - val_loss: 1.7347 - val_accuracy: 0.5229\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 1.8207 - accuracy: 0.3457 - val_loss: 1.5632 - val_accuracy: 0.4817\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 52s 456ms/step - loss: 1.6823 - accuracy: 0.4043 - val_loss: 1.4238 - val_accuracy: 0.5292\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 1.5911 - accuracy: 0.4288 - val_loss: 1.4441 - val_accuracy: 0.4283\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 51s 447ms/step - loss: 1.4888 - accuracy: 0.4505 - val_loss: 1.3275 - val_accuracy: 0.5387\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 52s 456ms/step - loss: 1.4199 - accuracy: 0.4913 - val_loss: 1.2729 - val_accuracy: 0.5879\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 1.3495 - accuracy: 0.5163 - val_loss: 1.2811 - val_accuracy: 0.5800\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 51s 445ms/step - loss: 1.2612 - accuracy: 0.5505 - val_loss: 1.1704 - val_accuracy: 0.6154\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 52s 456ms/step - loss: 1.1702 - accuracy: 0.5826 - val_loss: 1.2303 - val_accuracy: 0.6175\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 51s 445ms/step - loss: 1.1175 - accuracy: 0.5951 - val_loss: 1.0476 - val_accuracy: 0.6254\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 1.0550 - accuracy: 0.6315 - val_loss: 1.1776 - val_accuracy: 0.5879\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 46s 397ms/step - loss: 0.9767 - accuracy: 0.6647 - val_loss: 1.0539 - val_accuracy: 0.6171\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 52s 456ms/step - loss: 0.8735 - accuracy: 0.6918 - val_loss: 1.0925 - val_accuracy: 0.6454\n",
      "Epoch 18/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 0.8553 - accuracy: 0.7065 - val_loss: 1.0147 - val_accuracy: 0.6279\n",
      "Epoch 19/50\n",
      "115/115 [==============================] - 51s 444ms/step - loss: 0.8050 - accuracy: 0.7163 - val_loss: 1.0130 - val_accuracy: 0.6817\n",
      "Epoch 20/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 0.8144 - accuracy: 0.7201 - val_loss: 1.1016 - val_accuracy: 0.6408\n",
      "Epoch 21/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 0.7061 - accuracy: 0.7587 - val_loss: 1.1366 - val_accuracy: 0.6096\n",
      "Epoch 22/50\n",
      "115/115 [==============================] - 46s 397ms/step - loss: 0.6780 - accuracy: 0.7821 - val_loss: 0.9470 - val_accuracy: 0.6750\n",
      "Epoch 23/50\n",
      "115/115 [==============================] - 52s 455ms/step - loss: 0.6296 - accuracy: 0.7745 - val_loss: 0.9842 - val_accuracy: 0.6917\n",
      "Epoch 24/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 0.5555 - accuracy: 0.8038 - val_loss: 0.9378 - val_accuracy: 0.6892\n",
      "CV iteration 3 of 13\n",
      "Validation subjects are ['p049' 'p064' 'p042']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 45s 395ms/step - loss: 3.4729 - accuracy: 0.1337 - val_loss: 2.3263 - val_accuracy: 0.1092\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 46s 397ms/step - loss: 2.4715 - accuracy: 0.1793 - val_loss: 2.3153 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 45s 395ms/step - loss: 2.1646 - accuracy: 0.2440 - val_loss: 2.2732 - val_accuracy: 0.1392\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 46s 396ms/step - loss: 1.9709 - accuracy: 0.3033 - val_loss: 2.0475 - val_accuracy: 0.2512\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 46s 396ms/step - loss: 1.8530 - accuracy: 0.3380 - val_loss: 1.7040 - val_accuracy: 0.3963\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 45s 395ms/step - loss: 1.7332 - accuracy: 0.3636 - val_loss: 1.4858 - val_accuracy: 0.5083\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 45s 395ms/step - loss: 1.6522 - accuracy: 0.4022 - val_loss: 1.3756 - val_accuracy: 0.5142\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 45s 395ms/step - loss: 1.5684 - accuracy: 0.4217 - val_loss: 1.2864 - val_accuracy: 0.5612\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 45s 395ms/step - loss: 1.4349 - accuracy: 0.4842 - val_loss: 1.2283 - val_accuracy: 0.5913\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 45s 395ms/step - loss: 1.3958 - accuracy: 0.4946 - val_loss: 1.2058 - val_accuracy: 0.6050\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 45s 395ms/step - loss: 1.2498 - accuracy: 0.5467 - val_loss: 1.1141 - val_accuracy: 0.5992\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 45s 395ms/step - loss: 1.2401 - accuracy: 0.5440 - val_loss: 1.0775 - val_accuracy: 0.6025\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 45s 395ms/step - loss: 1.1186 - accuracy: 0.5984 - val_loss: 1.0112 - val_accuracy: 0.6179\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 47s 405ms/step - loss: 1.0786 - accuracy: 0.6196 - val_loss: 0.9581 - val_accuracy: 0.6796\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 45s 395ms/step - loss: 0.9861 - accuracy: 0.6473 - val_loss: 0.9745 - val_accuracy: 0.6321\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 45s 395ms/step - loss: 0.9132 - accuracy: 0.6658 - val_loss: 0.9105 - val_accuracy: 0.6854\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 51s 446ms/step - loss: 0.8987 - accuracy: 0.6826 - val_loss: 0.8691 - val_accuracy: 0.7008\n",
      "Epoch 18/50\n",
      "115/115 [==============================] - 52s 456ms/step - loss: 0.8247 - accuracy: 0.7228 - val_loss: 0.8372 - val_accuracy: 0.7292\n",
      "Epoch 19/50\n",
      "115/115 [==============================] - 45s 395ms/step - loss: 0.7770 - accuracy: 0.7359 - val_loss: 0.7979 - val_accuracy: 0.7258\n",
      "Epoch 20/50\n",
      "115/115 [==============================] - 47s 405ms/step - loss: 0.7307 - accuracy: 0.7380 - val_loss: 0.8144 - val_accuracy: 0.7279\n",
      "Epoch 21/50\n",
      "115/115 [==============================] - 45s 395ms/step - loss: 0.7147 - accuracy: 0.7462 - val_loss: 0.8897 - val_accuracy: 0.6779\n",
      "CV iteration 4 of 13\n",
      "Validation subjects are ['p051' 'p066' 'p014']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 45s 395ms/step - loss: 3.5181 - accuracy: 0.1272 - val_loss: 2.3241 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 45s 395ms/step - loss: 2.4316 - accuracy: 0.1788 - val_loss: 2.3182 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 45s 395ms/step - loss: 2.1266 - accuracy: 0.2522 - val_loss: 2.2712 - val_accuracy: 0.1375\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 46s 396ms/step - loss: 2.0011 - accuracy: 0.2946 - val_loss: 2.1682 - val_accuracy: 0.2058\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 46s 396ms/step - loss: 1.8130 - accuracy: 0.3500 - val_loss: 1.8853 - val_accuracy: 0.3900\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 1.7332 - accuracy: 0.3701 - val_loss: 1.6101 - val_accuracy: 0.4737\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 45s 395ms/step - loss: 1.6039 - accuracy: 0.4261 - val_loss: 1.5009 - val_accuracy: 0.4825\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 46s 396ms/step - loss: 1.5082 - accuracy: 0.4679 - val_loss: 1.4675 - val_accuracy: 0.4696\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 46s 397ms/step - loss: 1.4273 - accuracy: 0.4842 - val_loss: 1.3427 - val_accuracy: 0.5275\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 46s 396ms/step - loss: 1.3352 - accuracy: 0.5130 - val_loss: 1.3338 - val_accuracy: 0.5371\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 45s 395ms/step - loss: 1.2427 - accuracy: 0.5495 - val_loss: 1.2232 - val_accuracy: 0.5713\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 45s 395ms/step - loss: 1.1730 - accuracy: 0.5804 - val_loss: 1.2446 - val_accuracy: 0.5554\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 46s 396ms/step - loss: 1.0749 - accuracy: 0.6141 - val_loss: 1.2337 - val_accuracy: 0.5492\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 46s 397ms/step - loss: 0.9588 - accuracy: 0.6636 - val_loss: 1.1777 - val_accuracy: 0.6142\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 46s 396ms/step - loss: 0.9226 - accuracy: 0.6853 - val_loss: 1.0727 - val_accuracy: 0.6092\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 45s 395ms/step - loss: 0.8772 - accuracy: 0.6913 - val_loss: 0.9703 - val_accuracy: 0.6867\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 0.8020 - accuracy: 0.7168 - val_loss: 1.0322 - val_accuracy: 0.6258\n",
      "Epoch 18/50\n",
      "115/115 [==============================] - 46s 396ms/step - loss: 0.7721 - accuracy: 0.7212 - val_loss: 0.9436 - val_accuracy: 0.6529\n",
      "Epoch 19/50\n",
      "115/115 [==============================] - 45s 395ms/step - loss: 0.7268 - accuracy: 0.7538 - val_loss: 0.9833 - val_accuracy: 0.6592\n",
      "Epoch 20/50\n",
      "115/115 [==============================] - 46s 396ms/step - loss: 0.6934 - accuracy: 0.7603 - val_loss: 0.9119 - val_accuracy: 0.6883\n",
      "Epoch 21/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 0.6232 - accuracy: 0.7908 - val_loss: 0.8687 - val_accuracy: 0.7004\n",
      "Epoch 22/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 0.5898 - accuracy: 0.8005 - val_loss: 0.9072 - val_accuracy: 0.7212\n",
      "Epoch 23/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 0.5711 - accuracy: 0.8027 - val_loss: 0.9280 - val_accuracy: 0.6921\n",
      "CV iteration 5 of 13\n",
      "Validation subjects are ['p021' 'p045' 'p056']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 3.5318 - accuracy: 0.1196 - val_loss: 2.3313 - val_accuracy: 0.1117\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 2.4363 - accuracy: 0.1897 - val_loss: 2.3441 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 2.2052 - accuracy: 0.2370 - val_loss: 2.3454 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 2.0269 - accuracy: 0.2690 - val_loss: 2.2603 - val_accuracy: 0.1338\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 46s 401ms/step - loss: 1.8658 - accuracy: 0.3304 - val_loss: 1.9106 - val_accuracy: 0.3421\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 1.7617 - accuracy: 0.3598 - val_loss: 1.5988 - val_accuracy: 0.4387\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 1.6345 - accuracy: 0.4038 - val_loss: 1.5474 - val_accuracy: 0.4317\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 46s 401ms/step - loss: 1.5720 - accuracy: 0.4245 - val_loss: 1.4096 - val_accuracy: 0.5250\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 1.4730 - accuracy: 0.4750 - val_loss: 1.3814 - val_accuracy: 0.5004\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 1.3770 - accuracy: 0.4913 - val_loss: 1.3938 - val_accuracy: 0.5433\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 1.2667 - accuracy: 0.5391 - val_loss: 1.2456 - val_accuracy: 0.5733\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 1.2159 - accuracy: 0.5630 - val_loss: 1.2136 - val_accuracy: 0.6308\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 1.1159 - accuracy: 0.6109 - val_loss: 1.2273 - val_accuracy: 0.5929\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 1.0421 - accuracy: 0.6261 - val_loss: 1.0994 - val_accuracy: 0.6779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 0.9914 - accuracy: 0.6489 - val_loss: 1.2278 - val_accuracy: 0.6158\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 0.9614 - accuracy: 0.6598 - val_loss: 1.0784 - val_accuracy: 0.6392\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 0.8683 - accuracy: 0.6967 - val_loss: 1.0476 - val_accuracy: 0.6463\n",
      "Epoch 18/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 0.8498 - accuracy: 0.7027 - val_loss: 0.9932 - val_accuracy: 0.6496\n",
      "Epoch 19/50\n",
      "115/115 [==============================] - 47s 412ms/step - loss: 0.7614 - accuracy: 0.7337 - val_loss: 1.0750 - val_accuracy: 0.6517\n",
      "CV iteration 6 of 13\n",
      "Validation subjects are ['p042' 'p050' 'p049']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 47s 405ms/step - loss: 3.4608 - accuracy: 0.1283 - val_loss: 2.3171 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 46s 404ms/step - loss: 2.4563 - accuracy: 0.1886 - val_loss: 2.3332 - val_accuracy: 0.1704\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 2.1374 - accuracy: 0.2391 - val_loss: 2.3505 - val_accuracy: 0.1042\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 1.9994 - accuracy: 0.2750 - val_loss: 2.0925 - val_accuracy: 0.2562\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 46s 404ms/step - loss: 1.8827 - accuracy: 0.3136 - val_loss: 1.8619 - val_accuracy: 0.3146\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 47s 404ms/step - loss: 1.7232 - accuracy: 0.3636 - val_loss: 1.6992 - val_accuracy: 0.4596\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 1.5918 - accuracy: 0.4364 - val_loss: 1.6017 - val_accuracy: 0.4833\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 1.5376 - accuracy: 0.4391 - val_loss: 1.5938 - val_accuracy: 0.4717\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 47s 404ms/step - loss: 1.4408 - accuracy: 0.4663 - val_loss: 1.6658 - val_accuracy: 0.5004\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 47s 405ms/step - loss: 1.3250 - accuracy: 0.5201 - val_loss: 1.6764 - val_accuracy: 0.4825\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 46s 404ms/step - loss: 1.2529 - accuracy: 0.5495 - val_loss: 1.6512 - val_accuracy: 0.5175\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 47s 405ms/step - loss: 1.1889 - accuracy: 0.5755 - val_loss: 1.6547 - val_accuracy: 0.5071\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 46s 404ms/step - loss: 1.0912 - accuracy: 0.5989 - val_loss: 1.7031 - val_accuracy: 0.4642\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 47s 405ms/step - loss: 1.0152 - accuracy: 0.6402 - val_loss: 1.6793 - val_accuracy: 0.5271\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 47s 404ms/step - loss: 0.9239 - accuracy: 0.6647 - val_loss: 1.5913 - val_accuracy: 0.4837\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 46s 404ms/step - loss: 0.8916 - accuracy: 0.6897 - val_loss: 1.7266 - val_accuracy: 0.5188\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 47s 405ms/step - loss: 0.8122 - accuracy: 0.7163 - val_loss: 1.7670 - val_accuracy: 0.5171\n",
      "Epoch 18/50\n",
      "115/115 [==============================] - 47s 405ms/step - loss: 0.8181 - accuracy: 0.7223 - val_loss: 1.6682 - val_accuracy: 0.5658\n",
      "CV iteration 7 of 13\n",
      "Validation subjects are ['p002' 'p049' 'p045']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 3.4763 - accuracy: 0.1234 - val_loss: 2.3232 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 2.4584 - accuracy: 0.1842 - val_loss: 2.3044 - val_accuracy: 0.1146\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 46s 402ms/step - loss: 2.1880 - accuracy: 0.2364 - val_loss: 2.2481 - val_accuracy: 0.1500\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 46s 402ms/step - loss: 2.0015 - accuracy: 0.2913 - val_loss: 2.0828 - val_accuracy: 0.3071\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 46s 402ms/step - loss: 1.8800 - accuracy: 0.3266 - val_loss: 1.7968 - val_accuracy: 0.3462\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 1.7461 - accuracy: 0.3701 - val_loss: 1.6543 - val_accuracy: 0.4375\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 1.6546 - accuracy: 0.3962 - val_loss: 1.5740 - val_accuracy: 0.4717\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 1.5732 - accuracy: 0.4261 - val_loss: 1.6096 - val_accuracy: 0.4229\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 46s 402ms/step - loss: 1.4730 - accuracy: 0.4576 - val_loss: 1.5357 - val_accuracy: 0.4679\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 46s 402ms/step - loss: 1.3829 - accuracy: 0.4859 - val_loss: 1.4387 - val_accuracy: 0.5525\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 46s 402ms/step - loss: 1.3006 - accuracy: 0.5418 - val_loss: 1.4154 - val_accuracy: 0.5496\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 46s 401ms/step - loss: 1.1685 - accuracy: 0.5734 - val_loss: 1.3718 - val_accuracy: 0.5437\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 1.1202 - accuracy: 0.5978 - val_loss: 1.6293 - val_accuracy: 0.4575\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 46s 402ms/step - loss: 1.0704 - accuracy: 0.6163 - val_loss: 1.5632 - val_accuracy: 0.5600\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 46s 402ms/step - loss: 0.9733 - accuracy: 0.6598 - val_loss: 1.3840 - val_accuracy: 0.5325\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 46s 402ms/step - loss: 0.9998 - accuracy: 0.6353 - val_loss: 1.3754 - val_accuracy: 0.5725\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 48s 416ms/step - loss: 0.9169 - accuracy: 0.6745 - val_loss: 1.3869 - val_accuracy: 0.5708\n",
      "CV iteration 8 of 13\n",
      "Validation subjects are ['p061' 'p012' 'p041']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 47s 407ms/step - loss: 3.4275 - accuracy: 0.1310 - val_loss: 2.3141 - val_accuracy: 0.0892\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 2.5376 - accuracy: 0.1516 - val_loss: 2.3306 - val_accuracy: 0.1637\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 2.1823 - accuracy: 0.2359 - val_loss: 2.3441 - val_accuracy: 0.1063\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 1.9968 - accuracy: 0.2853 - val_loss: 2.1085 - val_accuracy: 0.2713\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 46s 404ms/step - loss: 1.9016 - accuracy: 0.3049 - val_loss: 1.8317 - val_accuracy: 0.3304\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 1.7363 - accuracy: 0.3707 - val_loss: 1.6705 - val_accuracy: 0.4829\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 1.6218 - accuracy: 0.4109 - val_loss: 1.6488 - val_accuracy: 0.3708\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 46s 404ms/step - loss: 1.5283 - accuracy: 0.4565 - val_loss: 1.5152 - val_accuracy: 0.5462\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 46s 403ms/step - loss: 1.4565 - accuracy: 0.4620 - val_loss: 1.6227 - val_accuracy: 0.4800\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 46s 404ms/step - loss: 1.3466 - accuracy: 0.5179 - val_loss: 1.5848 - val_accuracy: 0.4396\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 1.2740 - accuracy: 0.5576 - val_loss: 1.5422 - val_accuracy: 0.5104\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 1.2316 - accuracy: 0.5527 - val_loss: 1.6608 - val_accuracy: 0.4696\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 1.0926 - accuracy: 0.6332 - val_loss: 1.5348 - val_accuracy: 0.5267\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 1.0081 - accuracy: 0.6451 - val_loss: 1.6504 - val_accuracy: 0.5333\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 46s 404ms/step - loss: 0.9282 - accuracy: 0.6783 - val_loss: 1.4414 - val_accuracy: 0.5888\n",
      "CV iteration 9 of 13\n",
      "Validation subjects are ['p026' 'p049' 'p015']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 3.5085 - accuracy: 0.1261 - val_loss: 2.3219 - val_accuracy: 0.0996\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 2.4514 - accuracy: 0.1989 - val_loss: 2.3528 - val_accuracy: 0.1333\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 2.1704 - accuracy: 0.2380 - val_loss: 2.2902 - val_accuracy: 0.1325\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 2.0081 - accuracy: 0.2897 - val_loss: 2.1822 - val_accuracy: 0.2146\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 1.8482 - accuracy: 0.3364 - val_loss: 1.9053 - val_accuracy: 0.3137\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 46s 402ms/step - loss: 1.7225 - accuracy: 0.3821 - val_loss: 1.7180 - val_accuracy: 0.4017\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 47s 404ms/step - loss: 1.6118 - accuracy: 0.4277 - val_loss: 1.6515 - val_accuracy: 0.4233\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 46s 404ms/step - loss: 1.5346 - accuracy: 0.4462 - val_loss: 1.5868 - val_accuracy: 0.4329\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 1.4548 - accuracy: 0.4668 - val_loss: 1.5229 - val_accuracy: 0.4917\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 1.3252 - accuracy: 0.5326 - val_loss: 1.4245 - val_accuracy: 0.5667\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 46s 404ms/step - loss: 1.2306 - accuracy: 0.5489 - val_loss: 1.3715 - val_accuracy: 0.5696\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 1.1661 - accuracy: 0.5864 - val_loss: 1.5835 - val_accuracy: 0.4983\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 46s 404ms/step - loss: 1.0732 - accuracy: 0.6158 - val_loss: 1.3641 - val_accuracy: 0.6129\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 46s 404ms/step - loss: 1.0816 - accuracy: 0.6190 - val_loss: 1.3728 - val_accuracy: 0.6025\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 46s 402ms/step - loss: 0.9794 - accuracy: 0.6375 - val_loss: 1.3018 - val_accuracy: 0.6104\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 0.8974 - accuracy: 0.6804 - val_loss: 1.3694 - val_accuracy: 0.6162\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 46s 402ms/step - loss: 0.8427 - accuracy: 0.7103 - val_loss: 1.3734 - val_accuracy: 0.6254\n",
      "Epoch 18/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 0.8111 - accuracy: 0.7147 - val_loss: 1.3201 - val_accuracy: 0.5979\n",
      "Epoch 19/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 0.7493 - accuracy: 0.7272 - val_loss: 1.4214 - val_accuracy: 0.6012\n",
      "Epoch 20/50\n",
      "115/115 [==============================] - 47s 406ms/step - loss: 0.7089 - accuracy: 0.7467 - val_loss: 1.2934 - val_accuracy: 0.6338\n",
      "Epoch 21/50\n",
      "115/115 [==============================] - 46s 404ms/step - loss: 0.6648 - accuracy: 0.7701 - val_loss: 1.3125 - val_accuracy: 0.6654\n",
      "Epoch 22/50\n",
      "115/115 [==============================] - 46s 404ms/step - loss: 0.5974 - accuracy: 0.7940 - val_loss: 1.2194 - val_accuracy: 0.6800\n",
      "Epoch 23/50\n",
      "115/115 [==============================] - 46s 404ms/step - loss: 0.5846 - accuracy: 0.8033 - val_loss: 1.2774 - val_accuracy: 0.6696\n",
      "Epoch 24/50\n",
      "115/115 [==============================] - 46s 403ms/step - loss: 0.5794 - accuracy: 0.8103 - val_loss: 1.1248 - val_accuracy: 0.6896\n",
      "Epoch 25/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 0.5267 - accuracy: 0.8185 - val_loss: 1.2174 - val_accuracy: 0.7113\n",
      "Epoch 26/50\n",
      "115/115 [==============================] - 46s 401ms/step - loss: 0.5174 - accuracy: 0.8310 - val_loss: 1.1999 - val_accuracy: 0.7287\n",
      "Epoch 27/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 0.4834 - accuracy: 0.8359 - val_loss: 1.2123 - val_accuracy: 0.7267\n",
      "Epoch 28/50\n",
      "115/115 [==============================] - 46s 401ms/step - loss: 0.4465 - accuracy: 0.8533 - val_loss: 1.3860 - val_accuracy: 0.6721\n",
      "Epoch 29/50\n",
      "115/115 [==============================] - 46s 401ms/step - loss: 0.4224 - accuracy: 0.8554 - val_loss: 1.3496 - val_accuracy: 0.6792\n",
      "CV iteration 10 of 13\n",
      "Validation subjects are ['p052' 'p045' 'p051']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 46s 401ms/step - loss: 3.4984 - accuracy: 0.1141 - val_loss: 2.3285 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 46s 401ms/step - loss: 2.4941 - accuracy: 0.1647 - val_loss: 2.3121 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 46s 402ms/step - loss: 2.1907 - accuracy: 0.2201 - val_loss: 2.2652 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 46s 402ms/step - loss: 2.0437 - accuracy: 0.2717 - val_loss: 2.0355 - val_accuracy: 0.3008\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 46s 401ms/step - loss: 1.9198 - accuracy: 0.3190 - val_loss: 1.7041 - val_accuracy: 0.4204\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 46s 401ms/step - loss: 1.8083 - accuracy: 0.3315 - val_loss: 1.4446 - val_accuracy: 0.5708\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 46s 401ms/step - loss: 1.6716 - accuracy: 0.4027 - val_loss: 1.2775 - val_accuracy: 0.5758\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 46s 401ms/step - loss: 1.5620 - accuracy: 0.4179 - val_loss: 1.2238 - val_accuracy: 0.6737\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 1.4904 - accuracy: 0.4582 - val_loss: 1.2299 - val_accuracy: 0.5671\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 46s 402ms/step - loss: 1.3859 - accuracy: 0.4853 - val_loss: 1.1634 - val_accuracy: 0.6908\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 46s 401ms/step - loss: 1.2649 - accuracy: 0.5418 - val_loss: 1.1970 - val_accuracy: 0.6513\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 46s 401ms/step - loss: 1.2269 - accuracy: 0.5668 - val_loss: 1.1156 - val_accuracy: 0.6075\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 46s 402ms/step - loss: 1.1527 - accuracy: 0.5810 - val_loss: 1.0921 - val_accuracy: 0.6413\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 46s 401ms/step - loss: 1.0896 - accuracy: 0.5908 - val_loss: 0.9344 - val_accuracy: 0.6871\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 46s 401ms/step - loss: 1.0307 - accuracy: 0.6239 - val_loss: 0.9351 - val_accuracy: 0.6846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV iteration 11 of 13\n",
      "Validation subjects are ['p045' 'p021' 'p016']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 46s 401ms/step - loss: 3.5685 - accuracy: 0.1163 - val_loss: 2.3093 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 46s 402ms/step - loss: 2.5282 - accuracy: 0.1603 - val_loss: 2.3249 - val_accuracy: 0.1033\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 46s 401ms/step - loss: 2.1885 - accuracy: 0.2245 - val_loss: 2.3209 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 46s 401ms/step - loss: 2.0664 - accuracy: 0.2614 - val_loss: 2.1440 - val_accuracy: 0.1242\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 1.9253 - accuracy: 0.2957 - val_loss: 1.7808 - val_accuracy: 0.4387\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 1.7911 - accuracy: 0.3500 - val_loss: 1.4956 - val_accuracy: 0.5192\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 1.7059 - accuracy: 0.3766 - val_loss: 1.3263 - val_accuracy: 0.6325\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 1.6004 - accuracy: 0.4152 - val_loss: 1.3473 - val_accuracy: 0.5550\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 1.5021 - accuracy: 0.4527 - val_loss: 1.3135 - val_accuracy: 0.5183\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 1.4282 - accuracy: 0.4712 - val_loss: 1.1292 - val_accuracy: 0.6513\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 1.3413 - accuracy: 0.5033 - val_loss: 1.1198 - val_accuracy: 0.6646\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 1.2698 - accuracy: 0.5603 - val_loss: 1.1208 - val_accuracy: 0.6029\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 1.1554 - accuracy: 0.5918 - val_loss: 1.0725 - val_accuracy: 0.6463\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 1.0951 - accuracy: 0.6190 - val_loss: 1.0459 - val_accuracy: 0.6513\n",
      "CV iteration 12 of 13\n",
      "Validation subjects are ['p022' 'p064' 'p035']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 3.4293 - accuracy: 0.1288 - val_loss: 2.3382 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 2.4844 - accuracy: 0.1690 - val_loss: 2.4000 - val_accuracy: 0.1221\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 46s 397ms/step - loss: 2.1677 - accuracy: 0.2457 - val_loss: 2.3424 - val_accuracy: 0.1517\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 1.9896 - accuracy: 0.2935 - val_loss: 2.1267 - val_accuracy: 0.2142\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 1.8798 - accuracy: 0.3250 - val_loss: 1.7724 - val_accuracy: 0.3550\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 1.7125 - accuracy: 0.3897 - val_loss: 1.5375 - val_accuracy: 0.4871\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 1.6174 - accuracy: 0.4196 - val_loss: 1.5436 - val_accuracy: 0.4233\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 1.5352 - accuracy: 0.4582 - val_loss: 1.4846 - val_accuracy: 0.4500\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 1.4225 - accuracy: 0.4918 - val_loss: 1.3762 - val_accuracy: 0.4975\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 46s 397ms/step - loss: 1.3093 - accuracy: 0.5440 - val_loss: 1.2878 - val_accuracy: 0.5221\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 1.2302 - accuracy: 0.5565 - val_loss: 1.1913 - val_accuracy: 0.5733\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 1.1541 - accuracy: 0.5853 - val_loss: 1.2189 - val_accuracy: 0.5612\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 1.0618 - accuracy: 0.6168 - val_loss: 1.1052 - val_accuracy: 0.5900\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 1.0236 - accuracy: 0.6402 - val_loss: 1.3018 - val_accuracy: 0.5288\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 0.9888 - accuracy: 0.6505 - val_loss: 1.0848 - val_accuracy: 0.5792\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 0.8824 - accuracy: 0.7130 - val_loss: 1.1341 - val_accuracy: 0.5883\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 0.8304 - accuracy: 0.7065 - val_loss: 1.1496 - val_accuracy: 0.5633\n",
      "Epoch 18/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 0.7727 - accuracy: 0.7293 - val_loss: 1.0048 - val_accuracy: 0.6463\n",
      "Epoch 19/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 0.7294 - accuracy: 0.7408 - val_loss: 1.2192 - val_accuracy: 0.5683\n",
      "Epoch 20/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 0.7196 - accuracy: 0.7505 - val_loss: 1.0815 - val_accuracy: 0.5883\n",
      "Epoch 21/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 0.6540 - accuracy: 0.7723 - val_loss: 0.9390 - val_accuracy: 0.6604\n",
      "Epoch 22/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 0.5997 - accuracy: 0.7891 - val_loss: 0.9372 - val_accuracy: 0.6363\n",
      "Epoch 23/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 0.5366 - accuracy: 0.8158 - val_loss: 0.9301 - val_accuracy: 0.6521\n",
      "Epoch 24/50\n",
      "115/115 [==============================] - 46s 397ms/step - loss: 0.5201 - accuracy: 0.8277 - val_loss: 0.8669 - val_accuracy: 0.6717\n",
      "Epoch 25/50\n",
      "115/115 [==============================] - 46s 398ms/step - loss: 0.5105 - accuracy: 0.8212 - val_loss: 0.9683 - val_accuracy: 0.6471\n",
      "CV iteration 13 of 13\n",
      "Validation subjects are ['p021' 'p061' 'p042']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 3.5036 - accuracy: 0.1315 - val_loss: 2.3085 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 46s 401ms/step - loss: 2.5225 - accuracy: 0.1804 - val_loss: 2.3012 - val_accuracy: 0.1079\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 2.1772 - accuracy: 0.2293 - val_loss: 2.2716 - val_accuracy: 0.1317\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 2.0307 - accuracy: 0.2750 - val_loss: 2.0714 - val_accuracy: 0.3512\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 1.8932 - accuracy: 0.3212 - val_loss: 1.7488 - val_accuracy: 0.4121\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 1.7703 - accuracy: 0.3924 - val_loss: 1.5089 - val_accuracy: 0.5121\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 1.6665 - accuracy: 0.4125 - val_loss: 1.3221 - val_accuracy: 0.5883\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 1.5566 - accuracy: 0.4353 - val_loss: 1.2937 - val_accuracy: 0.5175\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 1.4530 - accuracy: 0.4804 - val_loss: 1.2040 - val_accuracy: 0.5800\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 46s 399ms/step - loss: 1.3107 - accuracy: 0.5272 - val_loss: 1.0641 - val_accuracy: 0.6767\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 1.2967 - accuracy: 0.5391 - val_loss: 1.1284 - val_accuracy: 0.6400\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 1.2049 - accuracy: 0.5663 - val_loss: 1.0275 - val_accuracy: 0.6375\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 46s 399ms/step - loss: 1.1675 - accuracy: 0.5870 - val_loss: 0.9996 - val_accuracy: 0.6275\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 51s 445ms/step - loss: 1.0341 - accuracy: 0.6446 - val_loss: 0.8655 - val_accuracy: 0.7425\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 0.9888 - accuracy: 0.6380 - val_loss: 0.9239 - val_accuracy: 0.7029\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 0.9405 - accuracy: 0.6717 - val_loss: 0.9919 - val_accuracy: 0.6787\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 0.9134 - accuracy: 0.6897 - val_loss: 0.8498 - val_accuracy: 0.6821\n",
      "Epoch 18/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 0.8067 - accuracy: 0.7277 - val_loss: 0.8119 - val_accuracy: 0.6900\n",
      "Epoch 19/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 0.7757 - accuracy: 0.7364 - val_loss: 0.8447 - val_accuracy: 0.7296\n",
      "Epoch 20/50\n",
      "115/115 [==============================] - 46s 400ms/step - loss: 0.6980 - accuracy: 0.7484 - val_loss: 0.7972 - val_accuracy: 0.7096\n",
      "Epoch 21/50\n",
      "115/115 [==============================] - 53s 457ms/step - loss: 0.6475 - accuracy: 0.7821 - val_loss: 0.7664 - val_accuracy: 0.7700\n",
      "Wall time: 3h 20min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model6data = cvrand(model6, \n",
    "                    df,\n",
    "                    trdgen,\n",
    "                    tedgen,\n",
    "                    n_iterations=13,\n",
    "                    batch_size=16,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch=115,\n",
    "                    target_size=(227,227),\n",
    "                    random_state=42,\n",
    "                    min_delta=0.05,\n",
    "                    patience=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>validation_subjects</th>\n",
       "      <th>train_accuracies</th>\n",
       "      <th>validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[p026, p050, p002]</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[p052, p039, p024]</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[p049, p064, p042]</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[p051, p066, p014]</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[p021, p045, p056]</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[p042, p050, p049]</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[p002, p049, p045]</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[p061, p012, p041]</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[p026, p049, p015]</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[p052, p045, p051]</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[p045, p021, p016]</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[p022, p064, p035]</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[p021, p061, p042]</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   validation_subjects  train_accuracies  validation_accuracy\n",
       "0   [p026, p050, p002]             0.662                0.414\n",
       "1   [p052, p039, p024]             0.774                0.692\n",
       "2   [p049, p064, p042]             0.723                0.729\n",
       "3   [p051, p066, p014]             0.801                0.721\n",
       "4   [p021, p045, p056]             0.626                0.678\n",
       "5   [p042, p050, p049]             0.722                0.566\n",
       "6   [p002, p049, p045]             0.635                0.572\n",
       "7   [p061, p012, p041]             0.678                0.589\n",
       "8   [p026, p049, p015]             0.831                0.729\n",
       "9   [p052, p045, p051]             0.485                0.691\n",
       "10  [p045, p021, p016]             0.503                0.665\n",
       "11  [p022, p064, p035]             0.828                0.672\n",
       "12  [p021, p061, p042]             0.782                0.770"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6data.to_csv('../metrics/model6metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "notify.send('completed model 6 fitting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking the train/validation set that had the highest accuracies that were closest to one another. Looks like validation set = p021, p061, p042. \n",
    "\n",
    "Let's fit just that one and then save the best coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeled Test Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = df\n",
    "test_path = 'C:/Users/Dylan/Desktop/Data Science/Projects/DistractedDrivers/data/raw/imgs/testlabeled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = modelbuild()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "train_dgen = ImageDataGenerator(samplewise_center=True,\n",
    "                                rescale=1./255,\n",
    "                                rotation_range=10,\n",
    "                                width_shift_range=0.05,\n",
    "                                height_shift_range=0.05,\n",
    "                                brightness_range=[0.8, 1.2])\n",
    "\n",
    "test_dgen = ImageDataGenerator(samplewise_center=True,\n",
    "                              rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20800 validated image filenames belonging to 10 classes.\n",
      "Found 200 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train = train_dgen.flow_from_dataframe(dftrain,\n",
    "                                       x_col='imgpath',\n",
    "                                       y_col='classname',\n",
    "                                       batch_size=16, \n",
    "                                       target_size=(227,227),\n",
    "                                       random_state=42)\n",
    "\n",
    "test = test_dgen.flow_from_directory(test_path,\n",
    "                                   target_size=(227,227),\n",
    "                                   seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate model checkpoint and callbacks_list\n",
    "checkpoint = ModelCheckpoint('weights.hdf5',\n",
    "                              mode='max',\n",
    "                              monitor='val_accuracy',\n",
    "                              save_best_only=True)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0.025, patience=5)\n",
    "\n",
    "callbacks_list = [checkpoint, earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 7 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 48s 417ms/step - loss: 3.3559 - accuracy: 0.1245 - val_loss: 2.3228 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 43s 374ms/step - loss: 2.6416 - accuracy: 0.1571 - val_loss: 2.3403 - val_accuracy: 0.1050\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 42s 365ms/step - loss: 2.3353 - accuracy: 0.2125 - val_loss: 2.3215 - val_accuracy: 0.1200\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 40s 352ms/step - loss: 2.1420 - accuracy: 0.2587 - val_loss: 2.2003 - val_accuracy: 0.1950\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 42s 363ms/step - loss: 2.0103 - accuracy: 0.2853 - val_loss: 1.9161 - val_accuracy: 0.3300\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 42s 368ms/step - loss: 1.8697 - accuracy: 0.3451 - val_loss: 1.7981 - val_accuracy: 0.3500\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 42s 366ms/step - loss: 1.7854 - accuracy: 0.3522 - val_loss: 1.7324 - val_accuracy: 0.3850\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 35s 301ms/step - loss: 1.6609 - accuracy: 0.4087 - val_loss: 1.7154 - val_accuracy: 0.3700\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 43s 378ms/step - loss: 1.6194 - accuracy: 0.4136 - val_loss: 1.6045 - val_accuracy: 0.4100\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 43s 373ms/step - loss: 1.5159 - accuracy: 0.4451 - val_loss: 1.6132 - val_accuracy: 0.4400\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 42s 367ms/step - loss: 1.4363 - accuracy: 0.4842 - val_loss: 1.5628 - val_accuracy: 0.4550\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 42s 367ms/step - loss: 1.3633 - accuracy: 0.5120 - val_loss: 1.5090 - val_accuracy: 0.4650\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 42s 367ms/step - loss: 1.3348 - accuracy: 0.5163 - val_loss: 1.4372 - val_accuracy: 0.5350\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 44s 385ms/step - loss: 1.2263 - accuracy: 0.5690 - val_loss: 1.4310 - val_accuracy: 0.5450\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 44s 379ms/step - loss: 1.1332 - accuracy: 0.5978 - val_loss: 1.3435 - val_accuracy: 0.5600\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 47s 407ms/step - loss: 1.0920 - accuracy: 0.6234 - val_loss: 1.3200 - val_accuracy: 0.5650\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 38s 328ms/step - loss: 1.0650 - accuracy: 0.6049 - val_loss: 1.3842 - val_accuracy: 0.5400\n",
      "Epoch 18/50\n",
      "115/115 [==============================] - 36s 315ms/step - loss: 0.9923 - accuracy: 0.6527 - val_loss: 1.4315 - val_accuracy: 0.5400\n",
      "Epoch 19/50\n",
      "115/115 [==============================] - 42s 363ms/step - loss: 0.8886 - accuracy: 0.6891 - val_loss: 1.4672 - val_accuracy: 0.5750\n",
      "Epoch 20/50\n",
      "115/115 [==============================] - 42s 364ms/step - loss: 0.8919 - accuracy: 0.6837 - val_loss: 1.2673 - val_accuracy: 0.5800\n",
      "Epoch 21/50\n",
      "115/115 [==============================] - 41s 357ms/step - loss: 0.8229 - accuracy: 0.7174 - val_loss: 1.2370 - val_accuracy: 0.5850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x201fd21fec8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit(train,\n",
    "          epochs=50,\n",
    "          steps_per_epoch=115,\n",
    "          validation_data=test,\n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
