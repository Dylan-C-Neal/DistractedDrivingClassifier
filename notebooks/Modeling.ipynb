{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPool2D, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Endpoint: <samp>https://notify.run/9CRUVPV2Tc5OBQpD</samp></p>\n",
       "<p>To subscribe, open: <a href=\"https://notify.run/c/9CRUVPV2Tc5OBQpD\">https://notify.run/c/9CRUVPV2Tc5OBQpD</a></p>\n",
       "<p>Or scan this QR code:</p>\n",
       "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"222\" width=\"222\" class=\"pyqrcode\"><path transform=\"scale(6)\" stroke=\"#000\" class=\"pyqrline\" d=\"M4 4.5h7m1 0h1m4 0h2m1 0h1m1 0h2m2 0h7m-29 1h1m5 0h1m1 0h1m2 0h3m3 0h3m2 0h1m5 0h1m-29 1h1m1 0h3m1 0h1m1 0h2m2 0h1m2 0h4m3 0h1m1 0h3m1 0h1m-29 1h1m1 0h3m1 0h1m1 0h3m1 0h1m2 0h1m1 0h4m1 0h1m1 0h3m1 0h1m-29 1h1m1 0h3m1 0h1m2 0h2m1 0h1m1 0h3m3 0h1m1 0h1m1 0h3m1 0h1m-29 1h1m5 0h1m1 0h3m5 0h4m2 0h1m5 0h1m-29 1h7m1 0h1m1 0h1m1 0h1m1 0h1m1 0h1m1 0h1m1 0h1m1 0h7m-19 1h2m1 0h1m1 0h3m1 0h2m-21 1h2m2 0h3m3 0h1m1 0h2m2 0h1m3 0h1m2 0h1m1 0h4m-28 1h3m1 0h1m7 0h2m1 0h1m3 0h1m1 0h7m-29 1h7m1 0h3m4 0h4m1 0h1m1 0h1m5 0h1m-29 1h2m3 0h1m2 0h2m2 0h1m1 0h1m1 0h4m1 0h1m1 0h3m1 0h2m-28 1h1m4 0h2m3 0h1m1 0h2m4 0h3m5 0h1m-28 1h2m2 0h2m1 0h4m1 0h1m3 0h1m4 0h8m-25 1h4m1 0h7m2 0h1m1 0h2m1 0h1m1 0h2m1 0h1m-29 1h1m3 0h2m2 0h1m1 0h2m1 0h1m3 0h1m1 0h2m1 0h1m4 0h2m-29 1h1m1 0h3m1 0h3m3 0h2m1 0h1m4 0h1m2 0h1m3 0h1m-28 1h3m2 0h1m2 0h2m3 0h2m1 0h1m3 0h1m1 0h4m1 0h2m-27 1h2m2 0h1m3 0h1m4 0h1m1 0h1m6 0h1m1 0h1m1 0h1m-26 1h2m4 0h1m2 0h1m1 0h2m1 0h1m1 0h3m2 0h1m2 0h2m-29 1h3m1 0h3m1 0h1m1 0h2m1 0h2m1 0h1m3 0h6m2 0h1m-21 1h2m2 0h1m2 0h1m3 0h2m3 0h1m3 0h1m-29 1h7m3 0h6m2 0h3m1 0h1m1 0h3m1 0h1m-29 1h1m5 0h1m1 0h1m1 0h2m1 0h1m2 0h2m1 0h2m3 0h1m3 0h1m-29 1h1m1 0h3m1 0h1m1 0h2m2 0h2m3 0h9m1 0h1m-28 1h1m1 0h3m1 0h1m2 0h2m2 0h4m3 0h2m6 0h1m-29 1h1m1 0h3m1 0h1m3 0h1m4 0h2m2 0h1m5 0h4m-29 1h1m5 0h1m1 0h2m2 0h1m1 0h2m1 0h1m1 0h7m1 0h2m-29 1h7m1 0h1m1 0h2m1 0h2m1 0h1m3 0h1m1 0h2m1 0h1m1 0h1\"/></svg>\n",
       "\n",
       "        "
      ],
      "text/plain": [
       "Endpoint: https://notify.run/9CRUVPV2Tc5OBQpD\n",
       "To subscribe, open: https://notify.run/c/9CRUVPV2Tc5OBQpD\n",
       "Or scan this QR code:\n",
       "\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\n",
       "        "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notify_run import Notify\n",
    "notify = Notify()\n",
    "notify.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom functions\n",
    "sys.path.append('C:\\\\Users\\\\Dylan\\\\Desktop\\\\Data Science\\\\Projects\\\\DistractedDrivers\\\\functions')\n",
    "from ddfuncs import trainsampling, cvrand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Set memory limit on GPU to keep it from freezing up when fitting TensorFlow models later\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB * 3 of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], \\\n",
    "                                                                [tf.config.experimental.\\\n",
    "                                                                 VirtualDeviceConfiguration\\\n",
    "                                                                 (memory_limit=1024 * 3)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed training data\n",
    "os.chdir('../data/processed')\n",
    "df = pd.read_csv('driver_image_list_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trainsampling(df, samples=80, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "      <th>imgpath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_13073.jpg</td>\n",
       "      <td>imgs/train/c0/img_13073.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_5585.jpg</td>\n",
       "      <td>imgs/train/c0/img_5585.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_48187.jpg</td>\n",
       "      <td>imgs/train/c0/img_48187.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_98115.jpg</td>\n",
       "      <td>imgs/train/c0/img_98115.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_66355.jpg</td>\n",
       "      <td>imgs/train/c0/img_66355.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22364</th>\n",
       "      <td>p081</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_95966.jpg</td>\n",
       "      <td>imgs/train/c9/img_95966.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22404</th>\n",
       "      <td>p081</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_18412.jpg</td>\n",
       "      <td>imgs/train/c9/img_18412.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22415</th>\n",
       "      <td>p081</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_23818.jpg</td>\n",
       "      <td>imgs/train/c9/img_23818.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22358</th>\n",
       "      <td>p081</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_54961.jpg</td>\n",
       "      <td>imgs/train/c9/img_54961.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22395</th>\n",
       "      <td>p081</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_13314.jpg</td>\n",
       "      <td>imgs/train/c9/img_13314.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20800 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject classname            img                      imgpath\n",
       "51       p002        c0  img_13073.jpg  imgs/train/c0/img_13073.jpg\n",
       "14       p002        c0   img_5585.jpg   imgs/train/c0/img_5585.jpg\n",
       "71       p002        c0  img_48187.jpg  imgs/train/c0/img_48187.jpg\n",
       "60       p002        c0  img_98115.jpg  imgs/train/c0/img_98115.jpg\n",
       "20       p002        c0  img_66355.jpg  imgs/train/c0/img_66355.jpg\n",
       "...       ...       ...            ...                          ...\n",
       "22364    p081        c9  img_95966.jpg  imgs/train/c9/img_95966.jpg\n",
       "22404    p081        c9  img_18412.jpg  imgs/train/c9/img_18412.jpg\n",
       "22415    p081        c9  img_23818.jpg  imgs/train/c9/img_23818.jpg\n",
       "22358    p081        c9  img_54961.jpg  imgs/train/c9/img_54961.jpg\n",
       "22395    p081        c9  img_13314.jpg  imgs/train/c9/img_13314.jpg\n",
       "\n",
       "[20800 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to appropriate directory for data generation\n",
    "os.chdir('../raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 - 1 Conv, 1 MaxPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 254, 254, 10)      280       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 25, 25, 10)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6250)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                62510     \n",
      "=================================================================\n",
      "Total params: 62,790\n",
      "Trainable params: 62,790\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Conv2D(10, kernel_size=3, activation='relu', input_shape=(256, 256, 3)))\n",
    "model1.add(MaxPool2D(10))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(10, activation='softmax'))\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "model1.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV iteration 1 of 13\n",
      "Validation subjects are ['p026' 'p050' 'p002']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 30s 262ms/step - loss: 63.9846 - accuracy: 0.1495 - val_loss: 57.7408 - val_accuracy: 0.1050\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 30s 260ms/step - loss: 27.8950 - accuracy: 0.2772 - val_loss: 45.9950 - val_accuracy: 0.1550\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 32s 278ms/step - loss: 18.5662 - accuracy: 0.3696 - val_loss: 44.0138 - val_accuracy: 0.1650\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 33s 287ms/step - loss: 13.2900 - accuracy: 0.4826 - val_loss: 41.8431 - val_accuracy: 0.1663\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 31s 270ms/step - loss: 10.8829 - accuracy: 0.5304 - val_loss: 43.3074 - val_accuracy: 0.1521\n",
      "CV iteration 2 of 13\n",
      "Validation subjects are ['p052' 'p039' 'p024']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 32s 281ms/step - loss: 65.8975 - accuracy: 0.1467 - val_loss: 29.4313 - val_accuracy: 0.1988\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 33s 284ms/step - loss: 26.0729 - accuracy: 0.2723 - val_loss: 29.0584 - val_accuracy: 0.2512\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 34s 295ms/step - loss: 18.3187 - accuracy: 0.3620 - val_loss: 26.8080 - val_accuracy: 0.2562\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 33s 286ms/step - loss: 11.7205 - accuracy: 0.4880 - val_loss: 25.2896 - val_accuracy: 0.2608\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 34s 292ms/step - loss: 8.9298 - accuracy: 0.5478 - val_loss: 19.0994 - val_accuracy: 0.3371\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 35s 303ms/step - loss: 7.3597 - accuracy: 0.6158 - val_loss: 23.2580 - val_accuracy: 0.2892\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 34s 295ms/step - loss: 6.1033 - accuracy: 0.6560 - val_loss: 21.2058 - val_accuracy: 0.3288\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 34s 294ms/step - loss: 4.6666 - accuracy: 0.7272 - val_loss: 21.8329 - val_accuracy: 0.3021\n",
      "CV iteration 3 of 13\n",
      "Validation subjects are ['p049' 'p064' 'p042']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 35s 302ms/step - loss: 63.5910 - accuracy: 0.1707 - val_loss: 44.1096 - val_accuracy: 0.2612\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 33s 286ms/step - loss: 23.7756 - accuracy: 0.3076 - val_loss: 36.3922 - val_accuracy: 0.2763\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 32s 279ms/step - loss: 16.2400 - accuracy: 0.3891 - val_loss: 32.7885 - val_accuracy: 0.2621\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 32s 281ms/step - loss: 11.2436 - accuracy: 0.5038 - val_loss: 29.1616 - val_accuracy: 0.3021\n",
      "CV iteration 4 of 13\n",
      "Validation subjects are ['p051' 'p066' 'p014']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 33s 284ms/step - loss: 61.8447 - accuracy: 0.1799 - val_loss: 51.0618 - val_accuracy: 0.1296\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 26s 230ms/step - loss: 24.1887 - accuracy: 0.3049 - val_loss: 43.7899 - val_accuracy: 0.2050\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 16.5005 - accuracy: 0.4207 - val_loss: 30.7766 - val_accuracy: 0.1988\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 10.4958 - accuracy: 0.5120 - val_loss: 35.0262 - val_accuracy: 0.1771\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 9.0714 - accuracy: 0.5826 - val_loss: 32.4986 - val_accuracy: 0.1767\n",
      "CV iteration 5 of 13\n",
      "Validation subjects are ['p021' 'p045' 'p056']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 70.9488 - accuracy: 0.1418 - val_loss: 34.8322 - val_accuracy: 0.1663\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 23.6026 - accuracy: 0.2989 - val_loss: 25.6425 - val_accuracy: 0.2313\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 15.9084 - accuracy: 0.4255 - val_loss: 20.5960 - val_accuracy: 0.2550\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 12.0664 - accuracy: 0.4935 - val_loss: 21.9621 - val_accuracy: 0.2517\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 8.4871 - accuracy: 0.5696 - val_loss: 19.3650 - val_accuracy: 0.2850\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 26s 227ms/step - loss: 6.3041 - accuracy: 0.6413 - val_loss: 21.5181 - val_accuracy: 0.2533\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 5.8301 - accuracy: 0.6598 - val_loss: 18.7169 - val_accuracy: 0.2879\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 4.8373 - accuracy: 0.7158 - val_loss: 15.6359 - val_accuracy: 0.3396\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 3.4435 - accuracy: 0.7745 - val_loss: 17.3445 - val_accuracy: 0.3258\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 213ms/step - loss: 2.9863 - accuracy: 0.7870 - val_loss: 15.8181 - val_accuracy: 0.3579\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 2.4343 - accuracy: 0.8120 - val_loss: 18.1653 - val_accuracy: 0.3171\n",
      "CV iteration 6 of 13\n",
      "Validation subjects are ['p042' 'p050' 'p049']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 65.6604 - accuracy: 0.1674 - val_loss: 54.6589 - val_accuracy: 0.2025\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 23.2401 - accuracy: 0.3033 - val_loss: 47.1943 - val_accuracy: 0.2496\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 15.2282 - accuracy: 0.4261 - val_loss: 29.4036 - val_accuracy: 0.3350\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 10.4434 - accuracy: 0.5168 - val_loss: 28.8443 - val_accuracy: 0.3054\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 7.7665 - accuracy: 0.5864 - val_loss: 25.7242 - val_accuracy: 0.3575\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 5.9223 - accuracy: 0.6484 - val_loss: 26.7747 - val_accuracy: 0.3433\n",
      "CV iteration 7 of 13\n",
      "Validation subjects are ['p002' 'p049' 'p045']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 64.2920 - accuracy: 0.1832 - val_loss: 39.9148 - val_accuracy: 0.1925\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 207ms/step - loss: 23.2603 - accuracy: 0.2973 - val_loss: 35.9264 - val_accuracy: 0.2138\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 207ms/step - loss: 14.9348 - accuracy: 0.4293 - val_loss: 26.7023 - val_accuracy: 0.2371\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 10.4464 - accuracy: 0.5136 - val_loss: 29.2199 - val_accuracy: 0.2925\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 7.7370 - accuracy: 0.5902 - val_loss: 21.0664 - val_accuracy: 0.3787\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 6.1224 - accuracy: 0.6505 - val_loss: 26.4768 - val_accuracy: 0.2750 6.2754 - accuracy:  - ETA: 3s - loss: 6\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 5.1235 - accuracy: 0.6951 - val_loss: 22.7199 - val_accuracy: 0.3413\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 3.9102 - accuracy: 0.7446 - val_loss: 26.8081 - val_accuracy: 0.3429\n",
      "CV iteration 8 of 13\n",
      "Validation subjects are ['p061' 'p012' 'p041']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 67.6192 - accuracy: 0.1690 - val_loss: 40.6854 - val_accuracy: 0.1900\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 21.7277 - accuracy: 0.3348 - val_loss: 36.7369 - val_accuracy: 0.2267\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 13.8181 - accuracy: 0.4489 - val_loss: 26.3733 - val_accuracy: 0.2592\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 9.8180 - accuracy: 0.5261 - val_loss: 26.9052 - val_accuracy: 0.2546\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 7.6989 - accuracy: 0.5973 - val_loss: 23.6627 - val_accuracy: 0.3033\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 6.4254 - accuracy: 0.6304 - val_loss: 22.1724 - val_accuracy: 0.3342\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 4.6145 - accuracy: 0.7207 - val_loss: 21.3616 - val_accuracy: 0.3137\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 4.0870 - accuracy: 0.7505 - val_loss: 20.9065 - val_accuracy: 0.3104\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 2.8875 - accuracy: 0.8022 - val_loss: 20.1722 - val_accuracy: 0.3225\n",
      "CV iteration 9 of 13\n",
      "Validation subjects are ['p026' 'p049' 'p015']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 62.9042 - accuracy: 0.1712 - val_loss: 52.4665 - val_accuracy: 0.1500\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 21.7978 - accuracy: 0.3277 - val_loss: 43.3760 - val_accuracy: 0.2104\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 13.8630 - accuracy: 0.4620 - val_loss: 36.1465 - val_accuracy: 0.2304\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 8.7117 - accuracy: 0.5658 - val_loss: 36.3627 - val_accuracy: 0.2554\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 6.4165 - accuracy: 0.6348 - val_loss: 33.1364 - val_accuracy: 0.2800\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 5.1543 - accuracy: 0.6848 - val_loss: 32.2411 - val_accuracy: 0.2912\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 26s 224ms/step - loss: 4.8360 - accuracy: 0.7060 - val_loss: 32.4132 - val_accuracy: 0.3079\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 3.7087 - accuracy: 0.7571 - val_loss: 28.8457 - val_accuracy: 0.3096\n",
      "CV iteration 10 of 13\n",
      "Validation subjects are ['p052' 'p045' 'p051']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 62.0044 - accuracy: 0.1793 - val_loss: 31.9811 - val_accuracy: 0.1767\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 22.7660 - accuracy: 0.3158 - val_loss: 26.9253 - val_accuracy: 0.2562\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 15.2295 - accuracy: 0.4304 - val_loss: 19.5690 - val_accuracy: 0.3092\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 10.4113 - accuracy: 0.5174 - val_loss: 17.4305 - val_accuracy: 0.3492\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 8.2267 - accuracy: 0.5772 - val_loss: 16.7742 - val_accuracy: 0.3554\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 6.6945 - accuracy: 0.6277 - val_loss: 16.9059 - val_accuracy: 0.3479\n",
      "CV iteration 11 of 13\n",
      "Validation subjects are ['p045' 'p021' 'p016']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 65.1546 - accuracy: 0.1636 - val_loss: 33.2319 - val_accuracy: 0.1675\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 23.0200 - accuracy: 0.3299 - val_loss: 22.0789 - val_accuracy: 0.2225\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 14.0470 - accuracy: 0.4467 - val_loss: 21.8051 - val_accuracy: 0.2167\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 10.0320 - accuracy: 0.5092 - val_loss: 21.0910 - val_accuracy: 0.2350\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 7.3630 - accuracy: 0.6141 - val_loss: 18.9741 - val_accuracy: 0.2754\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 26s 225ms/step - loss: 6.0123 - accuracy: 0.6576 - val_loss: 19.8837 - val_accuracy: 0.2354\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 27s 236ms/step - loss: 4.4206 - accuracy: 0.7332 - val_loss: 22.4483 - val_accuracy: 0.2587\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 3.8775 - accuracy: 0.7636 - val_loss: 17.6746 - val_accuracy: 0.2958\n",
      "CV iteration 12 of 13\n",
      "Validation subjects are ['p022' 'p064' 'p035']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 65.1850 - accuracy: 0.1630 - val_loss: 53.1196 - val_accuracy: 0.1700\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 24s 208ms/step - loss: 21.0940 - accuracy: 0.3152 - val_loss: 48.7487 - val_accuracy: 0.1933\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 13.6854 - accuracy: 0.4571 - val_loss: 38.4257 - val_accuracy: 0.1900\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 10.0064 - accuracy: 0.5332 - val_loss: 29.2126 - val_accuracy: 0.2517\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 7.2258 - accuracy: 0.6076 - val_loss: 33.0804 - val_accuracy: 0.1921\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 6.0985 - accuracy: 0.6402 - val_loss: 24.0196 - val_accuracy: 0.2858\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 4.7816 - accuracy: 0.7049 - val_loss: 24.7684 - val_accuracy: 0.2763\n",
      "CV iteration 13 of 13\n",
      "Validation subjects are ['p021' 'p061' 'p042']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 65.6681 - accuracy: 0.1859 - val_loss: 32.7522 - val_accuracy: 0.2196\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 27s 237ms/step - loss: 22.9929 - accuracy: 0.3147 - val_loss: 32.2186 - val_accuracy: 0.1896\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 26s 225ms/step - loss: 15.8718 - accuracy: 0.4087 - val_loss: 20.1037 - val_accuracy: 0.2887\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 9.7286 - accuracy: 0.5370 - val_loss: 22.3998 - val_accuracy: 0.3000\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 7.4709 - accuracy: 0.5989 - val_loss: 26.1068 - val_accuracy: 0.2479\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 5.8947 - accuracy: 0.6614 - val_loss: 21.4360 - val_accuracy: 0.2871\n"
     ]
    }
   ],
   "source": [
    "model1data = cvrand(model1, \n",
    "                    df,\n",
    "                    n_iterations=13,\n",
    "                    batch_size=16,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch=115,\n",
    "                    target_size=(256,256),\n",
    "                    random_state=42,\n",
    "                    min_delta=0.05,\n",
    "                    patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration</th>\n",
       "      <th>sampledvalues</th>\n",
       "      <th>train_accuracies</th>\n",
       "      <th>validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[p026, p050, p002]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[p052, p039, p024]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[p049, p064, p042]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[p051, p066, p014]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[p021, p045, p056]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>[p042, p050, p049]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>[p002, p049, p045]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>[p061, p012, p041]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>[p026, p049, p015]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>[p052, p045, p051]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>[p045, p021, p016]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>[p022, p064, p035]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>[p021, p061, p042]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    iteration       sampledvalues  train_accuracies  validation_accuracy\n",
       "0           1  [p026, p050, p002]                 0                0.166\n",
       "1           2  [p052, p039, p024]                 1                0.337\n",
       "2           3  [p049, p064, p042]                 1                0.302\n",
       "3           4  [p051, p066, p014]                 0                0.205\n",
       "4           5  [p021, p045, p056]                 1                0.358\n",
       "5           6  [p042, p050, p049]                 1                0.358\n",
       "6           7  [p002, p049, p045]                 1                0.379\n",
       "7           8  [p061, p012, p041]                 1                0.334\n",
       "8           9  [p026, p049, p015]                 1                0.310\n",
       "9          10  [p052, p045, p051]                 1                0.355\n",
       "10         11  [p045, p021, p016]                 1                0.296\n",
       "11         12  [p022, p064, p035]                 1                0.286\n",
       "12         13  [p021, p061, p042]                 1                0.300"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1data.to_csv('../metrics/model1metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 - Add Dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Conv2D(10, kernel_size=3, activation='relu', input_shape=(256, 256, 3)))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(MaxPool2D(10))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "model2.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2data = cvrand(model2, \n",
    "                    df,\n",
    "                    n_iterations=30,\n",
    "                    batch_size=16,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch=125,\n",
    "                    target_size=(256, 256),\n",
    "                    random_state=42,\n",
    "                    min_delta=0.05,\n",
    "                    patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2data.to_csv('../metrics/model2metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 - Architecture Modeled off AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Conv2D(99,\n",
    "                 kernel_size=11,\n",
    "                 strides=4,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 input_shape=(227, 227, 3)))\n",
    "model3.add(MaxPool2D(3,\n",
    "                    strides=2,\n",
    "                    padding='valid'))\n",
    "model3.add(Conv2D(256,\n",
    "                 kernel_size=5,\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model3.add(MaxPool2D(3,\n",
    "                    strides=2,\n",
    "                    padding='valid'))\n",
    "model3.add(Conv2D(384,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model3.add(Conv2D(384,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model3.add(Conv2D(256,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(100, activation='relu'))\n",
    "model3.add(Dense(100, activation='relu'))\n",
    "model3.add(Dense(10, activation='softmax'))\n",
    "opt = Adam(learning_rate=0.00001)\n",
    "model3.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model3data = cvrand(model3, \n",
    "                    df,\n",
    "                    n_iterations=30,\n",
    "                    batch_size=16,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch=125,\n",
    "                    target_size=(227,227),\n",
    "                    random_state=42,\n",
    "                    min_delta=0.05,\n",
    "                    patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model3data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3data.to_csv('../metrics/model3metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "notify.send('model 3 cv complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet Corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Conv2D(99,\n",
    "                  kernel_size=11,\n",
    "                  strides=4,\n",
    "                  padding='valid',\n",
    "                  input_shape=(227, 227, 3)))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(MaxPool2D(3,\n",
    "                     strides=2,\n",
    "                     padding='valid'))\n",
    "model4.add(Conv2D(256,\n",
    "                  kernel_size=5,\n",
    "                  strides=1,\n",
    "                  padding='same'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(MaxPool2D(3,\n",
    "                    strides=2,\n",
    "                    padding='valid'))\n",
    "model4.add(Conv2D(384,\n",
    "                  kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(Conv2D(384,\n",
    "                  kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(Conv2D(256,\n",
    "                  kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(MaxPool2D(3,\n",
    "                     strides=2,\n",
    "                     padding='valid'))\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(4096, activation='relu'))\n",
    "model4.add(Dropout(0.5))\n",
    "model4.add(Dense(4096, activation='relu'))\n",
    "model4.add(Dropout(0.5))\n",
    "model4.add(Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run test changing learning rate to see how model changes.\n",
    "Staring with Adam, lr=0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_20 (Conv2D)           (None, 55, 55, 99)        36036     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 55, 55, 99)        396       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 55, 55, 99)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 27, 27, 99)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 27, 27, 256)       633856    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 27, 27, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 27, 27, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 13, 13, 384)       885120    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 13, 13, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 13, 13, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 13, 13, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 13, 13, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 13, 13, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 13, 13, 256)       884992    \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 13, 13, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4096)              37752832  \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 58,348,122\n",
      "Trainable params: 58,345,364\n",
      "Non-trainable params: 2,758\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(learning_rate=0.00001)\n",
    "model4.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a batch size of 16 and training set consisting of 18,400 images, performing 50 epochs of 115 steps will mean that the training data is gone over 5 times. Early stopping callback is set to 10, so if the validation accuracy does not improve 10 times in a row then the training will cease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV iteration 1 of 13\n",
      "Validation subjects are ['p026' 'p050' 'p002']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 34s 299ms/step - loss: 3.2985 - accuracy: 0.1277 - val_loss: 2.2918 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 32s 279ms/step - loss: 2.4313 - accuracy: 0.2092 - val_loss: 2.2793 - val_accuracy: 0.1554\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 31s 269ms/step - loss: 2.0500 - accuracy: 0.2832 - val_loss: 2.1878 - val_accuracy: 0.2071\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 32s 282ms/step - loss: 1.7953 - accuracy: 0.3793 - val_loss: 2.2354 - val_accuracy: 0.2075\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 38s 332ms/step - loss: 1.5890 - accuracy: 0.4582 - val_loss: 2.0535 - val_accuracy: 0.2713\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 41s 355ms/step - loss: 1.4160 - accuracy: 0.5228 - val_loss: 1.9745 - val_accuracy: 0.3150\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 1.2384 - accuracy: 0.5707 - val_loss: 2.1231 - val_accuracy: 0.2708\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 213ms/step - loss: 1.1188 - accuracy: 0.6174 - val_loss: 2.1075 - val_accuracy: 0.2854\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 24s 207ms/step - loss: 0.9434 - accuracy: 0.6821 - val_loss: 1.9913 - val_accuracy: 0.2717\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 32s 276ms/step - loss: 0.8102 - accuracy: 0.7359 - val_loss: 1.9103 - val_accuracy: 0.3496\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 31s 267ms/step - loss: 0.7340 - accuracy: 0.7587 - val_loss: 1.8622 - val_accuracy: 0.3592\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 32s 276ms/step - loss: 0.6451 - accuracy: 0.8011 - val_loss: 1.8700 - val_accuracy: 0.3954\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.6044 - accuracy: 0.8076 - val_loss: 1.9051 - val_accuracy: 0.3446\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.5234 - accuracy: 0.8310 - val_loss: 1.8947 - val_accuracy: 0.3625\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 0.4200 - accuracy: 0.8647 - val_loss: 2.0599 - val_accuracy: 0.3613\n",
      "CV iteration 2 of 13\n",
      "Validation subjects are ['p052' 'p039' 'p024']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 3.2912 - accuracy: 0.1527 - val_loss: 2.1406 - val_accuracy: 0.2587\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 31s 266ms/step - loss: 2.3295 - accuracy: 0.2348 - val_loss: 1.9100 - val_accuracy: 0.4058\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 31s 269ms/step - loss: 1.9745 - accuracy: 0.3092 - val_loss: 1.7734 - val_accuracy: 0.4225\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 31s 272ms/step - loss: 1.7678 - accuracy: 0.3777 - val_loss: 1.5491 - val_accuracy: 0.5471\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 31s 266ms/step - loss: 1.5216 - accuracy: 0.4609 - val_loss: 1.3936 - val_accuracy: 0.5546\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 31s 267ms/step - loss: 1.3552 - accuracy: 0.5283 - val_loss: 1.3388 - val_accuracy: 0.6133\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.2131 - accuracy: 0.5935 - val_loss: 1.2446 - val_accuracy: 0.6108\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.0493 - accuracy: 0.6332 - val_loss: 1.2543 - val_accuracy: 0.5267\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 0.9040 - accuracy: 0.7000 - val_loss: 1.0796 - val_accuracy: 0.6112\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 33s 284ms/step - loss: 0.7985 - accuracy: 0.7228 - val_loss: 1.1006 - val_accuracy: 0.6175\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 31s 270ms/step - loss: 0.6847 - accuracy: 0.7788 - val_loss: 1.0017 - val_accuracy: 0.6617\n",
      "CV iteration 3 of 13\n",
      "Validation subjects are ['p049' 'p064' 'p042']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 3.2925 - accuracy: 0.1418 - val_loss: 2.2386 - val_accuracy: 0.2467\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 2.2933 - accuracy: 0.2408 - val_loss: 1.9283 - val_accuracy: 0.3071\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.8971 - accuracy: 0.3462 - val_loss: 1.7006 - val_accuracy: 0.3663\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.6613 - accuracy: 0.4076 - val_loss: 1.5213 - val_accuracy: 0.5067\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.4520 - accuracy: 0.4842 - val_loss: 1.5264 - val_accuracy: 0.4696\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 1.2956 - accuracy: 0.5489 - val_loss: 1.3788 - val_accuracy: 0.4796\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.1169 - accuracy: 0.6299 - val_loss: 1.3053 - val_accuracy: 0.5213\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 0.9887 - accuracy: 0.6772 - val_loss: 1.1671 - val_accuracy: 0.5417\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 0.8404 - accuracy: 0.7266 - val_loss: 1.1029 - val_accuracy: 0.6354\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 24s 213ms/step - loss: 0.7517 - accuracy: 0.7679 - val_loss: 1.0669 - val_accuracy: 0.6150\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 0.6649 - accuracy: 0.7832 - val_loss: 1.0962 - val_accuracy: 0.6183\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 0.5919 - accuracy: 0.8120 - val_loss: 1.2088 - val_accuracy: 0.5833\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 31s 269ms/step - loss: 0.5096 - accuracy: 0.8457 - val_loss: 0.9597 - val_accuracy: 0.6687\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 27s 232ms/step - loss: 0.4594 - accuracy: 0.8625 - val_loss: 1.1030 - val_accuracy: 0.6400\n",
      "CV iteration 4 of 13\n",
      "Validation subjects are ['p051' 'p066' 'p014']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 213ms/step - loss: 3.2814 - accuracy: 0.1576 - val_loss: 2.2100 - val_accuracy: 0.1892\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 2.2735 - accuracy: 0.2446 - val_loss: 2.0390 - val_accuracy: 0.2842\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 1.9233 - accuracy: 0.3283 - val_loss: 1.8461 - val_accuracy: 0.3858\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 1.6667 - accuracy: 0.4288 - val_loss: 1.7303 - val_accuracy: 0.4229\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 1.4729 - accuracy: 0.4810 - val_loss: 1.6069 - val_accuracy: 0.4483\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 25s 217ms/step - loss: 1.2763 - accuracy: 0.5522 - val_loss: 1.5978 - val_accuracy: 0.4800\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 1.1510 - accuracy: 0.6054 - val_loss: 1.5751 - val_accuracy: 0.3558\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 24s 213ms/step - loss: 0.9962 - accuracy: 0.6668 - val_loss: 1.3133 - val_accuracy: 0.4992\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.8731 - accuracy: 0.7005 - val_loss: 1.3224 - val_accuracy: 0.4992\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 26s 225ms/step - loss: 0.7422 - accuracy: 0.7576 - val_loss: 1.4454 - val_accuracy: 0.4592\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 26s 227ms/step - loss: 0.6994 - accuracy: 0.7728 - val_loss: 1.2345 - val_accuracy: 0.5250\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 0.5614 - accuracy: 0.8375 - val_loss: 1.2119 - val_accuracy: 0.5462\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 0.5209 - accuracy: 0.8386 - val_loss: 1.2367 - val_accuracy: 0.5442\n",
      "CV iteration 5 of 13\n",
      "Validation subjects are ['p021' 'p045' 'p056']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 27s 233ms/step - loss: 3.2678 - accuracy: 0.1397 - val_loss: 2.1609 - val_accuracy: 0.2604\n",
      "Epoch 2/50\n",
      "114/115 [============================>.] - ETA: 0s - loss: 2.2843 - accuracy: 0.2401"
     ]
    }
   ],
   "source": [
    "model4data = cvrand(model4, \n",
    "                    df,\n",
    "                    n_iterations=13,\n",
    "                    batch_size=16,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch=115,\n",
    "                    target_size=(227,227),\n",
    "                    random_state=42,\n",
    "                    min_delta=0.05,\n",
    "                    patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4data.to_csv('../metrics/model4metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notify.send('Model 4 fitting complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try SGD instead of Adam. Same learning rate. Momentum=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 55, 55, 99)        36036     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 55, 55, 99)        396       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 55, 55, 99)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 27, 27, 99)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 27, 27, 256)       633856    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 27, 27, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 27, 27, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 13, 13, 384)       885120    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 13, 13, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 13, 13, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 13, 13, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 13, 13, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 13, 13, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 13, 13, 256)       884992    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 13, 13, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4096)              37752832  \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 58,348,122\n",
      "Trainable params: 58,345,364\n",
      "Non-trainable params: 2,758\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = SGD(learning_rate=0.00001, momentum=0.9)\n",
    "model4.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV iteration 1 of 3\n",
      "Validation subjects are ['p026' 'p050' 'p002']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 30s 264ms/step - loss: 3.6087 - accuracy: 0.1185 - val_loss: 2.3625 - val_accuracy: 0.0925\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 35s 304ms/step - loss: 3.2773 - accuracy: 0.1239 - val_loss: 2.3285 - val_accuracy: 0.1308\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 27s 235ms/step - loss: 2.9305 - accuracy: 0.1478 - val_loss: 2.3546 - val_accuracy: 0.1279\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 2.7165 - accuracy: 0.1598 - val_loss: 2.2846 - val_accuracy: 0.1283\n",
      "CV iteration 2 of 3\n",
      "Validation subjects are ['p052' 'p039' 'p024']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 3.6980 - accuracy: 0.1065 - val_loss: 2.3698 - val_accuracy: 0.0921\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 30s 260ms/step - loss: 3.2548 - accuracy: 0.1310 - val_loss: 2.2168 - val_accuracy: 0.1737\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 30s 260ms/step - loss: 2.9943 - accuracy: 0.1304 - val_loss: 2.1429 - val_accuracy: 0.2579\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 31s 265ms/step - loss: 2.6523 - accuracy: 0.1859 - val_loss: 2.0600 - val_accuracy: 0.3338\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 31s 268ms/step - loss: 2.5820 - accuracy: 0.1783 - val_loss: 2.0253 - val_accuracy: 0.3592\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 29s 253ms/step - loss: 2.4431 - accuracy: 0.2076 - val_loss: 1.9480 - val_accuracy: 0.4196\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 213ms/step - loss: 2.2968 - accuracy: 0.2147 - val_loss: 1.8819 - val_accuracy: 0.4125\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 2.1864 - accuracy: 0.2527 - val_loss: 1.8086 - val_accuracy: 0.4008\n",
      "Epoch 9/50\n",
      "114/115 [============================>.] - ETA: 0s - loss: 2.0866 - accuracy: 0.2802 ETA: 1s - losWARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "115/115 [==============================] - 11s 99ms/step - loss: 2.0827 - accuracy: 0.2821\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-669db656990e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m                     \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                     \u001b[0mmin_delta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                     patience=3)\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Dylan\\Desktop\\Data Science\\Projects\\DistractedDrivers\\functions\\ddfuncs.py\u001b[0m in \u001b[0;36mcvrand\u001b[1;34m(model, data, itercol, n_iterations, val_subjects, epochs, batch_size, steps_per_epoch, validation_steps, target_size, random_state, min_delta, patience)\u001b[0m\n\u001b[0;32m    292\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m                   \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m                   callbacks=callbacks_list)\n\u001b[0m\u001b[0;32m    295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;31m# Append lists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dylan\\anaconda3\\envs\\DistractedDrivers\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\Users\\Dylan\\anaconda3\\envs\\DistractedDrivers\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    393\u001b[0m                       \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m                       \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m                       total_epochs=1)\n\u001b[0m\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[0;32m    397\u001b[0m                                  prefix='val_')\n",
      "\u001b[1;32mC:\\Users\\Dylan\\anaconda3\\envs\\DistractedDrivers\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dylan\\anaconda3\\envs\\DistractedDrivers\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dylan\\anaconda3\\envs\\DistractedDrivers\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dylan\\anaconda3\\envs\\DistractedDrivers\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    604\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mC:\\Users\\Dylan\\anaconda3\\envs\\DistractedDrivers\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dylan\\anaconda3\\envs\\DistractedDrivers\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dylan\\anaconda3\\envs\\DistractedDrivers\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dylan\\anaconda3\\envs\\DistractedDrivers\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\Users\\Dylan\\anaconda3\\envs\\DistractedDrivers\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model4data2 = cvrand(model4, \n",
    "                    df,\n",
    "                    n_iterations=3,\n",
    "                    batch_size=16,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch=115,\n",
    "                    target_size=(227,227),\n",
    "                    random_state=42,\n",
    "                    min_delta=0.05,\n",
    "                    patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
