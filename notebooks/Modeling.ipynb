{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPool2D, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Endpoint: <samp>https://notify.run/9CRUVPV2Tc5OBQpD</samp></p>\n",
       "<p>To subscribe, open: <a href=\"https://notify.run/c/9CRUVPV2Tc5OBQpD\">https://notify.run/c/9CRUVPV2Tc5OBQpD</a></p>\n",
       "<p>Or scan this QR code:</p>\n",
       "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"222\" width=\"222\" class=\"pyqrcode\"><path transform=\"scale(6)\" stroke=\"#000\" class=\"pyqrline\" d=\"M4 4.5h7m1 0h1m4 0h2m1 0h1m1 0h2m2 0h7m-29 1h1m5 0h1m1 0h1m2 0h3m3 0h3m2 0h1m5 0h1m-29 1h1m1 0h3m1 0h1m1 0h2m2 0h1m2 0h4m3 0h1m1 0h3m1 0h1m-29 1h1m1 0h3m1 0h1m1 0h3m1 0h1m2 0h1m1 0h4m1 0h1m1 0h3m1 0h1m-29 1h1m1 0h3m1 0h1m2 0h2m1 0h1m1 0h3m3 0h1m1 0h1m1 0h3m1 0h1m-29 1h1m5 0h1m1 0h3m5 0h4m2 0h1m5 0h1m-29 1h7m1 0h1m1 0h1m1 0h1m1 0h1m1 0h1m1 0h1m1 0h1m1 0h7m-19 1h2m1 0h1m1 0h3m1 0h2m-21 1h2m2 0h3m3 0h1m1 0h2m2 0h1m3 0h1m2 0h1m1 0h4m-28 1h3m1 0h1m7 0h2m1 0h1m3 0h1m1 0h7m-29 1h7m1 0h3m4 0h4m1 0h1m1 0h1m5 0h1m-29 1h2m3 0h1m2 0h2m2 0h1m1 0h1m1 0h4m1 0h1m1 0h3m1 0h2m-28 1h1m4 0h2m3 0h1m1 0h2m4 0h3m5 0h1m-28 1h2m2 0h2m1 0h4m1 0h1m3 0h1m4 0h8m-25 1h4m1 0h7m2 0h1m1 0h2m1 0h1m1 0h2m1 0h1m-29 1h1m3 0h2m2 0h1m1 0h2m1 0h1m3 0h1m1 0h2m1 0h1m4 0h2m-29 1h1m1 0h3m1 0h3m3 0h2m1 0h1m4 0h1m2 0h1m3 0h1m-28 1h3m2 0h1m2 0h2m3 0h2m1 0h1m3 0h1m1 0h4m1 0h2m-27 1h2m2 0h1m3 0h1m4 0h1m1 0h1m6 0h1m1 0h1m1 0h1m-26 1h2m4 0h1m2 0h1m1 0h2m1 0h1m1 0h3m2 0h1m2 0h2m-29 1h3m1 0h3m1 0h1m1 0h2m1 0h2m1 0h1m3 0h6m2 0h1m-21 1h2m2 0h1m2 0h1m3 0h2m3 0h1m3 0h1m-29 1h7m3 0h6m2 0h3m1 0h1m1 0h3m1 0h1m-29 1h1m5 0h1m1 0h1m1 0h2m1 0h1m2 0h2m1 0h2m3 0h1m3 0h1m-29 1h1m1 0h3m1 0h1m1 0h2m2 0h2m3 0h9m1 0h1m-28 1h1m1 0h3m1 0h1m2 0h2m2 0h4m3 0h2m6 0h1m-29 1h1m1 0h3m1 0h1m3 0h1m4 0h2m2 0h1m5 0h4m-29 1h1m5 0h1m1 0h2m2 0h1m1 0h2m1 0h1m1 0h7m1 0h2m-29 1h7m1 0h1m1 0h2m1 0h2m1 0h1m3 0h1m1 0h2m1 0h1m1 0h1\"/></svg>\n",
       "\n",
       "        "
      ],
      "text/plain": [
       "Endpoint: https://notify.run/9CRUVPV2Tc5OBQpD\n",
       "To subscribe, open: https://notify.run/c/9CRUVPV2Tc5OBQpD\n",
       "Or scan this QR code:\n",
       "\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[49m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\u001b[7m  \u001b[0m\n",
       "\n",
       "        "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notify_run import Notify\n",
    "notify = Notify()\n",
    "notify.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom functions\n",
    "sys.path.append('C:\\\\Users\\\\Dylan\\\\Desktop\\\\Data Science\\\\Projects\\\\DistractedDrivers\\\\functions')\n",
    "from ddfuncs import trainsampling, cvrand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Set memory limit on GPU to keep it from freezing up when fitting TensorFlow models later\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB * 3 of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], \\\n",
    "                                                                [tf.config.experimental.\\\n",
    "                                                                 VirtualDeviceConfiguration\\\n",
    "                                                                 (memory_limit=1024 * 3)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed training data\n",
    "os.chdir('../data/processed')\n",
    "df = pd.read_csv('driver_image_list_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trainsampling(df, samples=80, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>classname</th>\n",
       "      <th>img</th>\n",
       "      <th>imgpath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_13073.jpg</td>\n",
       "      <td>imgs/train/c0/img_13073.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_5585.jpg</td>\n",
       "      <td>imgs/train/c0/img_5585.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_48187.jpg</td>\n",
       "      <td>imgs/train/c0/img_48187.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_98115.jpg</td>\n",
       "      <td>imgs/train/c0/img_98115.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>p002</td>\n",
       "      <td>c0</td>\n",
       "      <td>img_66355.jpg</td>\n",
       "      <td>imgs/train/c0/img_66355.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22364</th>\n",
       "      <td>p081</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_95966.jpg</td>\n",
       "      <td>imgs/train/c9/img_95966.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22404</th>\n",
       "      <td>p081</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_18412.jpg</td>\n",
       "      <td>imgs/train/c9/img_18412.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22415</th>\n",
       "      <td>p081</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_23818.jpg</td>\n",
       "      <td>imgs/train/c9/img_23818.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22358</th>\n",
       "      <td>p081</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_54961.jpg</td>\n",
       "      <td>imgs/train/c9/img_54961.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22395</th>\n",
       "      <td>p081</td>\n",
       "      <td>c9</td>\n",
       "      <td>img_13314.jpg</td>\n",
       "      <td>imgs/train/c9/img_13314.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20800 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject classname            img                      imgpath\n",
       "51       p002        c0  img_13073.jpg  imgs/train/c0/img_13073.jpg\n",
       "14       p002        c0   img_5585.jpg   imgs/train/c0/img_5585.jpg\n",
       "71       p002        c0  img_48187.jpg  imgs/train/c0/img_48187.jpg\n",
       "60       p002        c0  img_98115.jpg  imgs/train/c0/img_98115.jpg\n",
       "20       p002        c0  img_66355.jpg  imgs/train/c0/img_66355.jpg\n",
       "...       ...       ...            ...                          ...\n",
       "22364    p081        c9  img_95966.jpg  imgs/train/c9/img_95966.jpg\n",
       "22404    p081        c9  img_18412.jpg  imgs/train/c9/img_18412.jpg\n",
       "22415    p081        c9  img_23818.jpg  imgs/train/c9/img_23818.jpg\n",
       "22358    p081        c9  img_54961.jpg  imgs/train/c9/img_54961.jpg\n",
       "22395    p081        c9  img_13314.jpg  imgs/train/c9/img_13314.jpg\n",
       "\n",
       "[20800 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to appropriate directory for data generation\n",
    "os.chdir('../raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 - 1 Conv, 1 MaxPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 254, 254, 10)      280       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 25, 25, 10)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6250)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                62510     \n",
      "=================================================================\n",
      "Total params: 62,790\n",
      "Trainable params: 62,790\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Conv2D(10, kernel_size=3, activation='relu', input_shape=(256, 256, 3)))\n",
    "model1.add(MaxPool2D(10))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(10, activation='softmax'))\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "model1.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV iteration 1 of 13\n",
      "Validation subjects are ['p026' 'p050' 'p002']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 30s 262ms/step - loss: 63.9846 - accuracy: 0.1495 - val_loss: 57.7408 - val_accuracy: 0.1050\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 30s 260ms/step - loss: 27.8950 - accuracy: 0.2772 - val_loss: 45.9950 - val_accuracy: 0.1550\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 32s 278ms/step - loss: 18.5662 - accuracy: 0.3696 - val_loss: 44.0138 - val_accuracy: 0.1650\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 33s 287ms/step - loss: 13.2900 - accuracy: 0.4826 - val_loss: 41.8431 - val_accuracy: 0.1663\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 31s 270ms/step - loss: 10.8829 - accuracy: 0.5304 - val_loss: 43.3074 - val_accuracy: 0.1521\n",
      "CV iteration 2 of 13\n",
      "Validation subjects are ['p052' 'p039' 'p024']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 32s 281ms/step - loss: 65.8975 - accuracy: 0.1467 - val_loss: 29.4313 - val_accuracy: 0.1988\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 33s 284ms/step - loss: 26.0729 - accuracy: 0.2723 - val_loss: 29.0584 - val_accuracy: 0.2512\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 34s 295ms/step - loss: 18.3187 - accuracy: 0.3620 - val_loss: 26.8080 - val_accuracy: 0.2562\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 33s 286ms/step - loss: 11.7205 - accuracy: 0.4880 - val_loss: 25.2896 - val_accuracy: 0.2608\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 34s 292ms/step - loss: 8.9298 - accuracy: 0.5478 - val_loss: 19.0994 - val_accuracy: 0.3371\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 35s 303ms/step - loss: 7.3597 - accuracy: 0.6158 - val_loss: 23.2580 - val_accuracy: 0.2892\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 34s 295ms/step - loss: 6.1033 - accuracy: 0.6560 - val_loss: 21.2058 - val_accuracy: 0.3288\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 34s 294ms/step - loss: 4.6666 - accuracy: 0.7272 - val_loss: 21.8329 - val_accuracy: 0.3021\n",
      "CV iteration 3 of 13\n",
      "Validation subjects are ['p049' 'p064' 'p042']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 35s 302ms/step - loss: 63.5910 - accuracy: 0.1707 - val_loss: 44.1096 - val_accuracy: 0.2612\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 33s 286ms/step - loss: 23.7756 - accuracy: 0.3076 - val_loss: 36.3922 - val_accuracy: 0.2763\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 32s 279ms/step - loss: 16.2400 - accuracy: 0.3891 - val_loss: 32.7885 - val_accuracy: 0.2621\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 32s 281ms/step - loss: 11.2436 - accuracy: 0.5038 - val_loss: 29.1616 - val_accuracy: 0.3021\n",
      "CV iteration 4 of 13\n",
      "Validation subjects are ['p051' 'p066' 'p014']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 33s 284ms/step - loss: 61.8447 - accuracy: 0.1799 - val_loss: 51.0618 - val_accuracy: 0.1296\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 26s 230ms/step - loss: 24.1887 - accuracy: 0.3049 - val_loss: 43.7899 - val_accuracy: 0.2050\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 16.5005 - accuracy: 0.4207 - val_loss: 30.7766 - val_accuracy: 0.1988\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 10.4958 - accuracy: 0.5120 - val_loss: 35.0262 - val_accuracy: 0.1771\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 9.0714 - accuracy: 0.5826 - val_loss: 32.4986 - val_accuracy: 0.1767\n",
      "CV iteration 5 of 13\n",
      "Validation subjects are ['p021' 'p045' 'p056']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 70.9488 - accuracy: 0.1418 - val_loss: 34.8322 - val_accuracy: 0.1663\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 23.6026 - accuracy: 0.2989 - val_loss: 25.6425 - val_accuracy: 0.2313\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 15.9084 - accuracy: 0.4255 - val_loss: 20.5960 - val_accuracy: 0.2550\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 12.0664 - accuracy: 0.4935 - val_loss: 21.9621 - val_accuracy: 0.2517\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 8.4871 - accuracy: 0.5696 - val_loss: 19.3650 - val_accuracy: 0.2850\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 26s 227ms/step - loss: 6.3041 - accuracy: 0.6413 - val_loss: 21.5181 - val_accuracy: 0.2533\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 5.8301 - accuracy: 0.6598 - val_loss: 18.7169 - val_accuracy: 0.2879\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 4.8373 - accuracy: 0.7158 - val_loss: 15.6359 - val_accuracy: 0.3396\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 3.4435 - accuracy: 0.7745 - val_loss: 17.3445 - val_accuracy: 0.3258\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 213ms/step - loss: 2.9863 - accuracy: 0.7870 - val_loss: 15.8181 - val_accuracy: 0.3579\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 2.4343 - accuracy: 0.8120 - val_loss: 18.1653 - val_accuracy: 0.3171\n",
      "CV iteration 6 of 13\n",
      "Validation subjects are ['p042' 'p050' 'p049']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 65.6604 - accuracy: 0.1674 - val_loss: 54.6589 - val_accuracy: 0.2025\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 23.2401 - accuracy: 0.3033 - val_loss: 47.1943 - val_accuracy: 0.2496\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 15.2282 - accuracy: 0.4261 - val_loss: 29.4036 - val_accuracy: 0.3350\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 10.4434 - accuracy: 0.5168 - val_loss: 28.8443 - val_accuracy: 0.3054\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 7.7665 - accuracy: 0.5864 - val_loss: 25.7242 - val_accuracy: 0.3575\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 5.9223 - accuracy: 0.6484 - val_loss: 26.7747 - val_accuracy: 0.3433\n",
      "CV iteration 7 of 13\n",
      "Validation subjects are ['p002' 'p049' 'p045']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 64.2920 - accuracy: 0.1832 - val_loss: 39.9148 - val_accuracy: 0.1925\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 207ms/step - loss: 23.2603 - accuracy: 0.2973 - val_loss: 35.9264 - val_accuracy: 0.2138\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 207ms/step - loss: 14.9348 - accuracy: 0.4293 - val_loss: 26.7023 - val_accuracy: 0.2371\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 10.4464 - accuracy: 0.5136 - val_loss: 29.2199 - val_accuracy: 0.2925\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 7.7370 - accuracy: 0.5902 - val_loss: 21.0664 - val_accuracy: 0.3787\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 6.1224 - accuracy: 0.6505 - val_loss: 26.4768 - val_accuracy: 0.2750 6.2754 - accuracy:  - ETA: 3s - loss: 6\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 5.1235 - accuracy: 0.6951 - val_loss: 22.7199 - val_accuracy: 0.3413\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 3.9102 - accuracy: 0.7446 - val_loss: 26.8081 - val_accuracy: 0.3429\n",
      "CV iteration 8 of 13\n",
      "Validation subjects are ['p061' 'p012' 'p041']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 67.6192 - accuracy: 0.1690 - val_loss: 40.6854 - val_accuracy: 0.1900\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 21.7277 - accuracy: 0.3348 - val_loss: 36.7369 - val_accuracy: 0.2267\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 13.8181 - accuracy: 0.4489 - val_loss: 26.3733 - val_accuracy: 0.2592\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 9.8180 - accuracy: 0.5261 - val_loss: 26.9052 - val_accuracy: 0.2546\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 7.6989 - accuracy: 0.5973 - val_loss: 23.6627 - val_accuracy: 0.3033\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 6.4254 - accuracy: 0.6304 - val_loss: 22.1724 - val_accuracy: 0.3342\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 4.6145 - accuracy: 0.7207 - val_loss: 21.3616 - val_accuracy: 0.3137\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 4.0870 - accuracy: 0.7505 - val_loss: 20.9065 - val_accuracy: 0.3104\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 2.8875 - accuracy: 0.8022 - val_loss: 20.1722 - val_accuracy: 0.3225\n",
      "CV iteration 9 of 13\n",
      "Validation subjects are ['p026' 'p049' 'p015']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 62.9042 - accuracy: 0.1712 - val_loss: 52.4665 - val_accuracy: 0.1500\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 21.7978 - accuracy: 0.3277 - val_loss: 43.3760 - val_accuracy: 0.2104\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 13.8630 - accuracy: 0.4620 - val_loss: 36.1465 - val_accuracy: 0.2304\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 8.7117 - accuracy: 0.5658 - val_loss: 36.3627 - val_accuracy: 0.2554\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 6.4165 - accuracy: 0.6348 - val_loss: 33.1364 - val_accuracy: 0.2800\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 5.1543 - accuracy: 0.6848 - val_loss: 32.2411 - val_accuracy: 0.2912\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 26s 224ms/step - loss: 4.8360 - accuracy: 0.7060 - val_loss: 32.4132 - val_accuracy: 0.3079\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 3.7087 - accuracy: 0.7571 - val_loss: 28.8457 - val_accuracy: 0.3096\n",
      "CV iteration 10 of 13\n",
      "Validation subjects are ['p052' 'p045' 'p051']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 62.0044 - accuracy: 0.1793 - val_loss: 31.9811 - val_accuracy: 0.1767\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 22.7660 - accuracy: 0.3158 - val_loss: 26.9253 - val_accuracy: 0.2562\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 15.2295 - accuracy: 0.4304 - val_loss: 19.5690 - val_accuracy: 0.3092\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 10.4113 - accuracy: 0.5174 - val_loss: 17.4305 - val_accuracy: 0.3492\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 8.2267 - accuracy: 0.5772 - val_loss: 16.7742 - val_accuracy: 0.3554\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 6.6945 - accuracy: 0.6277 - val_loss: 16.9059 - val_accuracy: 0.3479\n",
      "CV iteration 11 of 13\n",
      "Validation subjects are ['p045' 'p021' 'p016']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 65.1546 - accuracy: 0.1636 - val_loss: 33.2319 - val_accuracy: 0.1675\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 23.0200 - accuracy: 0.3299 - val_loss: 22.0789 - val_accuracy: 0.2225\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 14.0470 - accuracy: 0.4467 - val_loss: 21.8051 - val_accuracy: 0.2167\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 10.0320 - accuracy: 0.5092 - val_loss: 21.0910 - val_accuracy: 0.2350\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 7.3630 - accuracy: 0.6141 - val_loss: 18.9741 - val_accuracy: 0.2754\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 26s 225ms/step - loss: 6.0123 - accuracy: 0.6576 - val_loss: 19.8837 - val_accuracy: 0.2354\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 27s 236ms/step - loss: 4.4206 - accuracy: 0.7332 - val_loss: 22.4483 - val_accuracy: 0.2587\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 3.8775 - accuracy: 0.7636 - val_loss: 17.6746 - val_accuracy: 0.2958\n",
      "CV iteration 12 of 13\n",
      "Validation subjects are ['p022' 'p064' 'p035']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 65.1850 - accuracy: 0.1630 - val_loss: 53.1196 - val_accuracy: 0.1700\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 24s 208ms/step - loss: 21.0940 - accuracy: 0.3152 - val_loss: 48.7487 - val_accuracy: 0.1933\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 13.6854 - accuracy: 0.4571 - val_loss: 38.4257 - val_accuracy: 0.1900\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 10.0064 - accuracy: 0.5332 - val_loss: 29.2126 - val_accuracy: 0.2517\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 7.2258 - accuracy: 0.6076 - val_loss: 33.0804 - val_accuracy: 0.1921\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 208ms/step - loss: 6.0985 - accuracy: 0.6402 - val_loss: 24.0196 - val_accuracy: 0.2858\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 4.7816 - accuracy: 0.7049 - val_loss: 24.7684 - val_accuracy: 0.2763\n",
      "CV iteration 13 of 13\n",
      "Validation subjects are ['p021' 'p061' 'p042']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 65.6681 - accuracy: 0.1859 - val_loss: 32.7522 - val_accuracy: 0.2196\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 27s 237ms/step - loss: 22.9929 - accuracy: 0.3147 - val_loss: 32.2186 - val_accuracy: 0.1896\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 26s 225ms/step - loss: 15.8718 - accuracy: 0.4087 - val_loss: 20.1037 - val_accuracy: 0.2887\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 9.7286 - accuracy: 0.5370 - val_loss: 22.3998 - val_accuracy: 0.3000\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 7.4709 - accuracy: 0.5989 - val_loss: 26.1068 - val_accuracy: 0.2479\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 5.8947 - accuracy: 0.6614 - val_loss: 21.4360 - val_accuracy: 0.2871\n"
     ]
    }
   ],
   "source": [
    "model1data = cvrand(model1, \n",
    "                    df,\n",
    "                    n_iterations=13,\n",
    "                    batch_size=16,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch=115,\n",
    "                    target_size=(256,256),\n",
    "                    random_state=42,\n",
    "                    min_delta=0.05,\n",
    "                    patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration</th>\n",
       "      <th>sampledvalues</th>\n",
       "      <th>train_accuracies</th>\n",
       "      <th>validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[p026, p050, p002]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[p052, p039, p024]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[p049, p064, p042]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[p051, p066, p014]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[p021, p045, p056]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>[p042, p050, p049]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>[p002, p049, p045]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>[p061, p012, p041]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>[p026, p049, p015]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>[p052, p045, p051]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>[p045, p021, p016]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>[p022, p064, p035]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>[p021, p061, p042]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    iteration       sampledvalues  train_accuracies  validation_accuracy\n",
       "0           1  [p026, p050, p002]                 0                0.166\n",
       "1           2  [p052, p039, p024]                 1                0.337\n",
       "2           3  [p049, p064, p042]                 1                0.302\n",
       "3           4  [p051, p066, p014]                 0                0.205\n",
       "4           5  [p021, p045, p056]                 1                0.358\n",
       "5           6  [p042, p050, p049]                 1                0.358\n",
       "6           7  [p002, p049, p045]                 1                0.379\n",
       "7           8  [p061, p012, p041]                 1                0.334\n",
       "8           9  [p026, p049, p015]                 1                0.310\n",
       "9          10  [p052, p045, p051]                 1                0.355\n",
       "10         11  [p045, p021, p016]                 1                0.296\n",
       "11         12  [p022, p064, p035]                 1                0.286\n",
       "12         13  [p021, p061, p042]                 1                0.300"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1data.to_csv('../metrics/model1metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 - Add Dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Conv2D(10, kernel_size=3, activation='relu', input_shape=(256, 256, 3)))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(MaxPool2D(10))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "model2.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2data = cvrand(model2, \n",
    "                    df,\n",
    "                    n_iterations=30,\n",
    "                    batch_size=16,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch=125,\n",
    "                    target_size=(256, 256),\n",
    "                    random_state=42,\n",
    "                    min_delta=0.05,\n",
    "                    patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2data.to_csv('../metrics/model2metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 - Architecture Modeled off AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Conv2D(99,\n",
    "                 kernel_size=11,\n",
    "                 strides=4,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 input_shape=(227, 227, 3)))\n",
    "model3.add(MaxPool2D(3,\n",
    "                    strides=2,\n",
    "                    padding='valid'))\n",
    "model3.add(Conv2D(256,\n",
    "                 kernel_size=5,\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model3.add(MaxPool2D(3,\n",
    "                    strides=2,\n",
    "                    padding='valid'))\n",
    "model3.add(Conv2D(384,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model3.add(Conv2D(384,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model3.add(Conv2D(256,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(100, activation='relu'))\n",
    "model3.add(Dense(100, activation='relu'))\n",
    "model3.add(Dense(10, activation='softmax'))\n",
    "opt = Adam(learning_rate=0.00001)\n",
    "model3.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model3data = cvrand(model3, \n",
    "                    df,\n",
    "                    n_iterations=30,\n",
    "                    batch_size=16,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch=125,\n",
    "                    target_size=(227,227),\n",
    "                    random_state=42,\n",
    "                    min_delta=0.05,\n",
    "                    patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model3data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3data.to_csv('../metrics/model3metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "notify.send('model 3 cv complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet Corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Conv2D(99,\n",
    "                  kernel_size=11,\n",
    "                  strides=4,\n",
    "                  padding='valid',\n",
    "                  input_shape=(227, 227, 3)))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(MaxPool2D(3,\n",
    "                     strides=2,\n",
    "                     padding='valid'))\n",
    "model4.add(Conv2D(256,\n",
    "                  kernel_size=5,\n",
    "                  strides=1,\n",
    "                  padding='same'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(MaxPool2D(3,\n",
    "                    strides=2,\n",
    "                    padding='valid'))\n",
    "model4.add(Conv2D(384,\n",
    "                  kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(Conv2D(384,\n",
    "                  kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(Conv2D(256,\n",
    "                  kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(MaxPool2D(3,\n",
    "                     strides=2,\n",
    "                     padding='valid'))\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(4096, activation='relu'))\n",
    "model4.add(Dropout(0.5))\n",
    "model4.add(Dense(4096, activation='relu'))\n",
    "model4.add(Dropout(0.5))\n",
    "model4.add(Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run test changing learning rate to see how model changes.\n",
    "Staring with Adam, lr=0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_20 (Conv2D)           (None, 55, 55, 99)        36036     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 55, 55, 99)        396       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 55, 55, 99)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 27, 27, 99)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 27, 27, 256)       633856    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 27, 27, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 27, 27, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 13, 13, 384)       885120    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 13, 13, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 13, 13, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 13, 13, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 13, 13, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 13, 13, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 13, 13, 256)       884992    \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 13, 13, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4096)              37752832  \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 58,348,122\n",
      "Trainable params: 58,345,364\n",
      "Non-trainable params: 2,758\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(learning_rate=0.00001)\n",
    "model4.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a batch size of 16 and training set consisting of 18,400 images, performing 50 epochs of 115 steps will mean that the training data is gone over 5 times. Early stopping callback is set to 10, so if the validation accuracy does not improve 10 times in a row then the training will cease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV iteration 1 of 13\n",
      "Validation subjects are ['p026' 'p050' 'p002']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 34s 299ms/step - loss: 3.2985 - accuracy: 0.1277 - val_loss: 2.2918 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 32s 279ms/step - loss: 2.4313 - accuracy: 0.2092 - val_loss: 2.2793 - val_accuracy: 0.1554\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 31s 269ms/step - loss: 2.0500 - accuracy: 0.2832 - val_loss: 2.1878 - val_accuracy: 0.2071\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 32s 282ms/step - loss: 1.7953 - accuracy: 0.3793 - val_loss: 2.2354 - val_accuracy: 0.2075\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 38s 332ms/step - loss: 1.5890 - accuracy: 0.4582 - val_loss: 2.0535 - val_accuracy: 0.2713\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 41s 355ms/step - loss: 1.4160 - accuracy: 0.5228 - val_loss: 1.9745 - val_accuracy: 0.3150\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 1.2384 - accuracy: 0.5707 - val_loss: 2.1231 - val_accuracy: 0.2708\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 213ms/step - loss: 1.1188 - accuracy: 0.6174 - val_loss: 2.1075 - val_accuracy: 0.2854\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 24s 207ms/step - loss: 0.9434 - accuracy: 0.6821 - val_loss: 1.9913 - val_accuracy: 0.2717\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 32s 276ms/step - loss: 0.8102 - accuracy: 0.7359 - val_loss: 1.9103 - val_accuracy: 0.3496\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 31s 267ms/step - loss: 0.7340 - accuracy: 0.7587 - val_loss: 1.8622 - val_accuracy: 0.3592\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 32s 276ms/step - loss: 0.6451 - accuracy: 0.8011 - val_loss: 1.8700 - val_accuracy: 0.3954\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.6044 - accuracy: 0.8076 - val_loss: 1.9051 - val_accuracy: 0.3446\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.5234 - accuracy: 0.8310 - val_loss: 1.8947 - val_accuracy: 0.3625\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 0.4200 - accuracy: 0.8647 - val_loss: 2.0599 - val_accuracy: 0.3613\n",
      "CV iteration 2 of 13\n",
      "Validation subjects are ['p052' 'p039' 'p024']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 3.2912 - accuracy: 0.1527 - val_loss: 2.1406 - val_accuracy: 0.2587\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 31s 266ms/step - loss: 2.3295 - accuracy: 0.2348 - val_loss: 1.9100 - val_accuracy: 0.4058\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 31s 269ms/step - loss: 1.9745 - accuracy: 0.3092 - val_loss: 1.7734 - val_accuracy: 0.4225\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 31s 272ms/step - loss: 1.7678 - accuracy: 0.3777 - val_loss: 1.5491 - val_accuracy: 0.5471\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 31s 266ms/step - loss: 1.5216 - accuracy: 0.4609 - val_loss: 1.3936 - val_accuracy: 0.5546\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 31s 267ms/step - loss: 1.3552 - accuracy: 0.5283 - val_loss: 1.3388 - val_accuracy: 0.6133\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.2131 - accuracy: 0.5935 - val_loss: 1.2446 - val_accuracy: 0.6108\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.0493 - accuracy: 0.6332 - val_loss: 1.2543 - val_accuracy: 0.5267\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 0.9040 - accuracy: 0.7000 - val_loss: 1.0796 - val_accuracy: 0.6112\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 33s 284ms/step - loss: 0.7985 - accuracy: 0.7228 - val_loss: 1.1006 - val_accuracy: 0.6175\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 31s 270ms/step - loss: 0.6847 - accuracy: 0.7788 - val_loss: 1.0017 - val_accuracy: 0.6617\n",
      "CV iteration 3 of 13\n",
      "Validation subjects are ['p049' 'p064' 'p042']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 3.2925 - accuracy: 0.1418 - val_loss: 2.2386 - val_accuracy: 0.2467\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 2.2933 - accuracy: 0.2408 - val_loss: 1.9283 - val_accuracy: 0.3071\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.8971 - accuracy: 0.3462 - val_loss: 1.7006 - val_accuracy: 0.3663\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.6613 - accuracy: 0.4076 - val_loss: 1.5213 - val_accuracy: 0.5067\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.4520 - accuracy: 0.4842 - val_loss: 1.5264 - val_accuracy: 0.4696\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 1.2956 - accuracy: 0.5489 - val_loss: 1.3788 - val_accuracy: 0.4796\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.1169 - accuracy: 0.6299 - val_loss: 1.3053 - val_accuracy: 0.5213\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 0.9887 - accuracy: 0.6772 - val_loss: 1.1671 - val_accuracy: 0.5417\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 0.8404 - accuracy: 0.7266 - val_loss: 1.1029 - val_accuracy: 0.6354\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 24s 213ms/step - loss: 0.7517 - accuracy: 0.7679 - val_loss: 1.0669 - val_accuracy: 0.6150\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 0.6649 - accuracy: 0.7832 - val_loss: 1.0962 - val_accuracy: 0.6183\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 0.5919 - accuracy: 0.8120 - val_loss: 1.2088 - val_accuracy: 0.5833\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 31s 269ms/step - loss: 0.5096 - accuracy: 0.8457 - val_loss: 0.9597 - val_accuracy: 0.6687\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 27s 232ms/step - loss: 0.4594 - accuracy: 0.8625 - val_loss: 1.1030 - val_accuracy: 0.6400\n",
      "CV iteration 4 of 13\n",
      "Validation subjects are ['p051' 'p066' 'p014']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 213ms/step - loss: 3.2814 - accuracy: 0.1576 - val_loss: 2.2100 - val_accuracy: 0.1892\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 2.2735 - accuracy: 0.2446 - val_loss: 2.0390 - val_accuracy: 0.2842\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 1.9233 - accuracy: 0.3283 - val_loss: 1.8461 - val_accuracy: 0.3858\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 1.6667 - accuracy: 0.4288 - val_loss: 1.7303 - val_accuracy: 0.4229\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 1.4729 - accuracy: 0.4810 - val_loss: 1.6069 - val_accuracy: 0.4483\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 25s 217ms/step - loss: 1.2763 - accuracy: 0.5522 - val_loss: 1.5978 - val_accuracy: 0.4800\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 1.1510 - accuracy: 0.6054 - val_loss: 1.5751 - val_accuracy: 0.3558\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 24s 213ms/step - loss: 0.9962 - accuracy: 0.6668 - val_loss: 1.3133 - val_accuracy: 0.4992\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 0.8731 - accuracy: 0.7005 - val_loss: 1.3224 - val_accuracy: 0.4992\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 26s 225ms/step - loss: 0.7422 - accuracy: 0.7576 - val_loss: 1.4454 - val_accuracy: 0.4592\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 26s 227ms/step - loss: 0.6994 - accuracy: 0.7728 - val_loss: 1.2345 - val_accuracy: 0.5250\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 0.5614 - accuracy: 0.8375 - val_loss: 1.2119 - val_accuracy: 0.5462\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 0.5209 - accuracy: 0.8386 - val_loss: 1.2367 - val_accuracy: 0.5442\n",
      "CV iteration 5 of 13\n",
      "Validation subjects are ['p021' 'p045' 'p056']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 27s 233ms/step - loss: 3.2678 - accuracy: 0.1397 - val_loss: 2.1609 - val_accuracy: 0.2604\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 222ms/step - loss: 2.2835 - accuracy: 0.2397 - val_loss: 1.9545 - val_accuracy: 0.3450\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 26s 228ms/step - loss: 1.8742 - accuracy: 0.3391 - val_loss: 1.8443 - val_accuracy: 0.3408\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 1.6990 - accuracy: 0.4092 - val_loss: 1.7509 - val_accuracy: 0.3579\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 26s 225ms/step - loss: 1.4958 - accuracy: 0.4837 - val_loss: 1.6749 - val_accuracy: 0.3679\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.2936 - accuracy: 0.5364 - val_loss: 1.5275 - val_accuracy: 0.4712\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 1.1183 - accuracy: 0.6261 - val_loss: 1.4462 - val_accuracy: 0.4787\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 222ms/step - loss: 0.9589 - accuracy: 0.6848 - val_loss: 1.4052 - val_accuracy: 0.4850\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 27s 232ms/step - loss: 0.8694 - accuracy: 0.7185 - val_loss: 1.4109 - val_accuracy: 0.5354\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 0.7508 - accuracy: 0.7652 - val_loss: 1.3898 - val_accuracy: 0.5479\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 0.6706 - accuracy: 0.7880 - val_loss: 1.2661 - val_accuracy: 0.5612\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 0.5809 - accuracy: 0.8168 - val_loss: 1.4277 - val_accuracy: 0.4771\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.5252 - accuracy: 0.8342 - val_loss: 1.2995 - val_accuracy: 0.5763\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 24s 206ms/step - loss: 0.4193 - accuracy: 0.8701 - val_loss: 1.3744 - val_accuracy: 0.5429\n",
      "CV iteration 6 of 13\n",
      "Validation subjects are ['p042' 'p050' 'p049']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 3.2843 - accuracy: 0.1473 - val_loss: 2.1566 - val_accuracy: 0.2679\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 2.2995 - accuracy: 0.2380 - val_loss: 2.0108 - val_accuracy: 0.2763\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 1.8925 - accuracy: 0.3359 - val_loss: 1.8309 - val_accuracy: 0.3450\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 207ms/step - loss: 1.6534 - accuracy: 0.4283 - val_loss: 1.7273 - val_accuracy: 0.3417\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 1.4555 - accuracy: 0.4902 - val_loss: 1.8062 - val_accuracy: 0.3779\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.2766 - accuracy: 0.5712 - val_loss: 1.7008 - val_accuracy: 0.4175\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.1224 - accuracy: 0.6217 - val_loss: 1.6156 - val_accuracy: 0.4192\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 0.9757 - accuracy: 0.6696 - val_loss: 1.6215 - val_accuracy: 0.4321\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 26s 230ms/step - loss: 0.8885 - accuracy: 0.7043 - val_loss: 1.5789 - val_accuracy: 0.4204\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 27s 233ms/step - loss: 0.7847 - accuracy: 0.7288 - val_loss: 1.5962 - val_accuracy: 0.5054\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 0.6070 - accuracy: 0.8136 - val_loss: 1.4291 - val_accuracy: 0.5333\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 24s 213ms/step - loss: 0.5968 - accuracy: 0.8158 - val_loss: 1.6039 - val_accuracy: 0.5483\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 23s 204ms/step - loss: 0.5135 - accuracy: 0.8484 - val_loss: 1.3984 - val_accuracy: 0.5492\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 24s 205ms/step - loss: 0.4412 - accuracy: 0.8620 - val_loss: 1.6777 - val_accuracy: 0.5550\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 23s 204ms/step - loss: 0.4111 - accuracy: 0.8783 - val_loss: 1.6464 - val_accuracy: 0.5517\n",
      "CV iteration 7 of 13\n",
      "Validation subjects are ['p002' 'p049' 'p045']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 23s 203ms/step - loss: 3.3382 - accuracy: 0.1413 - val_loss: 2.2453 - val_accuracy: 0.2512\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 24s 204ms/step - loss: 2.2962 - accuracy: 0.2375 - val_loss: 2.0718 - val_accuracy: 0.2729\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 23s 203ms/step - loss: 1.9327 - accuracy: 0.3092 - val_loss: 1.8452 - val_accuracy: 0.3787\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 24s 205ms/step - loss: 1.6919 - accuracy: 0.3989 - val_loss: 1.7681 - val_accuracy: 0.3762\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 23s 204ms/step - loss: 1.4715 - accuracy: 0.4929 - val_loss: 1.5941 - val_accuracy: 0.4721\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 205ms/step - loss: 1.3068 - accuracy: 0.5495 - val_loss: 1.5851 - val_accuracy: 0.4712\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 23s 204ms/step - loss: 1.1217 - accuracy: 0.6223 - val_loss: 1.5374 - val_accuracy: 0.4558\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 24s 205ms/step - loss: 0.9970 - accuracy: 0.6668 - val_loss: 1.4177 - val_accuracy: 0.5154\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 23s 203ms/step - loss: 0.8752 - accuracy: 0.7098 - val_loss: 1.3868 - val_accuracy: 0.5446\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 24s 205ms/step - loss: 0.8083 - accuracy: 0.7315 - val_loss: 1.6266 - val_accuracy: 0.4654\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 23s 203ms/step - loss: 0.6759 - accuracy: 0.7804 - val_loss: 1.3108 - val_accuracy: 0.5296\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 24s 205ms/step - loss: 0.5989 - accuracy: 0.8054 - val_loss: 1.3020 - val_accuracy: 0.5971\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 23s 204ms/step - loss: 0.5249 - accuracy: 0.8375 - val_loss: 1.2161 - val_accuracy: 0.5150\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 23s 204ms/step - loss: 0.4554 - accuracy: 0.8582 - val_loss: 1.2226 - val_accuracy: 0.6071\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 23s 204ms/step - loss: 0.3943 - accuracy: 0.8799 - val_loss: 1.2442 - val_accuracy: 0.6467\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 0.3451 - accuracy: 0.9000 - val_loss: 1.4473 - val_accuracy: 0.5642\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.3064 - accuracy: 0.9125 - val_loss: 1.4569 - val_accuracy: 0.5879\n",
      "CV iteration 8 of 13\n",
      "Validation subjects are ['p061' 'p012' 'p041']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 3.3385 - accuracy: 0.1457 - val_loss: 2.1495 - val_accuracy: 0.2483\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 2.2888 - accuracy: 0.2310 - val_loss: 1.9658 - val_accuracy: 0.3567\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 1.8956 - accuracy: 0.3342 - val_loss: 1.8045 - val_accuracy: 0.4083\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 1.6384 - accuracy: 0.4293 - val_loss: 1.7008 - val_accuracy: 0.3821\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 1.3921 - accuracy: 0.5076 - val_loss: 1.5593 - val_accuracy: 0.5342\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 27s 231ms/step - loss: 1.2831 - accuracy: 0.5533 - val_loss: 1.5769 - val_accuracy: 0.5208\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 1.0816 - accuracy: 0.6326 - val_loss: 1.4926 - val_accuracy: 0.5200\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 26s 224ms/step - loss: 0.9217 - accuracy: 0.6897 - val_loss: 1.3796 - val_accuracy: 0.5858\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 0.7993 - accuracy: 0.7484 - val_loss: 1.3945 - val_accuracy: 0.6037\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 0.7493 - accuracy: 0.7451 - val_loss: 1.3462 - val_accuracy: 0.5896\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 27s 236ms/step - loss: 0.6247 - accuracy: 0.8092 - val_loss: 1.3577 - val_accuracy: 0.5808\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 26s 226ms/step - loss: 0.5163 - accuracy: 0.8413 - val_loss: 1.3689 - val_accuracy: 0.5763\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 0.4345 - accuracy: 0.8717 - val_loss: 1.5083 - val_accuracy: 0.5487\n",
      "CV iteration 9 of 13\n",
      "Validation subjects are ['p026' 'p049' 'p015']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 3.3307 - accuracy: 0.1380 - val_loss: 2.2584 - val_accuracy: 0.1883\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 222ms/step - loss: 2.2429 - accuracy: 0.2522 - val_loss: 2.0639 - val_accuracy: 0.3208\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.9291 - accuracy: 0.3397 - val_loss: 1.9065 - val_accuracy: 0.3204\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 1.6494 - accuracy: 0.4185 - val_loss: 1.7224 - val_accuracy: 0.4233\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 1.4142 - accuracy: 0.5011 - val_loss: 1.7071 - val_accuracy: 0.3921\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 1.2359 - accuracy: 0.5576 - val_loss: 1.6116 - val_accuracy: 0.4708\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.0325 - accuracy: 0.6587 - val_loss: 1.6451 - val_accuracy: 0.4558\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 0.9756 - accuracy: 0.6636 - val_loss: 1.4441 - val_accuracy: 0.5096\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.8083 - accuracy: 0.7359 - val_loss: 1.4239 - val_accuracy: 0.5358\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 0.6580 - accuracy: 0.7946 - val_loss: 1.4112 - val_accuracy: 0.5642\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 0.6031 - accuracy: 0.8120 - val_loss: 1.4719 - val_accuracy: 0.5375\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 24s 210ms/step - loss: 0.5540 - accuracy: 0.8304 - val_loss: 1.5298 - val_accuracy: 0.5204\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 0.4624 - accuracy: 0.8603 - val_loss: 1.4821 - val_accuracy: 0.5333\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 24s 209ms/step - loss: 0.3899 - accuracy: 0.8832 - val_loss: 1.3602 - val_accuracy: 0.5671\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 24s 212ms/step - loss: 0.3778 - accuracy: 0.8967 - val_loss: 1.4095 - val_accuracy: 0.6087\n",
      "CV iteration 10 of 13\n",
      "Validation subjects are ['p052' 'p045' 'p051']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 3.2995 - accuracy: 0.1413 - val_loss: 2.1136 - val_accuracy: 0.2458\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 222ms/step - loss: 2.3230 - accuracy: 0.2087 - val_loss: 1.8739 - val_accuracy: 0.3546\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 1.9863 - accuracy: 0.2837 - val_loss: 1.6416 - val_accuracy: 0.4950\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 1.7360 - accuracy: 0.3886 - val_loss: 1.4532 - val_accuracy: 0.5987\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 26s 224ms/step - loss: 1.4947 - accuracy: 0.4609 - val_loss: 1.2656 - val_accuracy: 0.6158\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.3335 - accuracy: 0.5348 - val_loss: 1.3291 - val_accuracy: 0.5383\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.1645 - accuracy: 0.5935 - val_loss: 1.1910 - val_accuracy: 0.6279\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.0182 - accuracy: 0.6516 - val_loss: 1.1051 - val_accuracy: 0.6621\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 30s 263ms/step - loss: 0.8770 - accuracy: 0.7098 - val_loss: 1.0424 - val_accuracy: 0.6708\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 213ms/step - loss: 0.7963 - accuracy: 0.7380 - val_loss: 0.9939 - val_accuracy: 0.6575\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 0.7034 - accuracy: 0.7707 - val_loss: 1.3434 - val_accuracy: 0.5437\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 31s 273ms/step - loss: 0.5550 - accuracy: 0.8272 - val_loss: 1.0475 - val_accuracy: 0.6779\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 0.5542 - accuracy: 0.8163 - val_loss: 1.2783 - val_accuracy: 0.5938\n",
      "CV iteration 11 of 13\n",
      "Validation subjects are ['p045' 'p021' 'p016']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 3.3388 - accuracy: 0.1457 - val_loss: 2.1644 - val_accuracy: 0.1663\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 2.3252 - accuracy: 0.2207 - val_loss: 1.9500 - val_accuracy: 0.3225\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 24s 211ms/step - loss: 1.9412 - accuracy: 0.3152 - val_loss: 1.6657 - val_accuracy: 0.5354\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.7081 - accuracy: 0.4125 - val_loss: 1.6318 - val_accuracy: 0.3917\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.4677 - accuracy: 0.4957 - val_loss: 1.3963 - val_accuracy: 0.5908\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 27s 231ms/step - loss: 1.3294 - accuracy: 0.5489 - val_loss: 1.3647 - val_accuracy: 0.4700\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 1.1727 - accuracy: 0.5929 - val_loss: 1.3467 - val_accuracy: 0.4787\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 221ms/step - loss: 1.0481 - accuracy: 0.6549 - val_loss: 1.2867 - val_accuracy: 0.5367\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.8802 - accuracy: 0.7130 - val_loss: 1.1566 - val_accuracy: 0.6187\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 0.7864 - accuracy: 0.7424 - val_loss: 1.1124 - val_accuracy: 0.5975\n",
      "CV iteration 12 of 13\n",
      "Validation subjects are ['p022' 'p064' 'p035']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 24s 213ms/step - loss: 3.2784 - accuracy: 0.1527 - val_loss: 2.2059 - val_accuracy: 0.1892\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 2.2803 - accuracy: 0.2424 - val_loss: 1.9728 - val_accuracy: 0.2642\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 1.9037 - accuracy: 0.3451 - val_loss: 1.8133 - val_accuracy: 0.3088\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 1.6674 - accuracy: 0.4136 - val_loss: 1.7666 - val_accuracy: 0.3383\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 1.4576 - accuracy: 0.4913 - val_loss: 1.7401 - val_accuracy: 0.3100\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.2745 - accuracy: 0.5668 - val_loss: 1.6146 - val_accuracy: 0.4075\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 1.1438 - accuracy: 0.6239 - val_loss: 1.6309 - val_accuracy: 0.3975\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 213ms/step - loss: 0.9749 - accuracy: 0.6766 - val_loss: 1.4562 - val_accuracy: 0.4792\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 0.8686 - accuracy: 0.7168 - val_loss: 1.3962 - val_accuracy: 0.4812\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 0.7261 - accuracy: 0.7739 - val_loss: 1.2934 - val_accuracy: 0.5333\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 0.6052 - accuracy: 0.8185 - val_loss: 1.3094 - val_accuracy: 0.5600\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 0.5537 - accuracy: 0.8293 - val_loss: 1.3201 - val_accuracy: 0.5392\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 0.5291 - accuracy: 0.8359 - val_loss: 1.3733 - val_accuracy: 0.5271\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 0.4289 - accuracy: 0.8783 - val_loss: 1.3424 - val_accuracy: 0.5875\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 0.3696 - accuracy: 0.8935 - val_loss: 1.3621 - val_accuracy: 0.5788\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 26s 223ms/step - loss: 0.3630 - accuracy: 0.8897 - val_loss: 1.3020 - val_accuracy: 0.5454\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 26s 222ms/step - loss: 0.2906 - accuracy: 0.9207 - val_loss: 1.1516 - val_accuracy: 0.6079\n",
      "Epoch 18/50\n",
      "115/115 [==============================] - 25s 216ms/step - loss: 0.2855 - accuracy: 0.9125 - val_loss: 1.2265 - val_accuracy: 0.5987\n",
      "Epoch 19/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 0.2744 - accuracy: 0.9152 - val_loss: 1.1003 - val_accuracy: 0.6083\n",
      "CV iteration 13 of 13\n",
      "Validation subjects are ['p021' 'p061' 'p042']\n",
      "Found 18400 validated image filenames belonging to 10 classes.\n",
      "Found 2400 validated image filenames belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 115 steps, validate for 75 steps\n",
      "Epoch 1/50\n",
      "115/115 [==============================] - 25s 214ms/step - loss: 3.3437 - accuracy: 0.1440 - val_loss: 2.1638 - val_accuracy: 0.2050\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 2.2837 - accuracy: 0.2337 - val_loss: 1.9531 - val_accuracy: 0.2188\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 25s 215ms/step - loss: 1.9037 - accuracy: 0.3348 - val_loss: 1.7303 - val_accuracy: 0.3408\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 25s 220ms/step - loss: 1.6857 - accuracy: 0.4125 - val_loss: 1.5326 - val_accuracy: 0.4233\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 26s 224ms/step - loss: 1.4746 - accuracy: 0.4821 - val_loss: 1.3959 - val_accuracy: 0.5600\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 26s 224ms/step - loss: 1.2293 - accuracy: 0.5875 - val_loss: 1.3325 - val_accuracy: 0.4583\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 25s 219ms/step - loss: 1.0728 - accuracy: 0.6538 - val_loss: 1.2133 - val_accuracy: 0.5667\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.9653 - accuracy: 0.6848 - val_loss: 1.1371 - val_accuracy: 0.6371\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 25s 217ms/step - loss: 0.8584 - accuracy: 0.7245 - val_loss: 1.0490 - val_accuracy: 0.6746\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 25s 222ms/step - loss: 0.7037 - accuracy: 0.7750 - val_loss: 1.1052 - val_accuracy: 0.6517\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 25s 218ms/step - loss: 0.6461 - accuracy: 0.7875 - val_loss: 1.1427 - val_accuracy: 0.6263\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 31s 273ms/step - loss: 0.5437 - accuracy: 0.8255 - val_loss: 0.9531 - val_accuracy: 0.6867\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 26s 226ms/step - loss: 0.5120 - accuracy: 0.8418 - val_loss: 1.0250 - val_accuracy: 0.6283\n"
     ]
    }
   ],
   "source": [
    "model4data = cvrand(model4, \n",
    "                    df,\n",
    "                    n_iterations=13,\n",
    "                    batch_size=16,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch=115,\n",
    "                    target_size=(227,227),\n",
    "                    random_state=42,\n",
    "                    min_delta=0.05,\n",
    "                    patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>validation_subjects</th>\n",
       "      <th>train_accuracies</th>\n",
       "      <th>validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[p026, p050, p002]</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[p052, p039, p024]</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[p049, p064, p042]</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[p051, p066, p014]</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[p021, p045, p056]</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[p042, p050, p049]</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[p002, p049, p045]</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[p061, p012, p041]</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[p026, p049, p015]</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[p052, p045, p051]</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[p045, p021, p016]</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[p022, p064, p035]</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[p021, p061, p042]</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   validation_subjects  train_accuracies  validation_accuracy\n",
       "0   [p026, p050, p002]             0.801                0.395\n",
       "1   [p052, p039, p024]             0.779                0.662\n",
       "2   [p049, p064, p042]             0.846                0.669\n",
       "3   [p051, p066, p014]             0.838                0.546\n",
       "4   [p021, p045, p056]             0.834                0.576\n",
       "5   [p042, p050, p049]             0.862                0.555\n",
       "6   [p002, p049, p045]             0.880                0.647\n",
       "7   [p061, p012, p041]             0.748                0.604\n",
       "8   [p026, p049, p015]             0.897                0.609\n",
       "9   [p052, p045, p051]             0.827                0.678\n",
       "10  [p045, p021, p016]             0.713                0.619\n",
       "11  [p022, p064, p035]             0.915                0.608\n",
       "12  [p021, p061, p042]             0.826                0.687"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4data.to_csv('../metrics/model4metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.figure(figsize=(10,10))\n",
    "_ = plt.xlim(0, 1)\n",
    "_ = plt.ylim(0, 1)\n",
    "_ = sns.scatterplot(data=model4data, x='train_accuracies', y='validation_accuracy')\n",
    "_ = plt.plot([0,1], [0,1])\n",
    "_ = plt.title('Cross Validation Training Accuracies by Validation Accuracies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
